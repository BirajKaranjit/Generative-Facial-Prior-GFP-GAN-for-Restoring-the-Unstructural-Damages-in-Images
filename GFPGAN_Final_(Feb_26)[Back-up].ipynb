{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "import os\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from math import sqrt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg19\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "97FCeE_JhEsr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqqLp-Qaqq1h",
        "outputId": "d576d517-f6c5-4191-fe0c-0240745f06e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/GFPGAN/pretrained/final.pth\"\n",
        "model_dict = torch.load(model_path, map_location=\"cpu\")\n",
        "\n",
        "print(model_dict.keys())  # Check what is stored inside\n",
        "\n",
        "# g_ema -> exponential moving average of Generator. It is a smoother, more stable version of the generator.\n",
        "# During training, it is updated slowly using an EMA formula. This prevents the model from changing too fast due to noisy updates\n",
        "#Œ± (decay factor) is usually 0.999 or 0.9999.\n",
        "\n",
        "# ùëî_ema = ùõº.g_ema + (1-ùõº).G"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFZYB0MeXcez",
        "outputId": "ccc406c4-5934-445f-cb5f-fbf75abf6894"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-3c39c3f67c13>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_dict = torch.load(model_path, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['G', 'g_ema', 'g_optim', 'D', 'd_optim', 'args', 'left_eye_d', 'right_eye_d', 'mouth_d'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Degradations.py\n",
        "# import cv2\n",
        "# import math\n",
        "# import numpy as np\n",
        "# import random\n",
        "# import torch\n",
        "# from scipy import special\n",
        "# from scipy.stats import multivariate_normal\n",
        "# # from torchvision.transforms.functional_tensor import rgb_to_grayscale\n",
        "\n",
        "# # -------------------------------------------------------------------- #\n",
        "# # --------------------------- blur kernels --------------------------- #\n",
        "# # -------------------------------------------------------------------- #\n",
        "\n",
        "\n",
        "# # --------------------------- util functions --------------------------- #\n",
        "# def sigma_matrix2(sig_x, sig_y, theta):\n",
        "#     \"\"\"Calculate the rotated sigma matrix (two dimensional matrix).\n",
        "\n",
        "#     Args:\n",
        "#         sig_x (float):\n",
        "#         sig_y (float):\n",
        "#         theta (float): Radian measurement.\n",
        "\n",
        "#     Returns:\n",
        "#         ndarray: Rotated sigma matrix.\n",
        "#     \"\"\"\n",
        "#     d_matrix = np.array([[sig_x**2, 0], [0, sig_y**2]])\n",
        "#     u_matrix = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])\n",
        "#     return np.dot(u_matrix, np.dot(d_matrix, u_matrix.T))\n",
        "\n",
        "\n",
        "# def mesh_grid(kernel_size):\n",
        "#     \"\"\"Generate the mesh grid, centering at zero.\n",
        "\n",
        "#     Args:\n",
        "#         kernel_size (int):\n",
        "\n",
        "#     Returns:\n",
        "#         xy (ndarray): with the shape (kernel_size, kernel_size, 2)\n",
        "#         xx (ndarray): with the shape (kernel_size, kernel_size)\n",
        "#         yy (ndarray): with the shape (kernel_size, kernel_size)\n",
        "#     \"\"\"\n",
        "#     ax = np.arange(-kernel_size // 2 + 1., kernel_size // 2 + 1.)\n",
        "#     xx, yy = np.meshgrid(ax, ax)\n",
        "#     xy = np.hstack((xx.reshape((kernel_size * kernel_size, 1)), yy.reshape(kernel_size * kernel_size,\n",
        "#                                                                            1))).reshape(kernel_size, kernel_size, 2)\n",
        "#     return xy, xx, yy\n",
        "\n",
        "\n",
        "# def pdf2(sigma_matrix, grid):\n",
        "#     \"\"\"Calculate PDF of the bivariate Gaussian distribution.\n",
        "\n",
        "#     Args:\n",
        "#         sigma_matrix (ndarray): with the shape (2, 2)\n",
        "#         grid (ndarray): generated by :func:`mesh_grid`,\n",
        "#             with the shape (K, K, 2), K is the kernel size.\n",
        "\n",
        "#     Returns:\n",
        "#         kernel (ndarrray): un-normalized kernel.\n",
        "#     \"\"\"\n",
        "#     inverse_sigma = np.linalg.inv(sigma_matrix)\n",
        "#     kernel = np.exp(-0.5 * np.sum(np.dot(grid, inverse_sigma) * grid, 2))\n",
        "#     return kernel\n",
        "\n",
        "\n",
        "# def cdf2(d_matrix, grid):\n",
        "#     \"\"\"Calculate the CDF of the standard bivariate Gaussian distribution.\n",
        "#         Used in skewed Gaussian distribution.\n",
        "\n",
        "#     Args:\n",
        "#         d_matrix (ndarrasy): skew matrix.\n",
        "#         grid (ndarray): generated by :func:`mesh_grid`,\n",
        "#             with the shape (K, K, 2), K is the kernel size.\n",
        "\n",
        "#     Returns:\n",
        "#         cdf (ndarray): skewed cdf.\n",
        "#     \"\"\"\n",
        "#     rv = multivariate_normal([0, 0], [[1, 0], [0, 1]])\n",
        "#     grid = np.dot(grid, d_matrix)\n",
        "#     cdf = rv.cdf(grid)\n",
        "#     return cdf\n",
        "\n",
        "\n",
        "# def bivariate_Gaussian(kernel_size, sig_x, sig_y, theta, grid=None, isotropic=True):\n",
        "#     \"\"\"Generate a bivariate isotropic or anisotropic Gaussian kernel.\n",
        "\n",
        "#     In the isotropic mode, only `sig_x` is used. `sig_y` and `theta` is ignored.\n",
        "\n",
        "#     Args:\n",
        "#         kernel_size (int):\n",
        "#         sig_x (float):\n",
        "#         sig_y (float):\n",
        "#         theta (float): Radian measurement.\n",
        "#         grid (ndarray, optional): generated by :func:`mesh_grid`,\n",
        "#             with the shape (K, K, 2), K is the kernel size. Default: None\n",
        "#         isotropic (bool):\n",
        "\n",
        "#     Returns:\n",
        "#         kernel (ndarray): normalized kernel.\n",
        "#     \"\"\"\n",
        "#     if grid is None:\n",
        "#         grid, _, _ = mesh_grid(kernel_size)\n",
        "#     if isotropic:\n",
        "#         sigma_matrix = np.array([[sig_x**2, 0], [0, sig_x**2]])\n",
        "#     else:\n",
        "#         sigma_matrix = sigma_matrix2(sig_x, sig_y, theta)\n",
        "#     kernel = pdf2(sigma_matrix, grid)\n",
        "#     kernel = kernel / np.sum(kernel)\n",
        "#     return kernel\n",
        "\n",
        "\n",
        "# def bivariate_generalized_Gaussian(kernel_size, sig_x, sig_y, theta, beta, grid=None, isotropic=True):\n",
        "#     \"\"\"Generate a bivariate generalized Gaussian kernel.\n",
        "#         Described in `Parameter Estimation For Multivariate Generalized\n",
        "#         Gaussian Distributions`_\n",
        "#         by Pascal et. al (2013).\n",
        "\n",
        "#     In the isotropic mode, only `sig_x` is used. `sig_y` and `theta` is ignored.\n",
        "\n",
        "#     Args:\n",
        "#         kernel_size (int):\n",
        "#         sig_x (float):\n",
        "#         sig_y (float):\n",
        "#         theta (float): Radian measurement.\n",
        "#         beta (float): shape parameter, beta = 1 is the normal distribution.\n",
        "#         grid (ndarray, optional): generated by :func:`mesh_grid`,\n",
        "#             with the shape (K, K, 2), K is the kernel size. Default: None\n",
        "\n",
        "#     Returns:\n",
        "#         kernel (ndarray): normalized kernel.\n",
        "\n",
        "#     .. _Parameter Estimation For Multivariate Generalized Gaussian\n",
        "#     Distributions: https://arxiv.org/abs/1302.6498\n",
        "#     \"\"\"\n",
        "#     if grid is None:\n",
        "#         grid, _, _ = mesh_grid(kernel_size)\n",
        "#     if isotropic:\n",
        "#         sigma_matrix = np.array([[sig_x**2, 0], [0, sig_x**2]])\n",
        "#     else:\n",
        "#         sigma_matrix = sigma_matrix2(sig_x, sig_y, theta)\n",
        "#     inverse_sigma = np.linalg.inv(sigma_matrix)\n",
        "#     kernel = np.exp(-0.5 * np.power(np.sum(np.dot(grid, inverse_sigma) * grid, 2), beta))\n",
        "#     kernel = kernel / np.sum(kernel)\n",
        "#     return kernel\n",
        "\n",
        "\n",
        "# def bivariate_plateau(kernel_size, sig_x, sig_y, theta, beta, grid=None, isotropic=True):\n",
        "#     \"\"\"Generate a plateau-like anisotropic kernel.\n",
        "#     1 / (1+x^(beta))\n",
        "\n",
        "#     Ref: https://stats.stackexchange.com/questions/203629/is-there-a-plateau-shaped-distribution\n",
        "\n",
        "#     In the isotropic mode, only `sig_x` is used. `sig_y` and `theta` is ignored.\n",
        "\n",
        "#     Args:\n",
        "#         kernel_size (int):\n",
        "#         sig_x (float):\n",
        "#         sig_y (float):\n",
        "#         theta (float): Radian measurement.\n",
        "#         beta (float): shape parameter, beta = 1 is the normal distribution.\n",
        "#         grid (ndarray, optional): generated by :func:`mesh_grid`,\n",
        "#             with the shape (K, K, 2), K is the kernel size. Default: None\n",
        "\n",
        "#     Returns:\n",
        "#         kernel (ndarray): normalized kernel.\n",
        "#     \"\"\"\n",
        "#     if grid is None:\n",
        "#         grid, _, _ = mesh_grid(kernel_size)\n",
        "#     if isotropic:\n",
        "#         sigma_matrix = np.array([[sig_x**2, 0], [0, sig_x**2]])\n",
        "#     else:\n",
        "#         sigma_matrix = sigma_matrix2(sig_x, sig_y, theta)\n",
        "#     inverse_sigma = np.linalg.inv(sigma_matrix)\n",
        "#     kernel = np.reciprocal(np.power(np.sum(np.dot(grid, inverse_sigma) * grid, 2), beta) + 1)\n",
        "#     kernel = kernel / np.sum(kernel)\n",
        "#     return kernel\n",
        "\n",
        "\n",
        "# def random_bivariate_Gaussian(kernel_size,\n",
        "#                               sigma_x_range,\n",
        "#                               sigma_y_range,\n",
        "#                               rotation_range,\n",
        "#                               noise_range=None,\n",
        "#                               isotropic=True):\n",
        "#     \"\"\"Randomly generate bivariate isotropic or anisotropic Gaussian kernels.\n",
        "\n",
        "#     In the isotropic mode, only `sigma_x_range` is used. `sigma_y_range` and `rotation_range` is ignored.\n",
        "\n",
        "#     Args:\n",
        "#         kernel_size (int):\n",
        "#         sigma_x_range (tuple): [0.6, 5]\n",
        "#         sigma_y_range (tuple): [0.6, 5]\n",
        "#         rotation range (tuple): [-math.pi, math.pi]\n",
        "#         noise_range(tuple, optional): multiplicative kernel noise,\n",
        "#             [0.75, 1.25]. Default: None\n",
        "\n",
        "#     Returns:\n",
        "#         kernel (ndarray):\n",
        "#     \"\"\"\n",
        "#     assert kernel_size % 2 == 1, 'Kernel size must be an odd number.'\n",
        "#     assert sigma_x_range[0] < sigma_x_range[1], 'Wrong sigma_x_range.'\n",
        "#     sigma_x = np.random.uniform(sigma_x_range[0], sigma_x_range[1])\n",
        "#     if isotropic is False:\n",
        "#         assert sigma_y_range[0] < sigma_y_range[1], 'Wrong sigma_y_range.'\n",
        "#         assert rotation_range[0] < rotation_range[1], 'Wrong rotation_range.'\n",
        "#         sigma_y = np.random.uniform(sigma_y_range[0], sigma_y_range[1])\n",
        "#         rotation = np.random.uniform(rotation_range[0], rotation_range[1])\n",
        "#     else:\n",
        "#         sigma_y = sigma_x\n",
        "#         rotation = 0\n",
        "\n",
        "#     kernel = bivariate_Gaussian(kernel_size, sigma_x, sigma_y, rotation, isotropic=isotropic)\n",
        "\n",
        "#     # add multiplicative noise\n",
        "#     if noise_range is not None:\n",
        "#         assert noise_range[0] < noise_range[1], 'Wrong noise range.'\n",
        "#         noise = np.random.uniform(noise_range[0], noise_range[1], size=kernel.shape)\n",
        "#         kernel = kernel * noise\n",
        "#     kernel = kernel / np.sum(kernel)\n",
        "#     return kernel\n",
        "\n",
        "\n",
        "# def random_bivariate_generalized_Gaussian(kernel_size,\n",
        "#                                           sigma_x_range,\n",
        "#                                           sigma_y_range,\n",
        "#                                           rotation_range,\n",
        "#                                           beta_range,\n",
        "#                                           noise_range=None,\n",
        "#                                           isotropic=True):\n",
        "#     \"\"\"Randomly generate bivariate generalized Gaussian kernels.\n",
        "\n",
        "#     In the isotropic mode, only `sigma_x_range` is used. `sigma_y_range` and `rotation_range` is ignored.\n",
        "\n",
        "#     Args:\n",
        "#         kernel_size (int):\n",
        "#         sigma_x_range (tuple): [0.6, 5]\n",
        "#         sigma_y_range (tuple): [0.6, 5]\n",
        "#         rotation range (tuple): [-math.pi, math.pi]\n",
        "#         beta_range (tuple): [0.5, 8]\n",
        "#         noise_range(tuple, optional): multiplicative kernel noise,\n",
        "#             [0.75, 1.25]. Default: None\n",
        "\n",
        "#     Returns:\n",
        "#         kernel (ndarray):\n",
        "#     \"\"\"\n",
        "#     assert kernel_size % 2 == 1, 'Kernel size must be an odd number.'\n",
        "#     assert sigma_x_range[0] < sigma_x_range[1], 'Wrong sigma_x_range.'\n",
        "#     sigma_x = np.random.uniform(sigma_x_range[0], sigma_x_range[1])\n",
        "#     if isotropic is False:\n",
        "#         assert sigma_y_range[0] < sigma_y_range[1], 'Wrong sigma_y_range.'\n",
        "#         assert rotation_range[0] < rotation_range[1], 'Wrong rotation_range.'\n",
        "#         sigma_y = np.random.uniform(sigma_y_range[0], sigma_y_range[1])\n",
        "#         rotation = np.random.uniform(rotation_range[0], rotation_range[1])\n",
        "#     else:\n",
        "#         sigma_y = sigma_x\n",
        "#         rotation = 0\n",
        "\n",
        "#     # assume beta_range[0] < 1 < beta_range[1]\n",
        "#     if np.random.uniform() < 0.5:\n",
        "#         beta = np.random.uniform(beta_range[0], 1)\n",
        "#     else:\n",
        "#         beta = np.random.uniform(1, beta_range[1])\n",
        "\n",
        "#     kernel = bivariate_generalized_Gaussian(kernel_size, sigma_x, sigma_y, rotation, beta, isotropic=isotropic)\n",
        "\n",
        "#     # add multiplicative noise\n",
        "#     if noise_range is not None:\n",
        "#         assert noise_range[0] < noise_range[1], 'Wrong noise range.'\n",
        "#         noise = np.random.uniform(noise_range[0], noise_range[1], size=kernel.shape)\n",
        "#         kernel = kernel * noise\n",
        "#     kernel = kernel / np.sum(kernel)\n",
        "#     return kernel\n",
        "\n",
        "\n",
        "# def random_bivariate_plateau(kernel_size,\n",
        "#                              sigma_x_range,\n",
        "#                              sigma_y_range,\n",
        "#                              rotation_range,\n",
        "#                              beta_range,\n",
        "#                              noise_range=None,\n",
        "#                              isotropic=True):\n",
        "#     \"\"\"Randomly generate bivariate plateau kernels.\n",
        "\n",
        "#     In the isotropic mode, only `sigma_x_range` is used. `sigma_y_range` and `rotation_range` is ignored.\n",
        "\n",
        "#     Args:\n",
        "#         kernel_size (int):\n",
        "#         sigma_x_range (tuple): [0.6, 5]\n",
        "#         sigma_y_range (tuple): [0.6, 5]\n",
        "#         rotation range (tuple): [-math.pi/2, math.pi/2]\n",
        "#         beta_range (tuple): [1, 4]\n",
        "#         noise_range(tuple, optional): multiplicative kernel noise,\n",
        "#             [0.75, 1.25]. Default: None\n",
        "\n",
        "#     Returns:\n",
        "#         kernel (ndarray):\n",
        "#     \"\"\"\n",
        "#     assert kernel_size % 2 == 1, 'Kernel size must be an odd number.'\n",
        "#     assert sigma_x_range[0] < sigma_x_range[1], 'Wrong sigma_x_range.'\n",
        "#     sigma_x = np.random.uniform(sigma_x_range[0], sigma_x_range[1])\n",
        "#     if isotropic is False:\n",
        "#         assert sigma_y_range[0] < sigma_y_range[1], 'Wrong sigma_y_range.'\n",
        "#         assert rotation_range[0] < rotation_range[1], 'Wrong rotation_range.'\n",
        "#         sigma_y = np.random.uniform(sigma_y_range[0], sigma_y_range[1])\n",
        "#         rotation = np.random.uniform(rotation_range[0], rotation_range[1])\n",
        "#     else:\n",
        "#         sigma_y = sigma_x\n",
        "#         rotation = 0\n",
        "\n",
        "#     # TODO: this may be not proper\n",
        "#     if np.random.uniform() < 0.5:\n",
        "#         beta = np.random.uniform(beta_range[0], 1)\n",
        "#     else:\n",
        "#         beta = np.random.uniform(1, beta_range[1])\n",
        "\n",
        "#     kernel = bivariate_plateau(kernel_size, sigma_x, sigma_y, rotation, beta, isotropic=isotropic)\n",
        "#     # add multiplicative noise\n",
        "#     if noise_range is not None:\n",
        "#         assert noise_range[0] < noise_range[1], 'Wrong noise range.'\n",
        "#         noise = np.random.uniform(noise_range[0], noise_range[1], size=kernel.shape)\n",
        "#         kernel = kernel * noise\n",
        "#     kernel = kernel / np.sum(kernel)\n",
        "\n",
        "#     return kernel\n",
        "\n",
        "\n",
        "# def random_mixed_kernels(kernel_list,\n",
        "#                          kernel_prob,\n",
        "#                          kernel_size=21,\n",
        "#                          sigma_x_range=(0.6, 5),\n",
        "#                          sigma_y_range=(0.6, 5),\n",
        "#                          rotation_range=(-math.pi, math.pi),\n",
        "#                          betag_range=(0.5, 8),\n",
        "#                          betap_range=(0.5, 8),\n",
        "#                          noise_range=None):\n",
        "#     \"\"\"Randomly generate mixed kernels.\n",
        "\n",
        "#     Args:\n",
        "#         kernel_list (tuple): a list name of kernel types,\n",
        "#             support ['iso', 'aniso', 'skew', 'generalized', 'plateau_iso',\n",
        "#             'plateau_aniso']\n",
        "#         kernel_prob (tuple): corresponding kernel probability for each\n",
        "#             kernel type\n",
        "#         kernel_size (int):\n",
        "#         sigma_x_range (tuple): [0.6, 5]\n",
        "#         sigma_y_range (tuple): [0.6, 5]\n",
        "#         rotation range (tuple): [-math.pi, math.pi]\n",
        "#         beta_range (tuple): [0.5, 8]\n",
        "#         noise_range(tuple, optional): multiplicative kernel noise,\n",
        "#             [0.75, 1.25]. Default: None\n",
        "\n",
        "#     Returns:\n",
        "#         kernel (ndarray):\n",
        "#     \"\"\"\n",
        "#     kernel_type = random.choices(kernel_list, kernel_prob)[0]\n",
        "#     if kernel_type == 'iso':\n",
        "#         kernel = random_bivariate_Gaussian(\n",
        "#             kernel_size, sigma_x_range, sigma_y_range, rotation_range, noise_range=noise_range, isotropic=True)\n",
        "#     elif kernel_type == 'aniso':\n",
        "#         kernel = random_bivariate_Gaussian(\n",
        "#             kernel_size, sigma_x_range, sigma_y_range, rotation_range, noise_range=noise_range, isotropic=False)\n",
        "#     elif kernel_type == 'generalized_iso':\n",
        "#         kernel = random_bivariate_generalized_Gaussian(\n",
        "#             kernel_size,\n",
        "#             sigma_x_range,\n",
        "#             sigma_y_range,\n",
        "#             rotation_range,\n",
        "#             betag_range,\n",
        "#             noise_range=noise_range,\n",
        "#             isotropic=True)\n",
        "#     elif kernel_type == 'generalized_aniso':\n",
        "#         kernel = random_bivariate_generalized_Gaussian(\n",
        "#             kernel_size,\n",
        "#             sigma_x_range,\n",
        "#             sigma_y_range,\n",
        "#             rotation_range,\n",
        "#             betag_range,\n",
        "#             noise_range=noise_range,\n",
        "#             isotropic=False)\n",
        "#     elif kernel_type == 'plateau_iso':\n",
        "#         kernel = random_bivariate_plateau(\n",
        "#             kernel_size, sigma_x_range, sigma_y_range, rotation_range, betap_range, noise_range=None, isotropic=True)\n",
        "#     elif kernel_type == 'plateau_aniso':\n",
        "#         kernel = random_bivariate_plateau(\n",
        "#             kernel_size, sigma_x_range, sigma_y_range, rotation_range, betap_range, noise_range=None, isotropic=False)\n",
        "#     return kernel\n",
        "\n",
        "\n",
        "# np.seterr(divide='ignore', invalid='ignore')\n",
        "\n",
        "\n",
        "# def circular_lowpass_kernel(cutoff, kernel_size, pad_to=0):\n",
        "#     \"\"\"2D sinc filter, ref: https://dsp.stackexchange.com/questions/58301/2-d-circularly-symmetric-low-pass-filter\n",
        "\n",
        "#     Args:\n",
        "#         cutoff (float): cutoff frequency in radians (pi is max)\n",
        "#         kernel_size (int): horizontal and vertical size, must be odd.\n",
        "#         pad_to (int): pad kernel size to desired size, must be odd or zero.\n",
        "#     \"\"\"\n",
        "#     assert kernel_size % 2 == 1, 'Kernel size must be an odd number.'\n",
        "#     kernel = np.fromfunction(\n",
        "#         lambda x, y: cutoff * special.j1(cutoff * np.sqrt(\n",
        "#             (x - (kernel_size - 1) / 2)**2 + (y - (kernel_size - 1) / 2)**2)) / (2 * np.pi * np.sqrt(\n",
        "#                 (x - (kernel_size - 1) / 2)**2 + (y - (kernel_size - 1) / 2)**2)), [kernel_size, kernel_size])\n",
        "#     kernel[(kernel_size - 1) // 2, (kernel_size - 1) // 2] = cutoff**2 / (4 * np.pi)\n",
        "#     kernel = kernel / np.sum(kernel)\n",
        "#     if pad_to > kernel_size:\n",
        "#         pad_size = (pad_to - kernel_size) // 2\n",
        "#         kernel = np.pad(kernel, ((pad_size, pad_size), (pad_size, pad_size)))\n",
        "#     return kernel\n",
        "\n",
        "\n",
        "# # ------------------------------------------------------------- #\n",
        "# # --------------------------- noise --------------------------- #\n",
        "# # ------------------------------------------------------------- #\n",
        "\n",
        "# # ----------------------- Gaussian Noise ----------------------- #\n",
        "\n",
        "\n",
        "# def generate_gaussian_noise(img, sigma=10, gray_noise=False):\n",
        "#     \"\"\"Generate Gaussian noise.\n",
        "\n",
        "#     Args:\n",
        "#         img (Numpy array): Input image, shape (h, w, c), range [0, 1], float32.\n",
        "#         sigma (float): Noise scale (measured in range 255). Default: 10.\n",
        "\n",
        "#     Returns:\n",
        "#         (Numpy array): Returned noisy image, shape (h, w, c), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     if gray_noise:\n",
        "#         noise = np.float32(np.random.randn(*(img.shape[0:2]))) * sigma / 255.\n",
        "#         noise = np.expand_dims(noise, axis=2).repeat(3, axis=2)\n",
        "#     else:\n",
        "#         noise = np.float32(np.random.randn(*(img.shape))) * sigma / 255.\n",
        "#     return noise\n",
        "\n",
        "\n",
        "# def add_gaussian_noise(img, sigma=10, clip=True, rounds=False, gray_noise=False):\n",
        "#     \"\"\"Add Gaussian noise.\n",
        "\n",
        "#     Args:\n",
        "#         img (Numpy array): Input image, shape (h, w, c), range [0, 1], float32.\n",
        "#         sigma (float): Noise scale (measured in range 255). Default: 10.\n",
        "\n",
        "#     Returns:\n",
        "#         (Numpy array): Returned noisy image, shape (h, w, c), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     noise = generate_gaussian_noise(img, sigma, gray_noise)\n",
        "#     out = img + noise\n",
        "#     if clip and rounds:\n",
        "#         out = np.clip((out * 255.0).round(), 0, 255) / 255.\n",
        "#     elif clip:\n",
        "#         out = np.clip(out, 0, 1)\n",
        "#     elif rounds:\n",
        "#         out = (out * 255.0).round() / 255.\n",
        "#     return out\n",
        "\n",
        "\n",
        "# def generate_gaussian_noise_pt(img, sigma=10, gray_noise=0):\n",
        "#     \"\"\"Add Gaussian noise (PyTorch version).\n",
        "\n",
        "#     Args:\n",
        "#         img (Tensor): Shape (b, c, h, w), range[0, 1], float32.\n",
        "#         scale (float | Tensor): Noise scale. Default: 1.0.\n",
        "\n",
        "#     Returns:\n",
        "#         (Tensor): Returned noisy image, shape (b, c, h, w), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     b, _, h, w = img.size()\n",
        "#     if not isinstance(sigma, (float, int)):\n",
        "#         sigma = sigma.view(img.size(0), 1, 1, 1)\n",
        "#     if isinstance(gray_noise, (float, int)):\n",
        "#         cal_gray_noise = gray_noise > 0\n",
        "#     else:\n",
        "#         gray_noise = gray_noise.view(b, 1, 1, 1)\n",
        "#         cal_gray_noise = torch.sum(gray_noise) > 0\n",
        "\n",
        "#     if cal_gray_noise:\n",
        "#         noise_gray = torch.randn(*img.size()[2:4], dtype=img.dtype, device=img.device) * sigma / 255.\n",
        "#         noise_gray = noise_gray.view(b, 1, h, w)\n",
        "\n",
        "#     # always calculate color noise\n",
        "#     noise = torch.randn(*img.size(), dtype=img.dtype, device=img.device) * sigma / 255.\n",
        "\n",
        "#     if cal_gray_noise:\n",
        "#         noise = noise * (1 - gray_noise) + noise_gray * gray_noise\n",
        "#     return noise\n",
        "\n",
        "\n",
        "# def add_gaussian_noise_pt(img, sigma=10, gray_noise=0, clip=True, rounds=False):\n",
        "#     \"\"\"Add Gaussian noise (PyTorch version).\n",
        "\n",
        "#     Args:\n",
        "#         img (Tensor): Shape (b, c, h, w), range[0, 1], float32.\n",
        "#         scale (float | Tensor): Noise scale. Default: 1.0.\n",
        "\n",
        "#     Returns:\n",
        "#         (Tensor): Returned noisy image, shape (b, c, h, w), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     noise = generate_gaussian_noise_pt(img, sigma, gray_noise)\n",
        "#     out = img + noise\n",
        "#     if clip and rounds:\n",
        "#         out = torch.clamp((out * 255.0).round(), 0, 255) / 255.\n",
        "#     elif clip:\n",
        "#         out = torch.clamp(out, 0, 1)\n",
        "#     elif rounds:\n",
        "#         out = (out * 255.0).round() / 255.\n",
        "#     return out\n",
        "\n",
        "\n",
        "# # ----------------------- Random Gaussian Noise ----------------------- #\n",
        "# def random_generate_gaussian_noise(img, sigma_range=(0, 10), gray_prob=0):\n",
        "#     sigma = np.random.uniform(sigma_range[0], sigma_range[1])\n",
        "#     if np.random.uniform() < gray_prob:\n",
        "#         gray_noise = True\n",
        "#     else:\n",
        "#         gray_noise = False\n",
        "#     return generate_gaussian_noise(img, sigma, gray_noise)\n",
        "\n",
        "\n",
        "# def random_add_gaussian_noise(img, sigma_range=(0, 1.0), gray_prob=0, clip=True, rounds=False):\n",
        "#     noise = random_generate_gaussian_noise(img, sigma_range, gray_prob)\n",
        "#     out = img + noise\n",
        "#     if clip and rounds:\n",
        "#         out = np.clip((out * 255.0).round(), 0, 255) / 255.\n",
        "#     elif clip:\n",
        "#         out = np.clip(out, 0, 1)\n",
        "#     elif rounds:\n",
        "#         out = (out * 255.0).round() / 255.\n",
        "#     return out\n",
        "\n",
        "\n",
        "# def random_generate_gaussian_noise_pt(img, sigma_range=(0, 10), gray_prob=0):\n",
        "#     sigma = torch.rand(\n",
        "#         img.size(0), dtype=img.dtype, device=img.device) * (sigma_range[1] - sigma_range[0]) + sigma_range[0]\n",
        "#     gray_noise = torch.rand(img.size(0), dtype=img.dtype, device=img.device)\n",
        "#     gray_noise = (gray_noise < gray_prob).float()\n",
        "#     return generate_gaussian_noise_pt(img, sigma, gray_noise)\n",
        "\n",
        "\n",
        "# def random_add_gaussian_noise_pt(img, sigma_range=(0, 1.0), gray_prob=0, clip=True, rounds=False):\n",
        "#     noise = random_generate_gaussian_noise_pt(img, sigma_range, gray_prob)\n",
        "#     out = img + noise\n",
        "#     if clip and rounds:\n",
        "#         out = torch.clamp((out * 255.0).round(), 0, 255) / 255.\n",
        "#     elif clip:\n",
        "#         out = torch.clamp(out, 0, 1)\n",
        "#     elif rounds:\n",
        "#         out = (out * 255.0).round() / 255.\n",
        "#     return out\n",
        "\n",
        "\n",
        "# # ----------------------- Poisson (Shot) Noise ----------------------- #\n",
        "\n",
        "\n",
        "# def generate_poisson_noise(img, scale=1.0, gray_noise=False):\n",
        "#     \"\"\"Generate poisson noise.\n",
        "\n",
        "#     Ref: https://github.com/scikit-image/scikit-image/blob/main/skimage/util/noise.py#L37-L219\n",
        "\n",
        "#     Args:\n",
        "#         img (Numpy array): Input image, shape (h, w, c), range [0, 1], float32.\n",
        "#         scale (float): Noise scale. Default: 1.0.\n",
        "#         gray_noise (bool): Whether generate gray noise. Default: False.\n",
        "\n",
        "#     Returns:\n",
        "#         (Numpy array): Returned noisy image, shape (h, w, c), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     if gray_noise:\n",
        "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "#     # round and clip image for counting vals correctly\n",
        "#     img = np.clip((img * 255.0).round(), 0, 255) / 255.\n",
        "#     vals = len(np.unique(img))\n",
        "#     vals = 2**np.ceil(np.log2(vals))\n",
        "#     out = np.float32(np.random.poisson(img * vals) / float(vals))\n",
        "#     noise = out - img\n",
        "#     if gray_noise:\n",
        "#         noise = np.repeat(noise[:, :, np.newaxis], 3, axis=2)\n",
        "#     return noise * scale\n",
        "\n",
        "\n",
        "# def add_poisson_noise(img, scale=1.0, clip=True, rounds=False, gray_noise=False):\n",
        "#     \"\"\"Add poisson noise.\n",
        "\n",
        "#     Args:\n",
        "#         img (Numpy array): Input image, shape (h, w, c), range [0, 1], float32.\n",
        "#         scale (float): Noise scale. Default: 1.0.\n",
        "#         gray_noise (bool): Whether generate gray noise. Default: False.\n",
        "\n",
        "#     Returns:\n",
        "#         (Numpy array): Returned noisy image, shape (h, w, c), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     noise = generate_poisson_noise(img, scale, gray_noise)\n",
        "#     out = img + noise\n",
        "#     if clip and rounds:\n",
        "#         out = np.clip((out * 255.0).round(), 0, 255) / 255.\n",
        "#     elif clip:\n",
        "#         out = np.clip(out, 0, 1)\n",
        "#     elif rounds:\n",
        "#         out = (out * 255.0).round() / 255.\n",
        "#     return out\n",
        "\n",
        "\n",
        "# def generate_poisson_noise_pt(img, scale=1.0, gray_noise=0):\n",
        "#     \"\"\"Generate a batch of poisson noise (PyTorch version)\n",
        "\n",
        "#     Args:\n",
        "#         img (Tensor): Input image, shape (b, c, h, w), range [0, 1], float32.\n",
        "#         scale (float | Tensor): Noise scale. Number or Tensor with shape (b).\n",
        "#             Default: 1.0.\n",
        "#         gray_noise (float | Tensor): 0-1 number or Tensor with shape (b).\n",
        "#             0 for False, 1 for True. Default: 0.\n",
        "\n",
        "#     Returns:\n",
        "#         (Tensor): Returned noisy image, shape (b, c, h, w), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     b, _, h, w = img.size()\n",
        "#     if isinstance(gray_noise, (float, int)):\n",
        "#         cal_gray_noise = gray_noise > 0\n",
        "#     else:\n",
        "#         gray_noise = gray_noise.view(b, 1, 1, 1)\n",
        "#         cal_gray_noise = torch.sum(gray_noise) > 0\n",
        "#     if cal_gray_noise:\n",
        "#         img_gray = rgb_to_grayscale(img, num_output_channels=1)\n",
        "#         # round and clip image for counting vals correctly\n",
        "#         img_gray = torch.clamp((img_gray * 255.0).round(), 0, 255) / 255.\n",
        "#         # use for-loop to get the unique values for each sample\n",
        "#         vals_list = [len(torch.unique(img_gray[i, :, :, :])) for i in range(b)]\n",
        "#         vals_list = [2**np.ceil(np.log2(vals)) for vals in vals_list]\n",
        "#         vals = img_gray.new_tensor(vals_list).view(b, 1, 1, 1)\n",
        "#         out = torch.poisson(img_gray * vals) / vals\n",
        "#         noise_gray = out - img_gray\n",
        "#         noise_gray = noise_gray.expand(b, 3, h, w)\n",
        "\n",
        "#     # always calculate color noise\n",
        "#     # round and clip image for counting vals correctly\n",
        "#     img = torch.clamp((img * 255.0).round(), 0, 255) / 255.\n",
        "#     # use for-loop to get the unique values for each sample\n",
        "#     vals_list = [len(torch.unique(img[i, :, :, :])) for i in range(b)]\n",
        "#     vals_list = [2**np.ceil(np.log2(vals)) for vals in vals_list]\n",
        "#     vals = img.new_tensor(vals_list).view(b, 1, 1, 1)\n",
        "#     out = torch.poisson(img * vals) / vals\n",
        "#     noise = out - img\n",
        "#     if cal_gray_noise:\n",
        "#         noise = noise * (1 - gray_noise) + noise_gray * gray_noise\n",
        "#     if not isinstance(scale, (float, int)):\n",
        "#         scale = scale.view(b, 1, 1, 1)\n",
        "#     return noise * scale\n",
        "\n",
        "\n",
        "# def add_poisson_noise_pt(img, scale=1.0, clip=True, rounds=False, gray_noise=0):\n",
        "#     \"\"\"Add poisson noise to a batch of images (PyTorch version).\n",
        "\n",
        "#     Args:\n",
        "#         img (Tensor): Input image, shape (b, c, h, w), range [0, 1], float32.\n",
        "#         scale (float | Tensor): Noise scale. Number or Tensor with shape (b).\n",
        "#             Default: 1.0.\n",
        "#         gray_noise (float | Tensor): 0-1 number or Tensor with shape (b).\n",
        "#             0 for False, 1 for True. Default: 0.\n",
        "\n",
        "#     Returns:\n",
        "#         (Tensor): Returned noisy image, shape (b, c, h, w), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     noise = generate_poisson_noise_pt(img, scale, gray_noise)\n",
        "#     out = img + noise\n",
        "#     if clip and rounds:\n",
        "#         out = torch.clamp((out * 255.0).round(), 0, 255) / 255.\n",
        "#     elif clip:\n",
        "#         out = torch.clamp(out, 0, 1)\n",
        "#     elif rounds:\n",
        "#         out = (out * 255.0).round() / 255.\n",
        "#     return out\n",
        "\n",
        "\n",
        "# # ----------------------- Random Poisson (Shot) Noise ----------------------- #\n",
        "\n",
        "\n",
        "# def random_generate_poisson_noise(img, scale_range=(0, 1.0), gray_prob=0):\n",
        "#     scale = np.random.uniform(scale_range[0], scale_range[1])\n",
        "#     if np.random.uniform() < gray_prob:\n",
        "#         gray_noise = True\n",
        "#     else:\n",
        "#         gray_noise = False\n",
        "#     return generate_poisson_noise(img, scale, gray_noise)\n",
        "\n",
        "\n",
        "# def random_add_poisson_noise(img, scale_range=(0, 1.0), gray_prob=0, clip=True, rounds=False):\n",
        "#     noise = random_generate_poisson_noise(img, scale_range, gray_prob)\n",
        "#     out = img + noise\n",
        "#     if clip and rounds:\n",
        "#         out = np.clip((out * 255.0).round(), 0, 255) / 255.\n",
        "#     elif clip:\n",
        "#         out = np.clip(out, 0, 1)\n",
        "#     elif rounds:\n",
        "#         out = (out * 255.0).round() / 255.\n",
        "#     return out\n",
        "\n",
        "\n",
        "# def random_generate_poisson_noise_pt(img, scale_range=(0, 1.0), gray_prob=0):\n",
        "#     scale = torch.rand(\n",
        "#         img.size(0), dtype=img.dtype, device=img.device) * (scale_range[1] - scale_range[0]) + scale_range[0]\n",
        "#     gray_noise = torch.rand(img.size(0), dtype=img.dtype, device=img.device)\n",
        "#     gray_noise = (gray_noise < gray_prob).float()\n",
        "#     return generate_poisson_noise_pt(img, scale, gray_noise)\n",
        "\n",
        "\n",
        "# def random_add_poisson_noise_pt(img, scale_range=(0, 1.0), gray_prob=0, clip=True, rounds=False):\n",
        "#     noise = random_generate_poisson_noise_pt(img, scale_range, gray_prob)\n",
        "#     out = img + noise\n",
        "#     if clip and rounds:\n",
        "#         out = torch.clamp((out * 255.0).round(), 0, 255) / 255.\n",
        "#     elif clip:\n",
        "#         out = torch.clamp(out, 0, 1)\n",
        "#     elif rounds:\n",
        "#         out = (out * 255.0).round() / 255.\n",
        "#     return out\n",
        "\n",
        "\n",
        "# # ------------------------------------------------------------------------ #\n",
        "# # --------------------------- JPEG compression --------------------------- #\n",
        "# # ------------------------------------------------------------------------ #\n",
        "\n",
        "\n",
        "# def add_jpg_compression(img, quality=90):\n",
        "#     \"\"\"Add JPG compression artifacts.\n",
        "\n",
        "#     Args:\n",
        "#         img (Numpy array): Input image, shape (h, w, c), range [0, 1], float32.\n",
        "#         quality (float): JPG compression quality. 0 for lowest quality, 100 for\n",
        "#             best quality. Default: 90.\n",
        "\n",
        "#     Returns:\n",
        "#         (Numpy array): Returned image after JPG, shape (h, w, c), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     img = np.clip(img, 0, 1)\n",
        "#     encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), int(quality)]\n",
        "#     _, encimg = cv2.imencode('.jpg', img * 255., encode_param)\n",
        "#     img = np.float32(cv2.imdecode(encimg, 1)) / 255.\n",
        "#     return img\n",
        "\n",
        "\n",
        "# def random_add_jpg_compression(img, quality_range=(90, 100)):\n",
        "#     \"\"\"Randomly add JPG compression artifacts.\n",
        "\n",
        "#     Args:\n",
        "#         img (Numpy array): Input image, shape (h, w, c), range [0, 1], float32.\n",
        "#         quality_range (tuple[float] | list[float]): JPG compression quality\n",
        "#             range. 0 for lowest quality, 100 for best quality.\n",
        "#             Default: (90, 100).\n",
        "\n",
        "#     Returns:\n",
        "#         (Numpy array): Returned image after JPG, shape (h, w, c), range[0, 1],\n",
        "#             float32.\n",
        "#     \"\"\"\n",
        "#     quality = np.random.uniform(quality_range[0], quality_range[1])\n",
        "#     return add_jpg_compression(img, quality)"
      ],
      "metadata": {
        "id": "rv0o-5geE5Ww"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataloader\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "class DatasetBase(Dataset):\n",
        "    def __init__(self,slice_id=0,slice_count=1,use_dist=False,**kwargs):\n",
        "\n",
        "        if use_dist:\n",
        "            slice_id = dist.get_rank()\n",
        "            slice_count = dist.get_world_size()\n",
        "        self.id = slice_id\n",
        "        self.count = slice_count\n",
        "\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1000"
      ],
      "metadata": {
        "id": "t2IW-OhoEa2r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DataLoader\n",
        "import os\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import PIL.Image as Image\n",
        "from torchvision.transforms.functional import  normalize\n",
        "import random\n",
        "import math\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch\n",
        "# from utils import *\n",
        "import cv2\n",
        "\n",
        "def img2tensor(imgs, bgr2rgb=True, float32=True):\n",
        "    \"\"\"Numpy array to tensor.\n",
        "\n",
        "    Args:\n",
        "        imgs (list[ndarray] | ndarray): Input images.\n",
        "        bgr2rgb (bool): Whether to change bgr to rgb.\n",
        "        float32 (bool): Whether to change to float32.\n",
        "\n",
        "    Returns:\n",
        "        list[tensor] | tensor: Tensor images. If returned results only have\n",
        "            one element, just return tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def _totensor(img, bgr2rgb, float32):\n",
        "        if img.shape[2] == 3 and bgr2rgb:\n",
        "            if img.dtype == 'float64':\n",
        "                img = img.astype('float32')\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        img = torch.from_numpy(img.transpose(2, 0, 1))\n",
        "        if float32:\n",
        "            img = img.float()\n",
        "        return img\n",
        "\n",
        "    if isinstance(imgs, list):\n",
        "        return [_totensor(img, bgr2rgb, float32) for img in imgs]\n",
        "    else:\n",
        "        return _totensor(imgs, bgr2rgb, float32)\n",
        "\n",
        "\n",
        "# def augment(imgs, hflip=True, rotation=True, flows=None, return_status=False):\n",
        "#     \"\"\"Augment: horizontal flips OR rotate (0, 90, 180, 270 degrees).\n",
        "\n",
        "#     We use vertical flip and transpose for rotation implementation.\n",
        "#     All the images in the list use the same augmentation.\n",
        "\n",
        "#     Args:\n",
        "#         imgs (list[ndarray] | ndarray): Images to be augmented. If the input\n",
        "#             is an ndarray, it will be transformed to a list.\n",
        "#         hflip (bool): Horizontal flip. Default: True.\n",
        "#         rotation (bool): Ratotation. Default: True.\n",
        "#         flows (list[ndarray]: Flows to be augmented. If the input is an\n",
        "#             ndarray, it will be transformed to a list.\n",
        "#             Dimension is (h, w, 2). Default: None.\n",
        "#         return_status (bool): Return the status of flip and rotation.\n",
        "#             Default: False.\n",
        "\n",
        "#     Returns:\n",
        "#         list[ndarray] | ndarray: Augmented images and flows. If returned\n",
        "#             results only have one element, just return ndarray.\n",
        "\n",
        "#     \"\"\"\n",
        "#     hflip = hflip and random.random() < 0.5\n",
        "#     vflip = rotation and random.random() < 0.5\n",
        "#     rot90 = rotation and random.random() < 0.5\n",
        "\n",
        "#     def _augment(img):\n",
        "#         if hflip:  # horizontal\n",
        "#             cv2.flip(img, 1, img)\n",
        "#         if vflip:  # vertical\n",
        "#             cv2.flip(img, 0, img)\n",
        "#         if rot90:\n",
        "#             img = img.transpose(1, 0, 2)\n",
        "#         return img\n",
        "\n",
        "#     def _augment_flow(flow):\n",
        "#         if hflip:  # horizontal\n",
        "#             cv2.flip(flow, 1, flow)\n",
        "#             flow[:, :, 0] *= -1\n",
        "#         if vflip:  # vertical\n",
        "#             cv2.flip(flow, 0, flow)\n",
        "#             flow[:, :, 1] *= -1\n",
        "#         if rot90:\n",
        "#             flow = flow.transpose(1, 0, 2)\n",
        "#             flow = flow[:, :, [1, 0]]\n",
        "#         return flow\n",
        "\n",
        "#     if not isinstance(imgs, list):\n",
        "#         imgs = [imgs]\n",
        "#     imgs = [_augment(img) for img in imgs]\n",
        "#     if len(imgs) == 1:\n",
        "#         imgs = imgs[0]\n",
        "\n",
        "#     if flows is not None:\n",
        "#         if not isinstance(flows, list):\n",
        "#             flows = [flows]\n",
        "#         flows = [_augment_flow(flow) for flow in flows]\n",
        "#         if len(flows) == 1:\n",
        "#             flows = flows[0]\n",
        "#         return imgs, flows\n",
        "#     else:\n",
        "#         if return_status:\n",
        "#             return imgs, (hflip, vflip, rot90)\n",
        "#         else:\n",
        "#             return imgs\n",
        "\n",
        "class GFPData(DatasetBase):\n",
        "    def __init__(self, slice_id=0, slice_count=1,dist=False, **kwargs):\n",
        "        super().__init__(slice_id, slice_count,dist, **kwargs)\n",
        "        self.eval = kwargs['eval']\n",
        "        self.mean = kwargs['mean']\n",
        "        self.std = kwargs['std']\n",
        "\n",
        "\n",
        "        # # degradation configurations\n",
        "        # self.blur_kernel_size = kwargs['blur_kernel_size']\n",
        "        # self.kernel_list = kwargs['kernel_list']\n",
        "        # self.kernel_prob = kwargs['kernel_prob']\n",
        "        # self.blur_sigma = kwargs['blur_sigma']\n",
        "        # self.downsample_range = kwargs['downsample_range']\n",
        "        # self.noise_range = kwargs['noise_range']\n",
        "        # self.jpeg_range = kwargs['jpeg_range']\n",
        "        # self.use_flip = kwargs['use_flip']\n",
        "\n",
        "        # self.crop_components = kwargs['crop_components']\n",
        "        # self.lmk_base = kwargs['lmk_base']\n",
        "        self.out_size = kwargs['size']\n",
        "        # self.eye_enlarge_ratio = kwargs['eye_enlarge_ratio']\n",
        "        print(\"Keys available in kwargs:\", kwargs.keys())  # Check what keys are in kwargs\n",
        "        hq_root = kwargs.get('hq_root', None)  # Safe access\n",
        "\n",
        "        # hq_root = kwargs['train_hq_root']\n",
        "        self.hq_paths = self.get_img_paths(hq_root)\n",
        "        self.lq_root = kwargs['lq_root']\n",
        "        self.img_len = 0\n",
        "\n",
        "        if self.eval:\n",
        "\n",
        "            dis1 = math.floor(len(self.hq_paths)/self.count)\n",
        "            self.hq_paths = self.hq_paths[self.id*dis1:(self.id+1)*dis1]\n",
        "            random.shuffle(self.hq_paths)\n",
        "\n",
        "\n",
        "        else:\n",
        "            img_root = kwargs['img_root']\n",
        "            self.img_paths = self.get_img_paths(img_root)\n",
        "            dis1 = math.floor(len(self.img_paths)/self.count)\n",
        "            self.img_paths = self.img_paths[self.id*dis1:(self.id+1)*dis1]\n",
        "            random.shuffle(self.img_paths)\n",
        "            self.img_len = len(self.img_paths)\n",
        "\n",
        "        self.hq_len = len(self.hq_paths)\n",
        "        self.length = max(self.img_len,self.hq_len)\n",
        "\n",
        "    def __getitem__(self,i):\n",
        "        if self.eval:\n",
        "\n",
        "            hq_path = self.hq_paths[i%self.hq_len]\n",
        "            lq_path = os.path.join(self.lq_root,os.path.basename(hq_path))\n",
        "            # lq_path = os.path.splitext(lq_path)[0] + '.jpg' # changed here to account for different file types\n",
        "\n",
        "\n",
        "        else:\n",
        "          hq_path = self.img_paths[i%self.img_len]\n",
        "          lq_path = os.path.join(self.lq_root, os.path.basename(hq_path))\n",
        "          # lq_path = os.path.splitext(lq_path)[0] + '.jpg' # changed here to account for different file types\n",
        "\n",
        "\n",
        "     # Check if paths exist\n",
        "        if not os.path.exists(hq_path):\n",
        "            print(f\"Error: HQ path does not exist: {hq_path}\")\n",
        "            # Return a default black image tensor\n",
        "            return torch.zeros((3, self.out_size, self.out_size), dtype=torch.float32), torch.zeros((3, self.out_size, self.out_size), dtype=torch.float32)\n",
        "\n",
        "        if not os.path.exists(lq_path):\n",
        "              print(f\"Error: LQ path does not exist: {lq_path}\")\n",
        "              # Return a default black image tensor\n",
        "              return torch.zeros((3, self.out_size, self.out_size), dtype=torch.float32), torch.zeros((3, self.out_size, self.out_size), dtype=torch.float32)\n",
        "\n",
        "        # Load images, handle potential errors\n",
        "        try:\n",
        "            img_hq = cv2.imread(hq_path).astype(np.float32) / 255.0\n",
        "            img_lq = cv2.imread(lq_path).astype(np.float32) / 255.0\n",
        "\n",
        "            if img_hq is None or img_lq is None:\n",
        "              print(f\"Error loading image(s) at hq_path: {hq_path} or lq_path: {lq_path}\")\n",
        "              return torch.zeros((3, self.out_size, self.out_size), dtype=torch.float32), torch.zeros((3, self.out_size, self.out_size), dtype=torch.float32)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image(s) at hq_path: {hq_path} or lq_path {lq_path} - {e}\")\n",
        "            # Return a default black image tensor\n",
        "            return torch.zeros((3, self.out_size, self.out_size), dtype=torch.float32), torch.zeros((3, self.out_size, self.out_size), dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "    #  # Debugging: Print paths to verify correctness\n",
        "    #     print(f\"HQ Path: {hq_path}, LQ Path: {lq_path}\")\n",
        "\n",
        "    #     # Check if file exists\n",
        "    #     if not os.path.exists(lq_path):\n",
        "    #         print(f\"Error: File not found at {lq_path}\")\n",
        "    #         return None, None\n",
        "\n",
        "\n",
        "\n",
        "    #     img_hq = cv2.imread(hq_path).astype(np.float32) / 255.\n",
        "    #     img_lq = cv2.imread(lq_path)\n",
        "    #     # Check if images loaded correctly\n",
        "    #     if img_hq is None:\n",
        "    #         print(f\"Error: Failed to load HQ image at {hq_path}\")\n",
        "    #         return None, None\n",
        "    #     if img_lq is None:\n",
        "    #         print(f\"Error: Failed to load LQ image at {lq_path}\")\n",
        "    #         return None, None\n",
        "    #     if not os.path.exists(lq_path):\n",
        "    #        print(f\"Warning: Low-quality image not found at {lq_path}, skipping this sample.\")\n",
        "    #        # You might want to handle this case differently, for example, by using a default image.\n",
        "    #        return None,None\n",
        "    #     img_lq = cv2.imread(lq_path)\n",
        "    #     if img_lq is None:\n",
        "    #          print(f\"Warning: Failed to read image at {lq_path}, skipping this sample.\")\n",
        "    #          return None, None\n",
        "    #     img_lq = img_lq.astype(np.float32) / 255.\n",
        "\n",
        "        # # random horizontal flip\n",
        "        # img, status = augment([img_hq,img_lq], hflip=self.use_flip, rotation=False, return_status=True)\n",
        "        # img_hq,img_lq = img\n",
        "        # h, w, _ = img_lq.shape\n",
        "\n",
        "        # # get facial component coordinates\n",
        "        # loc_left_eye, loc_right_eye, loc_mouth = 0,0,0\n",
        "        # if self.crop_components:\n",
        "        #     locations = self.get_component_coordinates(hq_path, status)\n",
        "        #     loc_left_eye, loc_right_eye, loc_mouth = locations\n",
        "\n",
        "        # ------------------------ generate lq image ------------------------ #\n",
        "        # blur\n",
        "        # if not self.eval:\n",
        "        #     kernel = degradations.random_mixed_kernels(\n",
        "        #         self.kernel_list,\n",
        "        #         self.kernel_prob,\n",
        "        #         self.blur_kernel_size,\n",
        "        #         self.blur_sigma,\n",
        "        #         self.blur_sigma, [-math.pi, math.pi],\n",
        "        #         noise_range=None)\n",
        "\n",
        "        #     img_lq = cv2.filter2D(img_lq, -1, kernel)\n",
        "        #     # downsample\n",
        "        #     scale = np.random.uniform(self.downsample_range[0], self.downsample_range[1])\n",
        "        #     img_lq = cv2.resize(img_lq, (int(w // scale), int(h // scale)), interpolation=cv2.INTER_LINEAR)\n",
        "        #     # # noise\n",
        "        #     if self.noise_range is not None:\n",
        "        #         img_lq = degradations.random_add_gaussian_noise(img_lq, self.noise_range)\n",
        "        #     # jpeg compression\n",
        "        #     if self.jpeg_range is not None:\n",
        "        #         img_lq = degradations.random_add_jpg_compression(img_lq, self.jpeg_range)\n",
        "\n",
        "        # resize to original size\n",
        "        img_lq = cv2.resize(img_lq, (512, 512), interpolation=cv2.INTER_LINEAR)\n",
        "        img_hq = cv2.resize(img_hq, (1024, 1024))\n",
        "\n",
        "        # BGR to RGB, HWC to CHW, numpy to tensor\n",
        "        img_hq, img_lq = img2tensor([img_hq, img_lq], bgr2rgb=True, float32=True)\n",
        "\n",
        "        # # Convert images to tensors (new tensor conversion feb 25)\n",
        "        # img_hq = torch.from_numpy(img_hq.transpose(2, 0, 1)).float()\n",
        "        # img_lq = torch.from_numpy(img_lq.transpose(2, 0, 1)).float()\n",
        "\n",
        "\n",
        "\n",
        "        # round and clip\n",
        "        img_lq = torch.clamp((img_lq * 255.0).round(), 0, 255) / 255.\n",
        "\n",
        "        # normalize\n",
        "        img_hq = normalize(img_hq, self.mean, self.std, inplace=True)\n",
        "        img_lq = normalize(img_lq, self.mean, self.std, inplace=True)\n",
        "\n",
        "\n",
        "        return img_lq, img_hq\n",
        "\n",
        "    # def get_img_paths(self,root):\n",
        "    #     return [os.path.join(root, f) for f in os.listdir(root) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "#updted get_img_paths()\n",
        "    def get_img_paths(self, root):\n",
        "            \"\"\"Get paths of all images in the directory.\"\"\"\n",
        "            if root is None:\n",
        "                print(f\"Warning: Image root path is None\")\n",
        "                return []\n",
        "            if not os.path.exists(root):\n",
        "                print(f\"Warning: Image root path does not exist: {root}\")\n",
        "                return []\n",
        "            return [os.path.join(root, f) for f in os.listdir(root) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    # def get_component_coordinates(self, hq_path, status):\n",
        "    #     \"\"\"Get facial component (left_eye, right_eye, mouth) coordinates from a pre-loaded pth file\"\"\"\n",
        "    #     name = os.path.splitext(os.path.basename(hq_path))[0]\n",
        "    #     info_path = os.path.join(self.lmk_base,name+'.npy')\n",
        "    #     components_bbox = np.load(info_path,allow_pickle=True).item()\n",
        "    #     if status[0]:  # hflip\n",
        "    #         # exchange right and left eye\n",
        "    #         tmp = components_bbox['left_eye']\n",
        "    #         components_bbox['left_eye'] = components_bbox['right_eye']\n",
        "    #         components_bbox['right_eye'] = tmp\n",
        "    #         # modify the width coordinate\n",
        "    #         components_bbox['left_eye'][0] = self.out_size - components_bbox['left_eye'][0]\n",
        "    #         components_bbox['right_eye'][0] = self.out_size - components_bbox['right_eye'][0]\n",
        "    #         components_bbox['mouth'][0] = self.out_size - components_bbox['mouth'][0]\n",
        "\n",
        "    #     # get coordinates\n",
        "    #     locations = []\n",
        "    #     for part in ['left_eye', 'right_eye', 'mouth']:\n",
        "    #         mean = components_bbox[part][0:2]\n",
        "    #         half_len = components_bbox[part][2] / 2.\n",
        "    #         if 'eye' in part:\n",
        "    #             half_len *= self.eye_enlarge_ratio\n",
        "    #         loc = np.hstack((mean - half_len + 1, mean + half_len))\n",
        "    #         loc = torch.from_numpy(loc).float()\n",
        "    #         locations.append(loc)\n",
        "    #     return locations\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.eval:\n",
        "            return min(self.length,1000)\n",
        "            # return 1\n",
        "        else:\n",
        "            return self.length\n",
        "            # return 100"
      ],
      "metadata": {
        "id": "-LCcyPfP1G7K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architecture goes here!!!  (added-on: 16/feb/2025 5:30PM)\n",
        "\n"
      ],
      "metadata": {
        "id": "yGyEJ5DVq0wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#fused activations\n",
        "\n",
        "class FusedLeakyReLU(nn.Module):\n",
        "\n",
        "    def __init__(self, channel, negative_slope=0.2, scale=2**0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(channel))\n",
        "        self.negative_slope = negative_slope\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(self, input):\n",
        "        return fused_leaky_relu(input, self.bias, self.negative_slope, self.scale)\n",
        "\n",
        "def fused_leaky_relu(input, bias=None, negative_slope=0.2, scale=2 ** 0.5):\n",
        "    if bias is not None:\n",
        "        rest_dim = [1] * (input.ndim - bias.ndim - 1)\n",
        "        return (\n",
        "            F.leaky_relu(\n",
        "                input + bias.view(1, bias.shape[0], *rest_dim), negative_slope=0.2\n",
        "            )\n",
        "            * scale\n",
        "        )\n",
        "\n",
        "class ScaledLeakyReLU(nn.Module):\n",
        "    \"\"\"Scaled LeakyReLU.\n",
        "\n",
        "    Args:\n",
        "        negative_slope (float): Negative slope. Default: 0.2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, negative_slope=0.2):\n",
        "        super(ScaledLeakyReLU, self).__init__()\n",
        "        self.negative_slope = negative_slope\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.leaky_relu(x, negative_slope=self.negative_slope)\n",
        "        return out * math.sqrt(2)"
      ],
      "metadata": {
        "id": "pk7-wqGrg9Vo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#upfirdn2d\n",
        "\n",
        "def upfirdn2d(input, kernel, up=1, down=1, pad=(0, 0)):\n",
        "    out = upfirdn2d_native(input, kernel, up, up, down, down, pad[0], pad[1], pad[0], pad[1])\n",
        "    return out\n",
        "\n",
        "def upfirdn2d_native(input, kernel, up_x, up_y, down_x, down_y, pad_x0, pad_x1, pad_y0, pad_y1):\n",
        "    _, channel, in_h, in_w = input.shape\n",
        "    input = input.reshape(-1, in_h, in_w, 1)\n",
        "\n",
        "    _, in_h, in_w, minor = input.shape\n",
        "    kernel_h, kernel_w = kernel.shape\n",
        "\n",
        "    out = input.view(-1, in_h, 1, in_w, 1, minor)\n",
        "    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])\n",
        "    out = out.view(-1, in_h * up_y, in_w * up_x, minor)\n",
        "\n",
        "    out = F.pad(out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)])\n",
        "    out = out[:, max(-pad_y0, 0):out.shape[1] - max(-pad_y1, 0), max(-pad_x0, 0):out.shape[2] - max(-pad_x1, 0), :, ]\n",
        "\n",
        "    out = out.permute(0, 3, 1, 2)\n",
        "    out = out.reshape([-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1])\n",
        "    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)\n",
        "    out = F.conv2d(out, w)\n",
        "    out = out.reshape(\n",
        "        -1,\n",
        "        minor,\n",
        "        in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1,\n",
        "        in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1,\n",
        "    )\n",
        "    out = out.permute(0, 2, 3, 1)\n",
        "    out = out[:, ::down_y, ::down_x, :]\n",
        "\n",
        "    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1\n",
        "    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1\n",
        "\n",
        "    return out.view(-1, channel, out_h, out_w)"
      ],
      "metadata": {
        "id": "v5MptG7IhI24"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#module.py\n",
        "\n",
        "from torch.nn import init as init\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "# from .ops.upfirdn2d import upfirdn2d\n",
        "# from .ops.fused_act import FusedLeakyReLU,ScaledLeakyReLU,fused_leaky_relu\n",
        "\n",
        "def default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
        "    \"\"\"Initialize network weights.\n",
        "\n",
        "    Args:\n",
        "        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
        "        scale (float): Scale initialized weights, especially for residual\n",
        "            blocks. Default: 1.\n",
        "        bias_fill (float): The value to fill bias. Default: 0\n",
        "        kwargs (dict): Other arguments for initialization function.\n",
        "    \"\"\"\n",
        "    if not isinstance(module_list, list):\n",
        "        module_list = [module_list]\n",
        "    for module in module_list:\n",
        "        for m in module.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, **kwargs)\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.kaiming_normal_(m.weight, **kwargs)\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "            elif isinstance(m, _BatchNorm):\n",
        "                init.constant_(m.weight, 1)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "class NormStyleCode(nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Normalize the style codes.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Style codes with shape (b, c).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Normalized tensor.\n",
        "        \"\"\"\n",
        "        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n",
        "\n",
        "\n",
        "class ModulatedConv2d(nn.Module):\n",
        "    \"\"\"Modulated Conv2d used in StyleGAN2.\n",
        "\n",
        "    There is no bias in ModulatedConv2d.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Channel number of the input.\n",
        "        out_channels (int): Channel number of the output.\n",
        "        kernel_size (int): Size of the convolving kernel.\n",
        "        num_style_feat (int): Channel number of style features.\n",
        "        demodulate (bool): Whether to demodulate in the conv layer. Default: True.\n",
        "        sample_mode (str | None): Indicating 'upsample', 'downsample' or None. Default: None.\n",
        "        eps (float): A value added to the denominator for numerical stability. Default: 1e-8.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 num_style_feat,\n",
        "                 demodulate=True,\n",
        "                 sample_mode=None,\n",
        "                 eps=1e-8):\n",
        "        super(ModulatedConv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.demodulate = demodulate\n",
        "        self.sample_mode = sample_mode\n",
        "        self.eps = eps\n",
        "\n",
        "        # modulation inside each modulated conv\n",
        "        self.modulation = nn.Linear(num_style_feat, in_channels, bias=True)\n",
        "        # initialization\n",
        "        default_init_weights(self.modulation, scale=1, bias_fill=1, a=0, mode='fan_in', nonlinearity='linear')\n",
        "\n",
        "        self.weight = nn.Parameter(\n",
        "            torch.randn(1, out_channels, in_channels, kernel_size, kernel_size) /\n",
        "            math.sqrt(in_channels * kernel_size**2))\n",
        "        self.padding = kernel_size // 2\n",
        "\n",
        "    def forward(self, x, style):\n",
        "        \"\"\"Forward function.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Tensor with shape (b, c, h, w).\n",
        "            style (Tensor): Tensor with shape (b, num_style_feat).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Modulated tensor after convolution.\n",
        "        \"\"\"\n",
        "        b, c, h, w = x.shape  # c = c_in\n",
        "        # weight modulation\n",
        "        style = self.modulation(style).view(b, 1, c, 1, 1)\n",
        "        # self.weight: (1, c_out, c_in, k, k); style: (b, 1, c, 1, 1)\n",
        "        weight = self.weight * style  # (b, c_out, c_in, k, k)\n",
        "\n",
        "        if self.demodulate:\n",
        "            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + self.eps)\n",
        "            weight = weight * demod.view(b, self.out_channels, 1, 1, 1)\n",
        "\n",
        "        weight = weight.view(b * self.out_channels, c, self.kernel_size, self.kernel_size)\n",
        "\n",
        "        # upsample or downsample if necessary\n",
        "        if self.sample_mode == 'upsample':\n",
        "            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        elif self.sample_mode == 'downsample':\n",
        "            x = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n",
        "\n",
        "        b, c, h, w = x.shape\n",
        "        x = x.view(1, b * c, h, w)\n",
        "        # weight: (b*c_out, c_in, k, k), groups=b\n",
        "        out = F.conv2d(x, weight, padding=self.padding, groups=b)\n",
        "        out = out.view(b, self.out_channels, *out.shape[2:4])\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f'{self.__class__.__name__}(in_channels={self.in_channels}, out_channels={self.out_channels}, '\n",
        "                f'kernel_size={self.kernel_size}, demodulate={self.demodulate}, sample_mode={self.sample_mode})')\n",
        "\n",
        "\n",
        "class StyleConv(nn.Module):\n",
        "    \"\"\"Style conv used in StyleGAN2.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Channel number of the input.\n",
        "        out_channels (int): Channel number of the output.\n",
        "        kernel_size (int): Size of the convolving kernel.\n",
        "        num_style_feat (int): Channel number of style features.\n",
        "        demodulate (bool): Whether demodulate in the conv layer. Default: True.\n",
        "        sample_mode (str | None): Indicating 'upsample', 'downsample' or None. Default: None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, num_style_feat, demodulate=True, sample_mode=None):\n",
        "        super(StyleConv, self).__init__()\n",
        "        self.modulated_conv = ModulatedConv2d(\n",
        "            in_channels, out_channels, kernel_size, num_style_feat, demodulate=demodulate, sample_mode=sample_mode)\n",
        "        self.weight = nn.Parameter(torch.zeros(1))  # for noise injection\n",
        "        self.bias = nn.Parameter(torch.zeros(1, out_channels, 1, 1))\n",
        "        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
        "\n",
        "    def forward(self, x, style, noise=None):\n",
        "        # modulate\n",
        "        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n",
        "        # noise injection\n",
        "        if noise is None:\n",
        "            b, _, h, w = out.shape\n",
        "            noise = out.new_empty(b, 1, h, w).normal_()\n",
        "        out = out + self.weight * noise\n",
        "        # add bias\n",
        "        out = out + self.bias\n",
        "        # activation\n",
        "        out = self.activate(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ToRGB(nn.Module):\n",
        "    \"\"\"To RGB (image space) from features.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Channel number of input.\n",
        "        num_style_feat (int): Channel number of style features.\n",
        "        upsample (bool): Whether to upsample. Default: True.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, num_style_feat, upsample=True):\n",
        "        super(ToRGB, self).__init__()\n",
        "        self.upsample = upsample\n",
        "        self.modulated_conv = ModulatedConv2d(\n",
        "            in_channels, 3, kernel_size=1, num_style_feat=num_style_feat, demodulate=False, sample_mode=None)\n",
        "        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, x, style, skip=None):\n",
        "        \"\"\"Forward function.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Feature tensor with shape (b, c, h, w).\n",
        "            style (Tensor): Tensor with shape (b, num_style_feat).\n",
        "            skip (Tensor): Base/skip tensor. Default: None.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: RGB images.\n",
        "        \"\"\"\n",
        "        out = self.modulated_conv(x, style)\n",
        "        out = out + self.bias\n",
        "        if skip is not None:\n",
        "            if self.upsample:\n",
        "                skip = F.interpolate(skip, scale_factor=2, mode='bilinear', align_corners=False)\n",
        "            out = out + skip\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConstantInput(nn.Module):\n",
        "    \"\"\"Constant input.\n",
        "\n",
        "    Args:\n",
        "        num_channel (int): Channel number of constant input.\n",
        "        size (int): Spatial size of constant input.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_channel, size):\n",
        "        super(ConstantInput, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(1, num_channel, size, size))\n",
        "\n",
        "    def forward(self, batch):\n",
        "        out = self.weight.repeat(batch, 1, 1, 1)\n",
        "        return out\n",
        "\n",
        "\n",
        "class StyleGAN2GeneratorClean(nn.Module):\n",
        "    \"\"\"Clean version of StyleGAN2 Generator.\n",
        "\n",
        "    Args:\n",
        "        out_size (int): The spatial size of outputs.\n",
        "        num_style_feat (int): Channel number of style features. Default: 512.\n",
        "        num_mlp (int): Layer number of MLP style layers. Default: 8.\n",
        "        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n",
        "        narrow (float): Narrow ratio for channels. Default: 1.0.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_size, num_style_feat=512, num_mlp=8, channel_multiplier=2, narrow=1):\n",
        "        super(StyleGAN2GeneratorClean, self).__init__()\n",
        "        # Style MLP layers\n",
        "        self.num_style_feat = num_style_feat\n",
        "        style_mlp_layers = [NormStyleCode()]\n",
        "        for i in range(num_mlp):\n",
        "            style_mlp_layers.extend(\n",
        "                [nn.Linear(num_style_feat, num_style_feat, bias=True),\n",
        "                 nn.LeakyReLU(negative_slope=0.2, inplace=True)])\n",
        "        self.style_mlp = nn.Sequential(*style_mlp_layers)\n",
        "        # initialization\n",
        "        default_init_weights(self.style_mlp, scale=1, bias_fill=0, a=0.2, mode='fan_in', nonlinearity='leaky_relu')\n",
        "\n",
        "        # channel list\n",
        "        channels = {\n",
        "            '4': int(512 * narrow),\n",
        "            '8': int(512 * narrow),\n",
        "            '16': int(512 * narrow),\n",
        "            '32': int(512 * narrow),\n",
        "            '64': int(256 * channel_multiplier * narrow),\n",
        "            '128': int(128 * channel_multiplier * narrow),\n",
        "            '256': int(64 * channel_multiplier * narrow),\n",
        "            '512': int(32 * channel_multiplier * narrow),\n",
        "            '1024': int(16 * channel_multiplier * narrow)\n",
        "        }\n",
        "        self.channels = channels\n",
        "\n",
        "        self.constant_input = ConstantInput(channels['4'], size=4)\n",
        "        self.style_conv1 = StyleConv(\n",
        "            channels['4'],\n",
        "            channels['4'],\n",
        "            kernel_size=3,\n",
        "            num_style_feat=num_style_feat,\n",
        "            demodulate=True,\n",
        "            sample_mode=None)\n",
        "        self.to_rgb1 = ToRGB(channels['4'], num_style_feat, upsample=False)\n",
        "\n",
        "        self.log_size = int(math.log(out_size, 2))\n",
        "        self.num_layers = (self.log_size - 2) * 2 + 1\n",
        "        self.num_latent = self.log_size * 2 - 2\n",
        "\n",
        "        self.style_convs = nn.ModuleList()\n",
        "        self.to_rgbs = nn.ModuleList()\n",
        "        self.noises = nn.Module()\n",
        "\n",
        "        in_channels = channels['4']\n",
        "        # noise\n",
        "        for layer_idx in range(self.num_layers):\n",
        "            resolution = 2**((layer_idx + 5) // 2)\n",
        "            shape = [1, 1, resolution, resolution]\n",
        "            self.noises.register_buffer(f'noise{layer_idx}', torch.randn(*shape))\n",
        "        # style convs and to_rgbs\n",
        "        for i in range(3, self.log_size + 1):\n",
        "            out_channels = channels[f'{2**i}']\n",
        "            self.style_convs.append(\n",
        "                StyleConv(\n",
        "                    in_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=3,\n",
        "                    num_style_feat=num_style_feat,\n",
        "                    demodulate=True,\n",
        "                    sample_mode='upsample'))\n",
        "            self.style_convs.append(\n",
        "                StyleConv(\n",
        "                    out_channels,\n",
        "                    out_channels,\n",
        "                    kernel_size=3,\n",
        "                    num_style_feat=num_style_feat,\n",
        "                    demodulate=True,\n",
        "                    sample_mode=None))\n",
        "            self.to_rgbs.append(ToRGB(out_channels, num_style_feat, upsample=True))\n",
        "            in_channels = out_channels\n",
        "\n",
        "    def make_noise(self):\n",
        "        \"\"\"Make noise for noise injection.\"\"\"\n",
        "        device = self.constant_input.weight.device\n",
        "        noises = [torch.randn(1, 1, 4, 4, device=device)]\n",
        "\n",
        "        for i in range(3, self.log_size + 1):\n",
        "            for _ in range(2):\n",
        "                noises.append(torch.randn(1, 1, 2**i, 2**i, device=device))\n",
        "\n",
        "        return noises\n",
        "\n",
        "    def get_latent(self, x):\n",
        "        return self.style_mlp(x)\n",
        "\n",
        "    def mean_latent(self, num_latent):\n",
        "        latent_in = torch.randn(num_latent, self.num_style_feat, device=self.constant_input.weight.device)\n",
        "        latent = self.style_mlp(latent_in).mean(0, keepdim=True)\n",
        "        return latent\n",
        "\n",
        "    def forward(self,\n",
        "                styles,\n",
        "                input_is_latent=False,\n",
        "                noise=None,\n",
        "                randomize_noise=True,\n",
        "                truncation=1,\n",
        "                truncation_latent=None,\n",
        "                inject_index=None,\n",
        "                return_latents=False):\n",
        "        \"\"\"Forward function for StyleGAN2GeneratorClean.\n",
        "\n",
        "        Args:\n",
        "            styles (list[Tensor]): Sample codes of styles.\n",
        "            input_is_latent (bool): Whether input is latent style. Default: False.\n",
        "            noise (Tensor | None): Input noise or None. Default: None.\n",
        "            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n",
        "            truncation (float): The truncation ratio. Default: 1.\n",
        "            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n",
        "            inject_index (int | None): The injection index for mixing noise. Default: None.\n",
        "            return_latents (bool): Whether to return style latents. Default: False.\n",
        "        \"\"\"\n",
        "        # style codes -> latents with Style MLP layer\n",
        "        if not input_is_latent:\n",
        "            styles = [self.style_mlp(s) for s in styles]\n",
        "        # noises\n",
        "        if noise is None:\n",
        "            if randomize_noise:\n",
        "                noise = [None] * self.num_layers  # for each style conv layer\n",
        "            else:  # use the stored noise\n",
        "                noise = [getattr(self.noises, f'noise{i}') for i in range(self.num_layers)]\n",
        "        # style truncation\n",
        "        if truncation < 1:\n",
        "            style_truncation = []\n",
        "            for style in styles:\n",
        "                style_truncation.append(truncation_latent + truncation * (style - truncation_latent))\n",
        "            styles = style_truncation\n",
        "        # get style latents with injection\n",
        "        if len(styles) == 1:\n",
        "            inject_index = self.num_latent\n",
        "\n",
        "            if styles[0].ndim < 3:\n",
        "                # repeat latent code for all the layers\n",
        "                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
        "            else:  # used for encoder with different latent code for each layer\n",
        "                latent = styles[0]\n",
        "        elif len(styles) == 2:  # mixing noises\n",
        "            if inject_index is None:\n",
        "                inject_index = random.randint(1, self.num_latent - 1)\n",
        "            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
        "            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n",
        "            latent = torch.cat([latent1, latent2], 1)\n",
        "\n",
        "        # main generation\n",
        "        out = self.constant_input(latent.shape[0])\n",
        "        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n",
        "        skip = self.to_rgb1(out, latent[:, 1])\n",
        "\n",
        "        i = 1\n",
        "        for conv1, conv2, noise1, noise2, to_rgb in zip(self.style_convs[::2], self.style_convs[1::2], noise[1::2],\n",
        "                                                        noise[2::2], self.to_rgbs):\n",
        "            out = conv1(out, latent[:, i], noise=noise1)\n",
        "            out = conv2(out, latent[:, i + 1], noise=noise2)\n",
        "            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n",
        "            i += 2\n",
        "\n",
        "        image = skip\n",
        "\n",
        "        if return_latents:\n",
        "            return image, latent\n",
        "        else:\n",
        "            return image, None\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"Residual block with bilinear upsampling/downsampling.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Channel number of the input.\n",
        "        out_channels (int): Channel number of the output.\n",
        "        mode (str): Upsampling/downsampling mode. Options: down | up. Default: down.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mode='down'):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n",
        "        self.skip = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
        "        if mode == 'down':\n",
        "            self.scale_factor = 0.5  #the input image is downsampled to half its size.\n",
        "        elif mode == 'up':\n",
        "            self.scale_factor = 2  #the input image is upsampled to double its size.\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.leaky_relu_(self.conv1(x), negative_slope=0.2)\n",
        "        # upsample/downsample\n",
        "        out = F.interpolate(out, scale_factor=self.scale_factor, mode='bilinear', align_corners=False)  #bilinear interpolation is done as per scale_factor\n",
        "        out = F.leaky_relu_(self.conv2(out), negative_slope=0.2)\n",
        "        # skip\n",
        "        x = F.interpolate(x, scale_factor=self.scale_factor, mode='bilinear', align_corners=False)\n",
        "        skip = self.skip(x)  # the original input x is also upsampled/downsampled to match the size of the output. This transformed input is passed through the self.skip convolution to adjust the number of channels, and then it is added to the output of conv2.\n",
        "        out = out + skip\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class ConvLayer(nn.Sequential):\n",
        "    \"\"\"Conv Layer used in StyleGAN2 Discriminator.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Channel number of the input.\n",
        "        out_channels (int): Channel number of the output.\n",
        "        kernel_size (int): Kernel size.\n",
        "        downsample (bool): Whether downsample by a factor of 2.\n",
        "            Default: False.\n",
        "        resample_kernel (list[int]): A list indicating the 1D resample\n",
        "            kernel magnitude. A cross production will be applied to\n",
        "            extent 1D resample kernel to 2D resample kernel.\n",
        "            Default: (1, 3, 3, 1).\n",
        "        bias (bool): Whether with bias. Default: True.\n",
        "        activate (bool): Whether use activateion. Default: True.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size,\n",
        "                 downsample=False,\n",
        "                 resample_kernel=(1, 3, 3, 1),\n",
        "                 bias=True,\n",
        "                 activate=True):\n",
        "        layers = []\n",
        "        # downsample\n",
        "        if downsample:\n",
        "            layers.append(\n",
        "                UpFirDnSmooth(resample_kernel, upsample_factor=1, downsample_factor=2, kernel_size=kernel_size))\n",
        "            stride = 2\n",
        "            self.padding = 0\n",
        "        else:\n",
        "            stride = 1\n",
        "            self.padding = kernel_size // 2\n",
        "        # conv\n",
        "        layers.append(\n",
        "            EqualConv2d(\n",
        "                in_channels, out_channels, kernel_size, stride=stride, padding=self.padding, bias=bias\n",
        "                and not activate))\n",
        "        # activation\n",
        "        if activate:\n",
        "            if bias:\n",
        "                layers.append(FusedLeakyReLU(out_channels))\n",
        "            else:\n",
        "                layers.append(ScaledLeakyReLU(0.2))\n",
        "\n",
        "        super(ConvLayer, self).__init__(*layers)\n",
        "\n",
        "def make_resample_kernel(k):\n",
        "    \"\"\"Make resampling kernel for UpFirDn.\n",
        "\n",
        "    Args:\n",
        "        k (list[int]): A list indicating the 1D resample kernel magnitude.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: 2D resampled kernel.\n",
        "    \"\"\"\n",
        "    k = torch.tensor(k, dtype=torch.float32)\n",
        "    if k.ndim == 1:\n",
        "        k = k[None, :] * k[:, None]  # to 2D kernel, outer product\n",
        "    # normalize\n",
        "    k /= k.sum()\n",
        "    return k\n",
        "\n",
        "\n",
        "class UpFirDnSmooth(nn.Module):\n",
        "    \"\"\"Upsample, FIR filter, and downsample (smooth version).\n",
        "\n",
        "    Args:\n",
        "        resample_kernel (list[int]): A list indicating the 1D resample kernel\n",
        "            magnitude.\n",
        "        upsample_factor (int): Upsampling scale factor. Default: 1.\n",
        "        downsample_factor (int): Downsampling scale factor. Default: 1.\n",
        "        kernel_size (int): Kernel size: Default: 1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, resample_kernel, upsample_factor=1, downsample_factor=1, kernel_size=1):\n",
        "        super(UpFirDnSmooth, self).__init__()\n",
        "        self.upsample_factor = upsample_factor\n",
        "        self.downsample_factor = downsample_factor\n",
        "        self.kernel = make_resample_kernel(resample_kernel)\n",
        "        if upsample_factor > 1:\n",
        "            self.kernel = self.kernel * (upsample_factor**2)\n",
        "\n",
        "        if upsample_factor > 1:\n",
        "            pad = (self.kernel.shape[0] - upsample_factor) - (kernel_size - 1)\n",
        "            self.pad = ((pad + 1) // 2 + upsample_factor - 1, pad // 2 + 1)\n",
        "        elif downsample_factor > 1:\n",
        "            pad = (self.kernel.shape[0] - downsample_factor) + (kernel_size - 1)\n",
        "            self.pad = ((pad + 1) // 2, pad // 2)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = upfirdn2d(x, self.kernel.type_as(x), up=1, down=1, pad=self.pad)\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f'{self.__class__.__name__}(upsample_factor={self.upsample_factor}'\n",
        "                f', downsample_factor={self.downsample_factor})')\n",
        "\n",
        "\n",
        "class EqualConv2d(nn.Module):\n",
        "    \"\"\"Equalized Linear as StyleGAN2.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Channel number of the input.\n",
        "        out_channels (int): Channel number of the output.\n",
        "        kernel_size (int): Size of the convolving kernel.\n",
        "        stride (int): Stride of the convolution. Default: 1\n",
        "        padding (int): Zero-padding added to both sides of the input.\n",
        "            Default: 0.\n",
        "        bias (bool): If ``True``, adds a learnable bias to the output.\n",
        "            Default: ``True``.\n",
        "        bias_init_val (float): Bias initialized value. Default: 0.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, bias_init_val=0):\n",
        "        super(EqualConv2d, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.conv2d(\n",
        "            x,\n",
        "            self.weight * self.scale,\n",
        "            bias=self.bias,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding,\n",
        "        )\n",
        "\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f'{self.__class__.__name__}(in_channels={self.in_channels}, '\n",
        "                f'out_channels={self.out_channels}, '\n",
        "                f'kernel_size={self.kernel_size},'\n",
        "                f' stride={self.stride}, padding={self.padding}, '\n",
        "                f'bias={self.bias is not None})')\n",
        "\n",
        "\n",
        "class EqualLinear(nn.Module):\n",
        "    \"\"\"Equalized Linear as StyleGAN2.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Size of each sample.\n",
        "        out_channels (int): Size of each output sample.\n",
        "        bias (bool): If set to ``False``, the layer will not learn an additive\n",
        "            bias. Default: ``True``.\n",
        "        bias_init_val (float): Bias initialized value. Default: 0.\n",
        "        lr_mul (float): Learning rate multiplier. Default: 1.\n",
        "        activation (None | str): The activation after ``linear`` operation.\n",
        "            Supported: 'fused_lrelu', None. Default: None.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bias=True, bias_init_val=0, lr_mul=1, activation=None):\n",
        "        super(EqualLinear, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.lr_mul = lr_mul\n",
        "        self.activation = activation\n",
        "        if self.activation not in ['fused_lrelu', None]:\n",
        "            raise ValueError(f'Wrong activation value in EqualLinear: {activation}'\n",
        "                             \"Supported ones are: ['fused_lrelu', None].\")\n",
        "        self.scale = (1 / math.sqrt(in_channels)) * lr_mul\n",
        "\n",
        "        self.weight = nn.Parameter(torch.randn(out_channels, in_channels).div_(lr_mul))\n",
        "        if bias:\n",
        "            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.bias is None:\n",
        "            bias = None\n",
        "        else:\n",
        "            bias = self.bias * self.lr_mul\n",
        "        if self.activation == 'fused_lrelu':\n",
        "            out = F.linear(x, self.weight * self.scale)\n",
        "            out = fused_leaky_relu(out, bias)\n",
        "        else:\n",
        "            out = F.linear(x, self.weight * self.scale, bias=bias)\n",
        "        return out\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f'{self.__class__.__name__}(in_channels={self.in_channels}, '\n",
        "                f'out_channels={self.out_channels}, bias={self.bias is not None})')\n",
        "\n",
        "class ResBlock2(nn.Module):\n",
        "    \"\"\"Residual block used in StyleGAN2 Discriminator.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Channel number of the input.\n",
        "        out_channels (int): Channel number of the output.\n",
        "        resample_kernel (list[int]): A list indicating the 1D resample\n",
        "            kernel magnitude. A cross production will be applied to\n",
        "            extent 1D resample kernel to 2D resample kernel.\n",
        "            Default: (1, 3, 3, 1).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, resample_kernel=(1, 3, 3, 1)):\n",
        "        super(ResBlock2, self).__init__()\n",
        "\n",
        "        self.conv1 = ConvLayer(in_channels, in_channels, 3, bias=True, activate=True)\n",
        "        self.conv2 = ConvLayer(\n",
        "            in_channels, out_channels, 3, downsample=True, resample_kernel=resample_kernel, bias=True, activate=True)\n",
        "        self.skip = ConvLayer(\n",
        "            in_channels, out_channels, 1, downsample=True, resample_kernel=resample_kernel, bias=False, activate=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        skip = self.skip(x)\n",
        "        out = (out + skip) / math.sqrt(2)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "XIihqbh2gyML"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generator.py\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "# from .module import StyleGAN2GeneratorClean,StyleConv,ToRGB,ResBlock\n",
        "\n",
        "class StyleGAN2GeneratorCSFT(StyleGAN2GeneratorClean):\n",
        "    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n",
        "\n",
        "    It is the clean version without custom compiled CUDA extensions used in StyleGAN2.\n",
        "\n",
        "    Args:\n",
        "        out_size (int): The spatial size of outputs.\n",
        "        num_style_feat (int): Channel number of style features. Default: 512.\n",
        "        num_mlp (int): Layer number of MLP style layers. Default: 8.\n",
        "        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n",
        "        narrow (float): The narrow ratio for channels. Default: 1.\n",
        "        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_size, num_style_feat=512, num_mlp=8, channel_multiplier=2, narrow=1, sft_half=False):\n",
        "        super(StyleGAN2GeneratorCSFT, self).__init__(\n",
        "            out_size,\n",
        "            num_style_feat=num_style_feat,\n",
        "            num_mlp=num_mlp,\n",
        "            channel_multiplier=channel_multiplier,\n",
        "            narrow=narrow)\n",
        "        self.sft_half = sft_half\n",
        "\n",
        "        # -----up 1024-----------\n",
        "        self.final_conv1 = \\\n",
        "                StyleConv(\n",
        "                    64,\n",
        "                    16,\n",
        "                    kernel_size=3,\n",
        "                    num_style_feat=num_style_feat,\n",
        "                    demodulate=True,\n",
        "                    sample_mode='upsample')\n",
        "        self.final_conv2 = \\\n",
        "            StyleConv(\n",
        "                16,\n",
        "                16,\n",
        "                kernel_size=3,\n",
        "                num_style_feat=num_style_feat,\n",
        "                demodulate=True,\n",
        "                sample_mode=None)\n",
        "        self.final_rgb = ToRGB(16, num_style_feat, upsample=True)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                styles,\n",
        "                conditions,\n",
        "                input_is_latent=False,\n",
        "                noise=None,\n",
        "                randomize_noise=True,\n",
        "                truncation=1,\n",
        "                truncation_latent=None,\n",
        "                inject_index=None,\n",
        "                return_latents=False):\n",
        "        \"\"\"Forward function for StyleGAN2GeneratorCSFT.\n",
        "\n",
        "        Args:\n",
        "            styles (list[Tensor]): Sample codes of styles.\n",
        "            conditions (list[Tensor]): SFT conditions to generators.\n",
        "            input_is_latent (bool): Whether input is latent style. Default: False.\n",
        "            noise (Tensor | None): Input noise or None. Default: None.\n",
        "            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n",
        "            truncation (float): The truncation ratio. Default: 1.\n",
        "            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n",
        "            inject_index (int | None): The injection index for mixing noise. Default: None.\n",
        "            return_latents (bool): Whether to return style latents. Default: False.\n",
        "        \"\"\"\n",
        "\n",
        "        # style codes -> latents with Style MLP layer\n",
        "        if not input_is_latent:\n",
        "            styles = [self.style_mlp(s) for s in styles]\n",
        "        # noises\n",
        "        if noise is None:\n",
        "            if randomize_noise:\n",
        "                noise = [None] * self.num_layers  # for each style conv layer\n",
        "\n",
        "            else:  # use the stored noise\n",
        "                noise = [getattr(self.noises, f'noise{i}') for i in range(self.num_layers)]\n",
        "        # style truncation\n",
        "        if truncation < 1:\n",
        "            style_truncation = []\n",
        "            for style in styles:\n",
        "                style_truncation.append(truncation_latent + truncation * (style - truncation_latent))\n",
        "            styles = style_truncation\n",
        "        # get style latents with injection\n",
        "\n",
        "        if len(styles) == 1:\n",
        "            inject_index = self.num_latent\n",
        "\n",
        "            if styles[0].ndim < 3:\n",
        "                # repeat latent code for all the layers\n",
        "                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
        "            else:  # used for encoder with different latent code for each layer\n",
        "                latent = styles[0]\n",
        "        elif len(styles) == 2:  # mixing noises\n",
        "            if inject_index is None:\n",
        "                inject_index = random.randint(1, self.num_latent - 1)\n",
        "            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n",
        "            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n",
        "            latent = torch.cat([latent1, latent2], 1)\n",
        "\n",
        "        # main generation\n",
        "        out = self.constant_input(latent.shape[0])\n",
        "        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n",
        "        skip = self.to_rgb1(out, latent[:, 1])\n",
        "\n",
        "        i = 1\n",
        "        for conv1, conv2, noise1, noise2, to_rgb in zip(self.style_convs[::2], self.style_convs[1::2], noise[1::2],noise[2::2], self.to_rgbs):\n",
        "\n",
        "            out = conv1(out, latent[:, i], noise=noise1)\n",
        "\n",
        "            # the conditions may have fewer levels\n",
        "            if i < len(conditions):\n",
        "                # SFT part to combine the conditions\n",
        "                if self.sft_half:  # only apply SFT to half of the channels\n",
        "                    out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n",
        "                    out_sft = out_sft * conditions[i - 1] + conditions[i]\n",
        "                    out = torch.cat([out_same, out_sft], dim=1)\n",
        "                else:  # apply SFT to all the channels\n",
        "                    out = out * conditions[i - 1] + conditions[i]\n",
        "\n",
        "            out = conv2(out, latent[:, i + 1], noise=noise2)\n",
        "            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n",
        "            i += 2\n",
        "\n",
        "        # -----------up 1024-----------------\n",
        "\n",
        "        out = self.final_conv1(out, latent[:, i], noise=None)\n",
        "\n",
        "        out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n",
        "        out_sft = out_sft * conditions[-2] + conditions[-1]\n",
        "        out = torch.cat([out_same, out_sft], dim=1)\n",
        "\n",
        "        out = self.final_conv2(out, latent[:, i+1], noise=None)\n",
        "        skip = self.final_rgb(out, latent[:, i+2], skip)  # feature back to the rgb space\n",
        "\n",
        "        image = skip\n",
        "\n",
        "        if return_latents:\n",
        "            return image, latent\n",
        "        else:\n",
        "            return image, None\n",
        "\n",
        "\n",
        "\n",
        "class GFPGANv1Clean(nn.Module):\n",
        "    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n",
        "\n",
        "    It is the clean version without custom compiled CUDA extensions used in StyleGAN2.\n",
        "\n",
        "    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n",
        "\n",
        "    Args:\n",
        "        out_size (int): The spatial size of outputs.\n",
        "        num_style_feat (int): Channel number of style features. Default: 512. i.e it defines the dimension of the latent vector (w) in StyleGAN.\n",
        "        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2. i.e. If channel_multiplier = 2, the model doubles the channels/feature maps in all layers.\n",
        "        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.\n",
        "        fix_decoder (bool): Whether to fix the decoder. Default: True.\n",
        "\n",
        "        num_mlp (int): Layer number of MLP style layers. Default: 8. i.e. num_mlp controls how many fully connected layers are used in transforming random noise (z) into a latent style vector (w).\n",
        "        input_is_latent (bool): Whether input is latent style. Default: False.\n",
        "        different_w (bool): Whether to use different latent w for different layers. Default: False.\n",
        "        narrow (float): The narrow ratio for channels. Default: 1.\n",
        "        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False. Applies spatial feature injection to half of the feature maps.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            out_size,\n",
        "            num_style_feat=512,\n",
        "            channel_multiplier=1,\n",
        "            decoder_load_path=None,\n",
        "            fix_decoder=True,\n",
        "            # for stylegan decoder\n",
        "            num_mlp=8,\n",
        "            input_is_latent=False,\n",
        "            different_w=False,\n",
        "            narrow=1,\n",
        "            sft_half=False):\n",
        "\n",
        "        super(GFPGANv1Clean, self).__init__()\n",
        "        self.input_is_latent = input_is_latent\n",
        "        self.different_w = different_w\n",
        "        self.num_style_feat = num_style_feat\n",
        "\n",
        "        unet_narrow = narrow * 0.5  # by default, use a half of input channels\n",
        "        channels = {\n",
        "            '4': int(512 * unet_narrow),\n",
        "            '8': int(512 * unet_narrow),\n",
        "            '16': int(512 * unet_narrow),\n",
        "            '32': int(512 * unet_narrow),\n",
        "            '64': int(256 * channel_multiplier * unet_narrow),\n",
        "            '128': int(128 * channel_multiplier * unet_narrow),\n",
        "            '256': int(64 * channel_multiplier * unet_narrow),\n",
        "            '512': int(32 * channel_multiplier * unet_narrow),\n",
        "            '1024': int(16 * channel_multiplier * unet_narrow)\n",
        "        }\n",
        "\n",
        "        self.log_size = int(math.log(out_size, 2))\n",
        "        first_out_size = 2**(int(math.log(out_size, 2)))  #The purpose of this is to ensure that out_size is a power of 2, which is often required in CNNs\n",
        "\n",
        "        self.conv_body_first = nn.Conv2d(3, channels[f'{first_out_size}'], 1)  #This accesses the dictionary 'channels' using the calculated first_out_size (a string)\n",
        "\n",
        "        # downsample\n",
        "        in_channels = channels[f'{first_out_size}']\n",
        "        self.conv_body_down = nn.ModuleList()\n",
        "        for i in range(self.log_size, 2, -1):\n",
        "            out_channels = channels[f'{2**(i - 1)}']\n",
        "            self.conv_body_down.append(ResBlock(in_channels, out_channels, mode='down'))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.final_conv = nn.Conv2d(in_channels, channels['4'], 3, 1, 1)\n",
        "\n",
        "        # upsample\n",
        "        in_channels = channels['4']\n",
        "        self.conv_body_up = nn.ModuleList()   # is initialized as an empty ModuleList\n",
        "        for i in range(3, self.log_size + 1):\n",
        "            out_channels = channels[f'{2**i}']\n",
        "            self.conv_body_up.append(ResBlock(in_channels, out_channels, mode='up'))\n",
        "            in_channels = out_channels\n",
        "\n",
        "        # to RGB\n",
        "        self.toRGB = nn.ModuleList()\n",
        "        for i in range(3, self.log_size + 1):\n",
        "            self.toRGB.append(nn.Conv2d(channels[f'{2**i}'], 3, 1))\n",
        "\n",
        "        if different_w:\n",
        "            linear_out_channel = (int(math.log(out_size, 2)) * 2 - 2) * num_style_feat\n",
        "        else:\n",
        "            linear_out_channel = num_style_feat\n",
        "\n",
        "        self.final_linear = nn.Linear(channels['4'] * 4 * 4, linear_out_channel)\n",
        "\n",
        "        # the decoder: stylegan2 generator with SFT modulations\n",
        "        self.stylegan_decoder = StyleGAN2GeneratorCSFT(\n",
        "            out_size=out_size,\n",
        "            num_style_feat=num_style_feat,\n",
        "            num_mlp=num_mlp,\n",
        "            channel_multiplier=channel_multiplier,\n",
        "            narrow=narrow,\n",
        "            sft_half=sft_half)\n",
        "\n",
        "\n",
        "        # load pre-trained stylegan2 model if necessary\n",
        "        if decoder_load_path:\n",
        "            self.stylegan_decoder.load_state_dict(\n",
        "                torch.load(decoder_load_path, map_location=lambda storage, loc: storage)['params_ema'])\n",
        "        # fix or freeze decoder without updating params\n",
        "        if fix_decoder:\n",
        "            for _, param in self.stylegan_decoder.named_parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # for SFT modulations (scale and shift)\n",
        "        self.condition_scale = nn.ModuleList()\n",
        "        self.condition_shift = nn.ModuleList()\n",
        "        for i in range(3, self.log_size + 1):\n",
        "            out_channels = channels[f'{2**i}']\n",
        "            if sft_half:\n",
        "                sft_out_channels = out_channels\n",
        "            else:\n",
        "                sft_out_channels = out_channels * 2\n",
        "            self.condition_scale.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(out_channels, out_channels, 3, 1, 1), nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(out_channels, sft_out_channels, 3, 1, 1)))\n",
        "            self.condition_shift.append(\n",
        "                nn.Sequential(\n",
        "                    nn.Conv2d(out_channels, out_channels, 3, 1, 1), nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(out_channels, sft_out_channels, 3, 1, 1)))\n",
        "\n",
        "\n",
        "        # ------------------up 1024---------------------------------\n",
        "            self.final_body_up = ResBlock(32, 32, mode='up')\n",
        "            self.final_scale = nn.Sequential(\n",
        "                    nn.Conv2d(out_channels, out_channels, 3, 1, 1), nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(out_channels, 8, 3, 1, 1))\n",
        "            self.final_shift = nn.Sequential(\n",
        "                    nn.Conv2d(out_channels, out_channels, 3, 1, 1), nn.LeakyReLU(0.2, True),\n",
        "                    nn.Conv2d(out_channels, 8, 3, 1, 1))\n",
        "            self.final_rgb = nn.Conv2d(32, 3, 1)\n",
        "            self.final_up =  nn.Upsample(scale_factor=2)\n",
        "\n",
        "            self.final_extend_linear = nn.Linear(4096, 1024)\n",
        "\n",
        "    def forward(self, x, return_latents=False, return_rgb=True, randomize_noise=True, **kwargs):\n",
        "        \"\"\"Forward function for GFPGANv1Clean.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input images.\n",
        "            return_latents (bool): Whether to return style latents. Default: False.\n",
        "            return_rgb (bool): Whether return intermediate rgb images. Default: True.\n",
        "            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n",
        "        \"\"\"\n",
        "        conditions = []  #Will store scale and shift values for Spatial Feature Transform (SFT) layers.\n",
        "        unet_skips = []  #Stores intermediate features (skip connections) from the encoder part of the network.\n",
        "        out_rgbs = []    #Stores the RGB images generated during the decoding process.\n",
        "\n",
        "        # encoder\n",
        "        feat = F.leaky_relu_(self.conv_body_first(x), negative_slope=0.2)\n",
        "        for i in range(self.log_size - 2):\n",
        "            feat = self.conv_body_down[i](feat)\n",
        "            unet_skips.insert(0, feat)  #After each downsampling step, the intermediate feature maps (feat) are stored in the unet_skips list.\n",
        "        feat = F.leaky_relu_(self.final_conv(feat), negative_slope=0.2)\n",
        "\n",
        "        # style code\n",
        "\n",
        "        style_code = self.final_linear(feat.view(feat.size(0), -1))  #The feature map feat is reshaped into a flat vector (feat.view(feat.size(0), -1)) and passed through the fully connected layer (final_linear). This generates the style_code, which is a latent code that encodes the image‚Äôs high-level features.\n",
        "        extend_style = self.final_extend_linear(feat.view(feat.size(0), -1))\n",
        "        style_code = torch.cat([style_code,extend_style],-1)   #The style_code and extend_style are concatenated along the last dimension to create a larger style vector. This combined vector will be used as input for the StyleGAN decoder.\n",
        "        if self.different_w:\n",
        "            style_code = style_code.view(style_code.size(0), -1, self.num_style_feat)\n",
        "\n",
        "        # decoder\n",
        "        for i in range(self.log_size - 2):\n",
        "            # add unet skip\n",
        "            feat = feat + unet_skips[i]\n",
        "            # ResUpLayer\n",
        "            feat = self.conv_body_up[i](feat)\n",
        "            # generate scale and shift for SFT layers\n",
        "            scale = self.condition_scale[i](feat)\n",
        "            conditions.append(scale.clone())\n",
        "            shift = self.condition_shift[i](feat)\n",
        "            conditions.append(shift.clone())\n",
        "            # generate rgb images\n",
        "            if return_rgb:\n",
        "                out_rgbs.append(self.toRGB[i](feat))\n",
        "\n",
        "        # ----final up for 1024--------------\n",
        "\n",
        "        feat = self.final_body_up(feat)\n",
        "        scale = self.final_scale(feat)\n",
        "        conditions.append(scale.clone())\n",
        "        shift =self.final_shift(feat)\n",
        "        conditions.append(shift.clone())\n",
        "        # generate rgb images\n",
        "        if return_rgb:\n",
        "            out_rgbs.append(self.final_rgb(feat))\n",
        "\n",
        "        # decoder\n",
        "        image, _ = self.stylegan_decoder([style_code],\n",
        "                                         conditions,\n",
        "                                         return_latents=return_latents,\n",
        "                                         input_is_latent=self.input_is_latent,\n",
        "                                         randomize_noise=randomize_noise)\n",
        "        # image = F.tanh(image)\n",
        "        return image, out_rgbs"
      ],
      "metadata": {
        "id": "iUXbxCCFgk31"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vgg model\n",
        "\n",
        "from collections import OrderedDict\n",
        "from torchvision.models import vgg as vgg\n",
        "\n",
        "VGG_PRETRAIN_PATH = 'pretrained_models/vgg19-dcbb9e9d.pth'\n",
        "\n",
        "NAMES = {\n",
        "    'vgg11': [\n",
        "        'conv1_1', 'relu1_1', 'pool1', 'conv2_1', 'relu2_1', 'pool2', 'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2',\n",
        "        'pool3', 'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'pool4', 'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2',\n",
        "        'pool5'\n",
        "    ],\n",
        "    'vgg13': [\n",
        "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'pool3', 'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'pool4',\n",
        "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'pool5'\n",
        "    ],\n",
        "    'vgg16': [\n",
        "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'pool3', 'conv4_1', 'relu4_1', 'conv4_2',\n",
        "        'relu4_2', 'conv4_3', 'relu4_3', 'pool4', 'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3',\n",
        "        'pool5'\n",
        "    ],\n",
        "    'vgg19': [\n",
        "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3', 'conv4_1',\n",
        "        'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4', 'conv5_1', 'relu5_1',\n",
        "        'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4', 'pool5'\n",
        "    ]\n",
        "}\n",
        "\n",
        "\n",
        "def insert_bn(names):\n",
        "    \"\"\"Insert bn layer after each conv.\n",
        "\n",
        "    Args:\n",
        "        names (list): The list of layer names.\n",
        "\n",
        "    Returns:\n",
        "        list: The list of layer names with bn layers.\n",
        "    \"\"\"\n",
        "    names_bn = []\n",
        "    for name in names:\n",
        "        names_bn.append(name)\n",
        "        if 'conv' in name:\n",
        "            position = name.replace('conv', '')\n",
        "            names_bn.append('bn' + position)\n",
        "    return names_bn\n",
        "\n",
        "\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    \"\"\"VGG network for feature extraction.\n",
        "\n",
        "    In this implementation, we allow users to choose whether use normalization\n",
        "    in the input feature and the type of vgg network. Note that the pretrained\n",
        "    path must fit the vgg type.\n",
        "\n",
        "    Args:\n",
        "        layer_name_list (list[str]): Forward function returns the corresponding\n",
        "            features according to the layer_name_list.\n",
        "            Example: {'relu1_1', 'relu2_1', 'relu3_1'}.\n",
        "        vgg_type (str): Set the type of vgg network. Default: 'vgg19'.\n",
        "        use_input_norm (bool): If True, normalize the input image. Importantly,\n",
        "            the input feature must be in the range [0, 1]. Default: True.\n",
        "        range_norm (bool): If True, norm images with range [-1, 1] to [0, 1].\n",
        "            Default: False.\n",
        "        requires_grad (bool): If true, the parameters of VGG network will be\n",
        "            optimized. Default: False.\n",
        "        remove_pooling (bool): If true, the max pooling operations in VGG net\n",
        "            will be removed. Default: False.\n",
        "        pooling_stride (int): The stride of max pooling operation. Default: 2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 layer_name_list,\n",
        "                 vgg_type='vgg19',\n",
        "                 use_input_norm=True,\n",
        "                 range_norm=False,\n",
        "                 requires_grad=False,\n",
        "                 remove_pooling=False,\n",
        "                 pooling_stride=2):\n",
        "        super(VGGFeatureExtractor, self).__init__()\n",
        "\n",
        "        self.layer_name_list = layer_name_list\n",
        "        self.use_input_norm = use_input_norm\n",
        "        self.range_norm = range_norm\n",
        "\n",
        "        self.names = NAMES[vgg_type.replace('_bn', '')]\n",
        "        if 'bn' in vgg_type:\n",
        "            self.names = insert_bn(self.names)\n",
        "\n",
        "        # only borrow layers that will be used to avoid unused params\n",
        "        max_idx = 0\n",
        "        for v in layer_name_list:\n",
        "            idx = self.names.index(v)\n",
        "            if idx > max_idx:\n",
        "                max_idx = idx\n",
        "\n",
        "        if os.path.exists(VGG_PRETRAIN_PATH):\n",
        "            vgg_net = getattr(vgg, vgg_type)(pretrained=False)\n",
        "            state_dict = torch.load(VGG_PRETRAIN_PATH, map_location=lambda storage, loc: storage)\n",
        "            vgg_net.load_state_dict(state_dict)\n",
        "            print(\"manual vgg used here\")\n",
        "        else:\n",
        "            vgg_net = getattr(vgg, vgg_type)(pretrained=True)\n",
        "            print(\"standard torchvision vgg model is used here\")\n",
        "\n",
        "        features = vgg_net.features[:max_idx + 1]\n",
        "\n",
        "        modified_net = OrderedDict()\n",
        "        for k, v in zip(self.names, features):\n",
        "            if 'pool' in k:\n",
        "                # if remove_pooling is true, pooling operation will be removed\n",
        "                if remove_pooling:\n",
        "                    continue\n",
        "                else:\n",
        "                    # in some cases, we may want to change the default stride\n",
        "                    modified_net[k] = nn.MaxPool2d(kernel_size=2, stride=pooling_stride)\n",
        "            else:\n",
        "                modified_net[k] = v\n",
        "\n",
        "        self.vgg_net = nn.Sequential(modified_net)\n",
        "\n",
        "        if not requires_grad:\n",
        "            self.vgg_net.eval()\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "        else:\n",
        "            self.vgg_net.train()\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = True\n",
        "\n",
        "        if self.use_input_norm:\n",
        "            # the mean is for image with range [0, 1]\n",
        "            self.register_buffer('mean', torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "            # the std is for image with range [0, 1]\n",
        "            self.register_buffer('std', torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward function.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor with shape (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Forward results.\n",
        "        \"\"\"\n",
        "        if self.range_norm:\n",
        "            x = (x + 1) / 2\n",
        "        if self.use_input_norm:\n",
        "            x = (x - self.mean) / self.std\n",
        "\n",
        "        output = {}\n",
        "        for key, layer in self.vgg_net._modules.items():\n",
        "            x = layer(x)\n",
        "            if key in self.layer_name_list:\n",
        "                output[key] = x.clone()\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "GzqdrTquiaQX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss.py\n",
        "from torch import autograd as autograd\n",
        "\n",
        "def l2_norm(input,axis=1):\n",
        "    norm = torch.norm(input,2,axis,True)\n",
        "    output = torch.div(input, norm)\n",
        "    return output\n",
        "\n",
        "def adv_loss(logits, target):\n",
        "    assert target in [1, 0]\n",
        "    targets = torch.full_like(logits, fill_value=target)\n",
        "    loss = F.binary_cross_entropy_with_logits(logits, targets)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def r1_reg(d_out, x_in):\n",
        "    # zero-centered gradient penalty for real images\n",
        "    batch_size = x_in.size(0)\n",
        "    grad_dout = torch.autograd.grad(\n",
        "        outputs=d_out.sum(), inputs=x_in,\n",
        "        create_graph=True, retain_graph=True, only_inputs=True\n",
        "    )[0]\n",
        "    grad_dout2 = grad_dout.pow(2)\n",
        "    assert(grad_dout2.size() == x_in.size())\n",
        "    reg = 0.5 * grad_dout2.view(batch_size, -1).sum(1).mean(0)\n",
        "    return reg\n",
        "\n",
        "class GANLoss(nn.Module):\n",
        "    \"\"\"Define GAN loss.\n",
        "\n",
        "    Args:\n",
        "        gan_type (str): Support 'vanilla', 'lsgan', 'wgan', 'hinge'.\n",
        "        real_label_val (float): The value for real label. Default: 1.0.\n",
        "        fake_label_val (float): The value for fake label. Default: 0.0.\n",
        "        loss_weight (float): Loss weight. Default: 1.0.\n",
        "            Note that loss_weight is only for generators; and it is always 1.0\n",
        "            for discriminators.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, gan_type = 'vanilla', real_label_val=1.0, fake_label_val=0.0, loss_weight=1.0):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.gan_type = gan_type\n",
        "        self.loss_weight = loss_weight\n",
        "        self.real_label_val = real_label_val\n",
        "        self.fake_label_val = fake_label_val\n",
        "\n",
        "        if self.gan_type == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif self.gan_type == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif self.gan_type == 'wgan':\n",
        "            self.loss = self._wgan_loss\n",
        "        elif self.gan_type == 'wgan_softplus':\n",
        "            self.loss = self._wgan_softplus_loss\n",
        "        elif self.gan_type == 'hinge':\n",
        "            self.loss = nn.ReLU()\n",
        "        else:\n",
        "            raise NotImplementedError(f'GAN type {self.gan_type} is not implemented.')\n",
        "\n",
        "    def _wgan_loss(self, input, target):\n",
        "        \"\"\"wgan loss.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): Input tensor.\n",
        "            target (bool): Target label.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: wgan loss.\n",
        "        \"\"\"\n",
        "        return -input.mean() if target else input.mean()\n",
        "\n",
        "    def _wgan_softplus_loss(self, input, target):\n",
        "        \"\"\"wgan loss with soft plus. softplus is a smooth approximation to the\n",
        "        ReLU function.\n",
        "\n",
        "        In StyleGAN2, it is called:\n",
        "            Logistic loss for discriminator;\n",
        "            Non-saturating loss for generator.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): Input tensor.\n",
        "            target (bool): Target label.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: wgan loss.\n",
        "        \"\"\"\n",
        "        return F.softplus(-input).mean() if target else F.softplus(input).mean()\n",
        "\n",
        "    def get_target_label(self, input, target_is_real):\n",
        "        \"\"\"Get target label.\n",
        "\n",
        "        Args:\n",
        "            input (Tensor): Input tensor.\n",
        "            target_is_real (bool): Whether the target is real or fake.\n",
        "\n",
        "        Returns:\n",
        "            (bool | Tensor): Target tensor. Return bool for wgan, otherwise,\n",
        "                return Tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.gan_type in ['wgan', 'wgan_softplus']:\n",
        "            return target_is_real\n",
        "        target_val = (self.real_label_val if target_is_real else self.fake_label_val)\n",
        "        return input.new_ones(input.size()) * target_val\n",
        "\n",
        "    def forward(self, input, target_is_real, is_disc=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input (Tensor): The input for the loss module, i.e., the network\n",
        "                prediction.\n",
        "            target_is_real (bool): Whether the targe is real or fake.\n",
        "            is_disc (bool): Whether the loss for discriminators or not.\n",
        "                Default: False.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: GAN loss value.\n",
        "        \"\"\"\n",
        "        target_label = self.get_target_label(input, target_is_real)\n",
        "        if self.gan_type == 'hinge':\n",
        "            if is_disc:  # for discriminators in hinge-gan\n",
        "                input = -input if target_is_real else input\n",
        "                loss = self.loss(1 + input).mean()\n",
        "            else:  # for generators in hinge-gan\n",
        "                loss = -input.mean()\n",
        "        else:  # other gan types\n",
        "            loss = self.loss(input, target_label)\n",
        "\n",
        "        # loss_weight is always 1.0 for discriminators\n",
        "        return loss if is_disc else loss * self.loss_weight\n",
        "\n",
        "\n",
        "def r1_penalty(real_pred, real_img):\n",
        "    \"\"\"R1 regularization for discriminator. The core idea is to\n",
        "        penalize the gradient on real data alone: when the\n",
        "        generator distribution produces the true data distribution\n",
        "        and the discriminator is equal to 0 on the data manifold, the\n",
        "        gradient penalty ensures that the discriminator cannot create\n",
        "        a non-zero gradient orthogonal to the data manifold without\n",
        "        suffering a loss in the GAN game.\n",
        "\n",
        "        Ref:\n",
        "        Eq. 9 in Which training methods for GANs do actually converge.\n",
        "        \"\"\"\n",
        "    grad_real = autograd.grad(outputs=real_pred.sum(), inputs=real_img, create_graph=True)[0]\n",
        "    grad_penalty = grad_real.pow(2).view(grad_real.shape[0], -1).sum(1).mean()\n",
        "    return grad_penalty\n",
        "\n",
        "class PerceptualLoss(nn.Module):\n",
        "    \"\"\"Perceptual loss with commonly used style loss.\n",
        "\n",
        "    Args:\n",
        "        layer_weights (dict): The weight for each layer of vgg feature.\n",
        "            Here is an example: {'conv5_4': 1.}, which means the conv5_4\n",
        "            feature layer (before relu5_4) will be extracted with weight\n",
        "            1.0 in calculting losses.\n",
        "        vgg_type (str): The type of vgg network used as feature extractor.\n",
        "            Default: 'vgg19'.\n",
        "        use_input_norm (bool):  If True, normalize the input image in vgg.\n",
        "            Default: True.\n",
        "        range_norm (bool): If True, norm images with range [-1, 1] to [0, 1].\n",
        "            Default: False.\n",
        "        perceptual_weight (float): If `perceptual_weight > 0`, the perceptual\n",
        "            loss will be calculated and the loss will multiplied by the\n",
        "            weight. Default: 1.0.\n",
        "        style_weight (float): If `style_weight > 0`, the style loss will be\n",
        "            calculated and the loss will multiplied by the weight.\n",
        "            Default: 0.\n",
        "        criterion (str): Criterion used for perceptual loss. Default: 'l1'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 layer_weights,\n",
        "                 vgg_type='vgg19',\n",
        "                 use_input_norm=True,\n",
        "                 range_norm=False,\n",
        "                 perceptual_weight=1.0,\n",
        "                 style_weight=0.,\n",
        "                 criterion='l1'):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "        self.perceptual_weight = perceptual_weight\n",
        "        self.style_weight = style_weight\n",
        "        self.layer_weights = layer_weights\n",
        "        self.vgg = VGGFeatureExtractor(\n",
        "            layer_name_list=list(layer_weights.keys()),\n",
        "            vgg_type=vgg_type,\n",
        "            use_input_norm=use_input_norm,\n",
        "            range_norm=range_norm)\n",
        "\n",
        "        self.criterion_type = criterion\n",
        "        if self.criterion_type == 'l1':\n",
        "            self.criterion = torch.nn.L1Loss()\n",
        "        elif self.criterion_type == 'l2':\n",
        "            self.criterion = torch.nn.L2loss()\n",
        "        elif self.criterion_type == 'fro':\n",
        "            self.criterion = None\n",
        "        else:\n",
        "            raise NotImplementedError(f'{criterion} criterion has not been supported.')\n",
        "\n",
        "    def forward(self, x, gt):\n",
        "        \"\"\"Forward function.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input tensor with shape (n, c, h, w).\n",
        "            gt (Tensor): Ground-truth tensor with shape (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            Tensor: Forward results.\n",
        "        \"\"\"\n",
        "        # extract vgg features\n",
        "        x_features = self.vgg(x)\n",
        "        gt_features = self.vgg(gt.detach())\n",
        "\n",
        "        # calculate perceptual loss\n",
        "        if self.perceptual_weight > 0:\n",
        "            percep_loss = 0\n",
        "            for k in x_features.keys():\n",
        "                if self.criterion_type == 'fro':\n",
        "                    percep_loss += torch.norm(x_features[k] - gt_features[k], p='fro') * self.layer_weights[k]\n",
        "                else:\n",
        "                    percep_loss += self.criterion(x_features[k], gt_features[k]) * self.layer_weights[k]\n",
        "            percep_loss *= self.perceptual_weight\n",
        "        else:\n",
        "            percep_loss = None\n",
        "\n",
        "        # calculate style loss\n",
        "        if self.style_weight > 0:\n",
        "            style_loss = 0\n",
        "            for k in x_features.keys():\n",
        "                if self.criterion_type == 'fro':\n",
        "                    style_loss += torch.norm(\n",
        "                        self._gram_mat(x_features[k]) - self._gram_mat(gt_features[k]), p='fro') * self.layer_weights[k]\n",
        "                else:\n",
        "                    style_loss += self.criterion(self._gram_mat(x_features[k]), self._gram_mat(\n",
        "                        gt_features[k])) * self.layer_weights[k]\n",
        "            style_loss *= self.style_weight\n",
        "        else:\n",
        "            style_loss = None\n",
        "\n",
        "        return percep_loss, style_loss\n",
        "\n",
        "    def _gram_mat(self, x):\n",
        "        \"\"\"Calculate Gram matrix.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor with shape of (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Gram matrix.\n",
        "        \"\"\"\n",
        "        n, c, h, w = x.size()\n",
        "        features = x.view(n, c, w * h)\n",
        "        features_t = features.transpose(1, 2)\n",
        "        gram = features.bmm(features_t) / (c * h * w)\n",
        "        return gram\n"
      ],
      "metadata": {
        "id": "39JfKLfGiEQG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#resnet arcFace\n",
        "# from basicsr.utils.registry import ARCH_REGISTRY\n",
        "\n",
        "def conv3x3(inplanes, outplanes, stride=1):\n",
        "    \"\"\"A simple wrapper for 3x3 convolution with padding.\n",
        "\n",
        "    Args:\n",
        "        inplanes (int): Channel number of inputs.\n",
        "        outplanes (int): Channel number of outputs.\n",
        "        stride (int): Stride in convolution. Default: 1.\n",
        "    \"\"\"\n",
        "    return nn.Conv2d(inplanes, outplanes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"Basic residual block used in the ResNetArcFace architecture.\n",
        "\n",
        "    Args:\n",
        "        inplanes (int): Channel number of inputs.\n",
        "        planes (int): Channel number of outputs.\n",
        "        stride (int): Stride in convolution. Default: 1.\n",
        "        downsample (nn.Module): The downsample module. Default: None.\n",
        "    \"\"\"\n",
        "    expansion = 1  # output channel expansion ratio\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class IRBlock(nn.Module):\n",
        "    \"\"\"Improved residual block (IR Block) used in the ResNetArcFace architecture.\n",
        "\n",
        "    Args:\n",
        "        inplanes (int): Channel number of inputs.\n",
        "        planes (int): Channel number of outputs.\n",
        "        stride (int): Stride in convolution. Default: 1.\n",
        "        downsample (nn.Module): The downsample module. Default: None.\n",
        "        use_se (bool): Whether use the SEBlock (squeeze and excitation block). Default: True.\n",
        "    \"\"\"\n",
        "    expansion = 1  # output channel expansion ratio\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):\n",
        "        super(IRBlock, self).__init__()\n",
        "        self.bn0 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = conv3x3(inplanes, inplanes)\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.conv2 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "        self.use_se = use_se\n",
        "        if self.use_se:\n",
        "            self.se = SEBlock(planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.bn0(x)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn1(out)\n",
        "        out = self.prelu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.use_se:\n",
        "            out = self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.prelu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"Bottleneck block used in the ResNetArcFace architecture.\n",
        "\n",
        "    Args:\n",
        "        inplanes (int): Channel number of inputs.\n",
        "        planes (int): Channel number of outputs.\n",
        "        stride (int): Stride in convolution. Default: 1.\n",
        "        downsample (nn.Module): The downsample module. Default: None.\n",
        "    \"\"\"\n",
        "    expansion = 4  # output channel expansion ratio\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"The squeeze-and-excitation block (SEBlock) used in the IRBlock.\n",
        "\n",
        "    Args:\n",
        "        channel (int): Channel number of inputs.\n",
        "        reduction (int): Channel reduction ration. Default: 16.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # pool to 1x1 without spatial information\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction), nn.PReLU(), nn.Linear(channel // reduction, channel),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "# @ARCH_REGISTRY.register()\n",
        "class ResNetArcFace(nn.Module):\n",
        "    \"\"\"ArcFace with ResNet architectures.\n",
        "\n",
        "    Ref: ArcFace: Additive Angular Margin Loss for Deep Face Recognition.\n",
        "\n",
        "    Args:\n",
        "        block (str): Block used in the ArcFace architecture.\n",
        "        layers (tuple(int)): Block numbers in each layer.\n",
        "        use_se (bool): Whether use the SEBlock (squeeze and excitation block). Default: True.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_size, block, layers, use_se=True):\n",
        "        if block == 'IRBlock':\n",
        "            block = IRBlock\n",
        "        self.inplanes = 64\n",
        "        self.out_size = out_size\n",
        "        self.use_se = use_se\n",
        "        super(ResNetArcFace, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.prelu = nn.PReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.bn4 = nn.BatchNorm2d(512)\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.fc5 = nn.Linear(512 * 8 * 8, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "\n",
        "        # initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample, use_se=self.use_se))\n",
        "        self.inplanes = planes\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(block(self.inplanes, planes, use_se=self.use_se))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.prelu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc5(x)\n",
        "        x = self.bn5(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Gj87-Rtzi1e3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Discriminator\n",
        "\n",
        "class StyleGAN2Discriminator(nn.Module):\n",
        "    \"\"\"StyleGAN2 Discriminator.\n",
        "\n",
        "    Args:\n",
        "        out_size (int): The spatial size of outputs.\n",
        "        channel_multiplier (int): Channel multiplier for large networks of\n",
        "            StyleGAN2. Default: 2.\n",
        "        resample_kernel (list[int]): A list indicating the 1D resample kernel\n",
        "            magnitude. A cross production will be applied to extent 1D resample\n",
        "            kernel to 2D resample kernel. Default: (1, 3, 3, 1).\n",
        "        stddev_group (int): For group stddev statistics. Default: 4.\n",
        "        narrow (float): Narrow ratio for channels. Default: 1.0.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_size, channel_multiplier=2, resample_kernel=(1, 3, 3, 1), stddev_group=4, narrow=1):\n",
        "        super(StyleGAN2Discriminator, self).__init__()\n",
        "\n",
        "        channels = {\n",
        "            '4': int(512 * narrow),\n",
        "            '8': int(512 * narrow),\n",
        "            '16': int(512 * narrow),\n",
        "            '32': int(512 * narrow),\n",
        "            '64': int(256 * channel_multiplier * narrow),\n",
        "            '128': int(128 * channel_multiplier * narrow),\n",
        "            '256': int(64 * channel_multiplier * narrow),\n",
        "            '512': int(32 * channel_multiplier * narrow),\n",
        "            '1024': int(16 * channel_multiplier * narrow)\n",
        "        }\n",
        "\n",
        "        log_size = int(math.log(out_size, 2))\n",
        "\n",
        "        conv_body = [ConvLayer(3, channels[f'{out_size}'], 1, bias=True, activate=True)]\n",
        "\n",
        "        in_channels = channels[f'{out_size}']\n",
        "        for i in range(log_size, 2, -1):\n",
        "            out_channels = channels[f'{2**(i - 1)}']\n",
        "            conv_body.append(ResBlock2(in_channels, out_channels, resample_kernel))\n",
        "            in_channels = out_channels\n",
        "        self.conv_body = nn.Sequential(*conv_body)\n",
        "\n",
        "        self.final_conv = ConvLayer(in_channels + 1, channels['4'], 3, bias=True, activate=True)\n",
        "        self.final_linear = nn.Sequential(\n",
        "            EqualLinear(\n",
        "                channels['4'] * 4 * 4, channels['4'], bias=True, bias_init_val=0, lr_mul=1, activation='fused_lrelu'),\n",
        "            EqualLinear(channels['4'], 1, bias=True, bias_init_val=0, lr_mul=1, activation=None),\n",
        "        )\n",
        "        self.stddev_group = stddev_group\n",
        "        self.stddev_feat = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        feat = []\n",
        "        for i,module in enumerate(self.conv_body):\n",
        "            x = module(x)\n",
        "            if i > 0:\n",
        "                feat.append(x)\n",
        "\n",
        "        out = x\n",
        "\n",
        "        b, c, h, w = out.shape\n",
        "        # concatenate a group stddev statistics to out\n",
        "        group = min(b, self.stddev_group)  # Minibatch must be divisible by (or smaller than) group_size\n",
        "        stddev = out.view(group, -1, self.stddev_feat, c // self.stddev_feat, h, w)\n",
        "        stddev = torch.sqrt(stddev.var(0, unbiased=False) + 1e-8)\n",
        "        stddev = stddev.mean([2, 3, 4], keepdims=True).squeeze(2)\n",
        "        stddev = stddev.repeat(group, 1, h, w)\n",
        "        out = torch.cat([out, stddev], 1)\n",
        "\n",
        "        for j,m in enumerate(self.final_conv):\n",
        "            out = m(out)\n",
        "            if j == 0:\n",
        "                feat.append(out)\n",
        "        out = out.view(b, -1)\n",
        "        out = self.final_linear(out)\n",
        "\n",
        "        return out,feat\n",
        "\n",
        "class FacialComponentDiscriminator(nn.Module):\n",
        "    \"\"\"Facial component (eyes, mouth, noise) discriminator used in GFPGAN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FacialComponentDiscriminator, self).__init__()\n",
        "        # It now uses a VGG-style architectrue with fixed model size\n",
        "        self.conv1 = ConvLayer(3, 64, 3, downsample=False, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n",
        "        self.conv2 = ConvLayer(64, 128, 3, downsample=True, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n",
        "        self.conv3 = ConvLayer(128, 128, 3, downsample=False, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n",
        "        self.conv4 = ConvLayer(128, 256, 3, downsample=True, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n",
        "        self.conv5 = ConvLayer(256, 256, 3, downsample=False, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n",
        "        self.final_conv = ConvLayer(256, 1, 3, bias=True, activate=False)\n",
        "\n",
        "    def forward(self, x, return_feats=False, **kwargs):\n",
        "        \"\"\"Forward function for FacialComponentDiscriminator.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): Input images.\n",
        "            return_feats (bool): Whether to return intermediate features. Default: False.\n",
        "        \"\"\"\n",
        "        feat = self.conv1(x)\n",
        "        feat = self.conv3(self.conv2(feat))\n",
        "        rlt_feats = []\n",
        "        if return_feats:\n",
        "            rlt_feats.append(feat.clone())\n",
        "        feat = self.conv5(self.conv4(feat))\n",
        "        if return_feats:\n",
        "            rlt_feats.append(feat.clone())\n",
        "        out = self.final_conv(feat)\n",
        "\n",
        "        if return_feats:\n",
        "            return out, rlt_feats\n",
        "        else:\n",
        "            return out, None"
      ],
      "metadata": {
        "id": "D4l06lfTjVsj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training Visualizer class\n",
        "\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "!pip install tensorboardX\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "\n",
        "class Visualizer:\n",
        "    def __init__(self,opt,mode='train'):\n",
        "        self.opt = opt\n",
        "        self.name = opt.name\n",
        "        self.mode = mode\n",
        "        self.train_log_dir = os.path.join(opt.checkpoint_path,\"logs/%s\"%mode)\n",
        "        self.log_name = os.path.join(opt.checkpoint_path,'loss_log_%s.txt'%mode)\n",
        "        if opt.local_rank == 0:\n",
        "            if not os.path.exists(self.train_log_dir):\n",
        "                os.makedirs(self.train_log_dir)\n",
        "\n",
        "            self.train_writer = SummaryWriter(self.train_log_dir)\n",
        "\n",
        "            self.log_file = open(self.log_name,\"a\")\n",
        "            now = time.strftime(\"%c\")\n",
        "            self.log_file.write('================ Training Loss (%s) =================\\n'%now)\n",
        "            self.log_file.flush()\n",
        "\n",
        "\n",
        "    # errors:dictionary of error labels and values\n",
        "    def plot_current_errors(self,errors,step):\n",
        "\n",
        "        for tag,value in errors.items():\n",
        "\n",
        "            self.train_writer.add_scalar(\"%s/\"%self.name+tag,value,step)\n",
        "            self.train_writer.flush()\n",
        "\n",
        "\n",
        "    # errors: same format as |errors| of CurrentErrors\n",
        "    def print_current_errors(self,epoch,i,errors,t):\n",
        "        message = '(epoch: %d\\t iters: %d\\t time: %.5f)\\t'%(epoch,i,t)\n",
        "        for k,v in errors.items():\n",
        "\n",
        "            message += '%s: %.5f\\t' %(k,v)\n",
        "\n",
        "        print(message)\n",
        "\n",
        "        self.log_file.write('%s\\n' % message)\n",
        "        self.log_file.flush()\n",
        "\n",
        "    def display_current_results(self, visuals, step):\n",
        "        if visuals is None:\n",
        "            return\n",
        "        for label, image in visuals.items():\n",
        "            # Write the image to a string\n",
        "\n",
        "            self.train_writer.add_image(\"%s/\"%self.name+label,image,global_step=step)\n",
        "\n",
        "    def close(self):\n",
        "\n",
        "        self.train_writer.close()\n",
        "        self.log_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X3t9UDEoI5U",
        "outputId": "b80235e9-0744-4aa1-d878-e1f87e59cc39"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (24.2)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX) (4.25.6)\n",
            "Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/101.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.6.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#utils.py\n",
        "import torch\n",
        "# from dataloader.GFPLoader import  GFPData\n",
        "from functools import partial\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "#L2 Normalization helps stabilize training by ensuring all feature vectors have a uniform scale.\n",
        "def l2_norm(input,axis=1):  #normalized the input tensor\n",
        "    norm = torch.norm(input,2,axis,True) #Computes the L2 norm (Euclidean norm) of input along the specified axis.\n",
        "    output = torch.div(input, norm) #Element-wise division of input by norm to normalize it.\n",
        "    return output\n",
        "\n",
        "\n",
        "#Function Description:\n",
        "#Handles cases where some samples in a batch are None.\n",
        "#Removes 'None' values and replaces missing samples with random samples from the dataset.\n",
        "# my_collate () function ensures that the batch size remains constant.\n",
        "# def my_collate(batch,dataset):\n",
        "#     len_batch = len(batch) # original batch length\n",
        "#     batch = list(filter (lambda x:x is not None, batch)) # \"lambda x:x is not None\" -> This is a lambda function that takes an input x and returns True if x is not None, otherwise False.\n",
        "#     if len_batch > len(batch): # source all the required samples from the original dataset at random\n",
        "#         diff = len_batch - len(batch)\n",
        "#         while diff > 0:\n",
        "#             data = dataset[np.random.randint(0, len(dataset))]\n",
        "#             if data is not None:\n",
        "#                 batch.append(data)\n",
        "#                 diff -= 1\n",
        "\n",
        "\n",
        "#     return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "# simplified my_collate function  #If our dataset does not contain any 'None' values then we can use it\n",
        "def my_collate(batch):\n",
        "    return torch.utils.data.dataloader.default_collate(batch)\n",
        "\n",
        "def requires_grad(model, flag=True):\n",
        "    if model is None:\n",
        "        return\n",
        "    for p in model.parameters():\n",
        "        p.requires_grad = flag\n",
        "\n",
        "def need_grad(x):\n",
        "    x = x.detach()\n",
        "    x.requires_grad_()\n",
        "    return x\n",
        "\n",
        "def init_weights(model,init_type='normal', gain=0.02):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('BatchNorm2d') != -1:\n",
        "            if hasattr(m, 'weight') and m.weight is not None:\n",
        "                torch.nn.init.normal_(m.weight.data, 1.0, gain)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "        elif hasattr(m, 'bias') and m.bias is not None:\n",
        "                torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "        elif hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                torch.nn.init.normal_(m.weight.data, 0.0, gain)\n",
        "            elif init_type == 'xavier':\n",
        "                torch.nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'xavier_uniform':\n",
        "                torch.nn.init.xavier_uniform_(m.weight.data, gain=1.0)\n",
        "            elif init_type == 'kaiming':\n",
        "                torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                torch.nn.init.orthogonal_(m.weight.data, gain=gain)\n",
        "            elif init_type == 'none':  # uses pytorch's default init method\n",
        "                m.reset_parameters()\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "\n",
        "    model.apply(init_func)\n",
        "\n",
        "#This accumulate() function performs exponential moving average (EMA) updates between two models, model1 and model2\n",
        "def accumulate(model1, model2, decay=0.999,use_buffer=False):\n",
        "    par1 = dict(model1.named_parameters())\n",
        "    par2 = dict(model2.named_parameters())\n",
        "\n",
        "    for k in par1.keys():\n",
        "        par1[k].data.mul_(decay).add_(par2[k].data, alpha=1 - decay)\n",
        "\n",
        "    if use_buffer:\n",
        "        for p1,p2 in zip(model1.buffers(),model2.buffers()):\n",
        "            p1.detach().copy_(decay*p1.detach()+(1-decay)*p2.detach())\n",
        "\n",
        "def setup_seed(seed):\n",
        "    # random.seed(seed)\n",
        "    # np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def load_state_dict_with_prefix_removal(model, state_dict_path, device=\"cpu\"):\n",
        "    \"\"\"Loads a state dictionary, removing 'module.' prefixes if present.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to load the state dictionary into.\n",
        "        state_dict_path (str): The path to the state dictionary.\n",
        "        device (str): Device to load the state_dict to. Default: \"cpu\"\n",
        "    \"\"\"\n",
        "    state_dict = torch.load(state_dict_path, map_location=device)\n",
        "\n",
        "    # Check if any keys have the 'module.' prefix\n",
        "    if any(key.startswith('module.') for key in state_dict.keys()):\n",
        "        new_state_dict = {}\n",
        "        for key, value in state_dict.items():\n",
        "             new_key = key.replace('module.', '')\n",
        "             new_state_dict[new_key] = value\n",
        "        state_dict = new_state_dict\n",
        "        print(\"Removed 'module.' prefix from state_dict keys.\")\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "\n",
        "def get_data_loader(args):\n",
        "\n",
        "    train_data = GFPData(dist=args.dist,\n",
        "                    mean=args.mean,\n",
        "                    std=args.std,\n",
        "                    # blur_kernel_size=args.blur_kernel_size,\n",
        "                    # kernel_list=args.kernel_list,\n",
        "                    # kernel_prob=args.kernel_prob,\n",
        "                    # blur_sigma=args.blur_sigma,\n",
        "                    # downsample_range=args.downsample_range,\n",
        "                    # noise_range=args.noise_range,\n",
        "                    # jpeg_range=args.jpeg_range,\n",
        "                    # use_flip=args.use_flip,\n",
        "                    hq_root=args.train_hq_root,\n",
        "                    lq_root=args.train_lq_root,\n",
        "                    img_root=args.img_root,\n",
        "                    # crop_components=args.crop_components,\n",
        "                    # lmk_base=args.train_lmk_base,\n",
        "                    size=args.size,\n",
        "                    # eye_enlarge_ratio=args.eye_enlarge_ratio,\n",
        "                    eval=False)\n",
        "\n",
        "    test_data = None\n",
        "    if args.eval:\n",
        "        test_data = GFPData(dist=args.dist,\n",
        "                    mean=args.mean,\n",
        "                    std=args.std,\n",
        "                    # blur_kernel_size=args.blur_kernel_size,\n",
        "                    # kernel_list=args.kernel_list,\n",
        "                    # kernel_prob=args.kernel_prob,\n",
        "                    # blur_sigma=args.blur_sigma,\n",
        "                    # downsample_range=args.downsample_range,\n",
        "                    # noise_range=args.noise_range,\n",
        "                    # jpeg_range=args.jpeg_range,\n",
        "                    # use_flip=args.use_flip,\n",
        "                    hq_root=args.train_hq_root,    #only for testing  -change the train to val for evaluation. i just keep it for avoiding file path errors\n",
        "                    lq_root=args.train_lq_root,     #only for testing\n",
        "                    # crop_components=args.crop_components,\n",
        "                    # lmk_base=args.val_lmk_base,\n",
        "                    size=args.size,\n",
        "                    # eye_enlarge_ratio=args.eye_enlarge_ratio\n",
        "                    eval=True )\n",
        "\n",
        "    use_collate = partial(my_collate,dataset=train_data)  #functools.partial is a Python function that allows you to fix certain arguments of a function while keeping others flexible. This means that when calling use_collate, you only need to provide the remaining arguments.\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_data,\n",
        "                        collate_fn= my_collate,  # i used simple default_collate()\n",
        "                        batch_size=args.batch_size,\n",
        "                        num_workers=args.nDataLoaderThread,\n",
        "                        pin_memory=False,\n",
        "                        drop_last=True) #####myadd just test\n",
        "\n",
        "\n",
        "    test_loader = None if test_data is None else \\\n",
        "        torch.utils.data.DataLoader(\n",
        "                        test_data,\n",
        "                        collate_fn = my_collate,\n",
        "                        batch_size=args.batch_size,\n",
        "                        num_workers=args.nDataLoaderThread,\n",
        "                        pin_memory=False,\n",
        "                        drop_last=True\n",
        "                    )\n",
        "    return train_loader,test_loader,len(train_loader)\n",
        "\n",
        "\n",
        "#he function merge_args(args, params) is used to update an existing argument object (args) with values from another object (params).\n",
        "def merge_args(args,params):\n",
        "   for k,v in vars(params).items():  #vars(params).items() extracts all attributes and their values from params as key-value pairs.\n",
        "      setattr(args,k,v) #The setattr(args, k, v) function dynamically sets the attribute k in args to the value v.\n",
        "   return args\n",
        "\n",
        "\n",
        "#the function torch.clamp() is used to restrict values within a specified range.\n",
        "#torch.clamp(input, min=None, max=None)\n",
        "def convert_img(img,unit=False):\n",
        "\n",
        "    img = (img + 1) * 0.5\n",
        "    if unit: #If unit=True, the function scales pixel values to [0,255], which is the standard range for 8-bit images (used for saving or displaying).\n",
        "      return torch.clamp(img*255+0.5,0,255)\n",
        "\n",
        "    return torch.clamp(img,0,1) ##If unit=False, the function keeps values in [0,1], which is useful for processing images in PyTorch.\n",
        "\n",
        "\n",
        "def convert_flow_to_deformation(flow):\n",
        "    r\"\"\"convert flow fields to deformations.\n",
        "\n",
        "    Args:\n",
        "        flow (tensor): Flow field obtained by the model\n",
        "    Returns:\n",
        "        deformation (tensor): The deformation used for warpping\n",
        "    \"\"\"\n",
        "    b,c,h,w = flow.shape\n",
        "    flow_norm = 2 * torch.cat([flow[:,:1,...]/(w-1),flow[:,1:,...]/(h-1)], 1)\n",
        "    grid = make_coordinate_grid(flow)\n",
        "    deformation = grid + flow_norm.permute(0,2,3,1)\n",
        "    return deformation\n",
        "\n",
        "def make_coordinate_grid(flow):\n",
        "    r\"\"\"obtain coordinate grid with the same size as the flow filed.\n",
        "\n",
        "    Args:\n",
        "        flow (tensor): Flow field obtained by the model\n",
        "    Returns:\n",
        "        grid (tensor): The grid with the same size as the input flow\n",
        "    \"\"\"\n",
        "    b,c,h,w = flow.shape\n",
        "\n",
        "    x = torch.arange(w).to(flow)\n",
        "    y = torch.arange(h).to(flow)\n",
        "\n",
        "    x = (2 * (x / (w - 1)) - 1)\n",
        "    y = (2 * (y / (h - 1)) - 1)\n",
        "\n",
        "    yy = y.view(-1, 1).repeat(1, w)\n",
        "    xx = x.view(1, -1).repeat(h, 1)\n",
        "\n",
        "    meshed = torch.cat([xx.unsqueeze_(2), yy.unsqueeze_(2)], 2)\n",
        "    meshed = meshed.expand(b, -1, -1, -1)\n",
        "    return meshed\n",
        "\n",
        "\n",
        "def warp_image(source_image, deformation):\n",
        "    r\"\"\"warp the input image according to the deformation\n",
        "\n",
        "    Args:\n",
        "        source_image (tensor): source images to be warpped\n",
        "        deformation (tensor): deformations used to warp the images; value in range (-1, 1)\n",
        "    Returns:\n",
        "        output (tensor): the warpped images\n",
        "    \"\"\"\n",
        "    _, h_old, w_old, _ = deformation.shape\n",
        "    _, _, h, w = source_image.shape\n",
        "    if h_old != h or w_old != w:\n",
        "        deformation = deformation.permute(0, 3, 1, 2)\n",
        "        deformation = torch.nn.functional.interpolate(deformation, size=(h, w), mode='bilinear')\n",
        "        deformation = deformation.permute(0, 2, 3, 1)\n",
        "    return torch.nn.functional.grid_sample(source_image, deformation)\n",
        "\n",
        "\n",
        "def add_list(x):\n",
        "    r = None\n",
        "\n",
        "    for v in x:\n",
        "        r = v if r is None else r + v\n",
        "    return r\n",
        "\n",
        "def compute_cosine(x):\n",
        "    mm = np.matmul(x,x.T)\n",
        "    norm = np.linalg.norm(x,axis=-1,keepdims=True)\n",
        "    dis = mm / np.matmul(norm,norm.T)\n",
        "    return  dis - np.eye(*dis.shape)\n",
        "\n",
        "def compute_graph(cos_dis):\n",
        "    index = np.where(np.triu(cos_dis) >= 0.68)\n",
        "\n",
        "    dd = {}\n",
        "    vis = {}\n",
        "\n",
        "\n",
        "    for i in np.unique(index[0]):\n",
        "\n",
        "        if i not in vis:\n",
        "            vis[i] = i\n",
        "            dd[vis[i]] = [i]\n",
        "\n",
        "        for j in index[1][index[0]==i]:\n",
        "            if j not in vis:\n",
        "                vis[j] = vis[vis[i]]\n",
        "                dd[vis[i]] = dd.get(vis[i],[]) + [j]\n",
        "\n",
        "\n",
        "            elif j in vis and vis[vis[j]] != vis[vis[i]]:\n",
        "                old_root = vis[vis[j]]\n",
        "                for v in dd[vis[vis[j]]]:\n",
        "                    dd[vis[vis[i]]] += [v]\n",
        "                    vis[v] = vis[vis[i]]\n",
        "                del dd[old_root]\n",
        "\n",
        "    for k,v in dd.items():\n",
        "        dd[k] = list(set(v+[k]))\n",
        "    return dd,index"
      ],
      "metadata": {
        "id": "Ge9XQ1hn3WZu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#modelTrainer.py\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "class ModelTrainer:\n",
        "\n",
        "    def __init__(self,args):\n",
        "        super(ModelTrainer, self).__init__()\n",
        "        self.args = args\n",
        "        self.batch_size = args.batch_size\n",
        "        self.old_lr = args.lr\n",
        "        if args.rank == 0 :\n",
        "            self.vis = Visualizer(args)\n",
        "\n",
        "            if args.eval:\n",
        "                self.val_vis = Visualizer(args,\"val\")\n",
        "\n",
        "    # ## ===== ===== ===== ===== =====\n",
        "    # ## Train network\n",
        "    # ## ===== ===== ===== ===== =====\n",
        "\n",
        "    def train_network(self,train_loader,test_loader):\n",
        "\n",
        "        counter = 0\n",
        "        loss_dict = {}\n",
        "        acc_num = 0\n",
        "        mn_loss = float('inf')\n",
        "\n",
        "        steps = 0\n",
        "        begin_it = 0\n",
        "        if self.args.pretrain_path:\n",
        "            try:\n",
        "                begin_it = int(self.args.pretrain_path.split('/')[-1].split('-')[0]) + 1  #The epoch and step are extracted to resume training from the point where the pretraining left off.\n",
        "                steps = int(self.args.pretrain_path.split('/')[-1].split('-')[1].replace('.pth','')) + 1\n",
        "            except:\n",
        "                steps = 0\n",
        "                begin_it = 0\n",
        "            print(\"current steps: %d | one epoch steps: %d \"%(steps,self.args.mx_data_length))\n",
        "        if not self.args.apply_begin_it:   #If apply_begin_it is set to False, it forces begin_it and steps to be reset to 0, starting the training from scratch, regardless of the pretraining path.\n",
        "            begin_it = 0\n",
        "            steps = 0\n",
        "\n",
        "        for epoch in range(begin_it,self.args.max_epoch):\n",
        "\n",
        "            for ii,(data) in enumerate(train_loader):\n",
        "\n",
        "                tstart = time.time()\n",
        "\n",
        "                lq,hq = data\n",
        "                # Freeze discriminator weights\n",
        "                for param in self.netD.parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "                # Unfreeze generator weights\n",
        "                for param in self.netG.parameters():\n",
        "                    param.requires_grad = True\n",
        "\n",
        "                self.run_single_step(data,steps)\n",
        "                losses = self.get_latest_losses()\n",
        "\n",
        "                for key,val in losses.items():\n",
        "                    loss_dict[key] = loss_dict.get(key,0) + val.mean().item()\n",
        "\n",
        "                counter += 1\n",
        "\n",
        "\n",
        "                telapsed = time.time() - tstart\n",
        "\n",
        "\n",
        "                if steps % self.args.print_interval == 0 and self.args.rank == 0:\n",
        "\n",
        "                    for key,val in loss_dict.items():\n",
        "                        loss_dict[key] /= counter\n",
        "                    lr_rate = self.get_lr()\n",
        "                    print_dict = {**{\"time\":telapsed,\"lr\":lr_rate},\n",
        "                                    **loss_dict}\n",
        "                    self.vis.print_current_errors(epoch,steps,print_dict,telapsed)\n",
        "\n",
        "                    self.vis.plot_current_errors(print_dict,steps)\n",
        "\n",
        "                    display_data = self.select_img(self.get_latest_generated())\n",
        "\n",
        "                    self.vis.display_current_results(display_data,steps)\n",
        "\n",
        "                    loss_dict = {}\n",
        "                    counter = 0\n",
        "\n",
        "                    # torch.cuda.empty_cache()\n",
        "                if self.args.save_interval != 0 and steps % self.args.save_interval == 0 and \\\n",
        "                    self.args.rank == 0:\n",
        "                    self.saveParameters(os.path.join(self.args.checkpoint_path,\"%03d-%08d.pth\"%(epoch,steps)))\n",
        "                    self.current_epoch,self.current_step = epoch,steps\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                if self.args.eval and self.args.test_interval > 0 and steps % self.args.test_interval == 0:\n",
        "                    val_loss = self.evalution(test_loader,steps,epoch)\n",
        "\n",
        "                    if self.args.early_stop:\n",
        "\n",
        "                        acc_num,mn_loss,stop_flag = self.early_stop_wait(self.get_loss_from_val(val_loss),acc_num,mn_loss)\n",
        "                        if stop_flag:\n",
        "                            return\n",
        "\n",
        "                # print('******************memory:',psutil.virtual_memory()[3])\n",
        "                steps += 1\n",
        "            if self.args.rank == 0 :\n",
        "                self.saveParameters(os.path.join(self.args.checkpoint_path,\"%03d-%08d.pth\"%(epoch+1,steps)))\n",
        "                self.current_epoch,self.current_step = epoch+1,steps\n",
        "\n",
        "\n",
        "            if test_loader and self.args.eval and self.args.test_interval > 0:\n",
        "                val_loss = self.evalution(test_loader,steps,epoch)\n",
        "\n",
        "                if self.args.early_stop:\n",
        "\n",
        "                    acc_num,mn_loss,stop_flag = self.early_stop_wait(self.get_loss_from_val(val_loss),acc_num,mn_loss)\n",
        "                    if stop_flag:\n",
        "                        return\n",
        "\n",
        "\n",
        "        if self.args.rank == 0 :\n",
        "            self.vis.close()\n",
        "\n",
        "\n",
        "\n",
        "    def early_stop_wait(self,loss,acc_num,mn_loss):\n",
        "\n",
        "        if self.args.rank == 0:\n",
        "            if loss < mn_loss:\n",
        "                mn_loss = loss\n",
        "                cmd_one = 'cp -r %s %s'%(os.path.join(\n",
        "                        self.args.checkpoint_path,\"%03d-%08d.pth\"%(\n",
        "                                self.current_epoch,self.current_step)),\n",
        "                                                os.path.join(self.args.checkpoint_path,'final.pth'))\n",
        "                done_one = subprocess.Popen(cmd_one,stdout=subprocess.PIPE,shell=True)\n",
        "                done_one.wait()\n",
        "                acc_num = 0\n",
        "            else:\n",
        "                acc_num += 1\n",
        "\n",
        "            if self.args.dist:\n",
        "\n",
        "                if acc_num > self.args.stop_interval:\n",
        "                    signal = torch.tensor([0]).cuda()\n",
        "                else:\n",
        "                    signal = torch.tensor([1]).cuda()\n",
        "        else:\n",
        "            if self.args.dist:\n",
        "                signal = torch.tensor([1]).cuda()\n",
        "\n",
        "        if self.args.dist:\n",
        "            dist.all_reduce(signal)\n",
        "            value = signal.item()\n",
        "            if value >= int(os.environ.get(\"WORLD_SIZE\",\"1\")):\n",
        "                dist.all_reduce(torch.tensor([0]).cuda())\n",
        "                return acc_num,mn_loss,False\n",
        "            else:\n",
        "                return acc_num,mn_loss,True\n",
        "\n",
        "        else:\n",
        "            if acc_num > self.args.stop_interval:\n",
        "                return acc_num,mn_loss,True\n",
        "            else:\n",
        "                return acc_num,mn_loss,False\n",
        "\n",
        "    def run_single_step(self,data,steps):\n",
        "        data = self.process_input(data)\n",
        "        # self.netG.train()\n",
        "        # lq,gt = data #do i have to do this, let's see if it works\n",
        "        # self.run_discriminator_one_step(data,steps)\n",
        "        self.run_generator_one_step(data,steps)\n",
        "\n",
        "    # def run_discriminator_one_step(self, data, steps):\n",
        "    #     \"\"\"Run one step of discriminator training.\n",
        "    #     \"\"\"\n",
        "    #     requires_grad(self.netD, True)\n",
        "    #     if self.args.crop_components:\n",
        "    #         requires_grad(self.left_eye_d, True)\n",
        "    #         requires_grad(self.right_eye_d, True)\n",
        "    #         requires_grad(self.mouth_d, True)\n",
        "    #     requires_grad(self.netG, False)\n",
        "\n",
        "\n",
        "    #     lq, gt = data\n",
        "    #     D_losses = self.compute_d_loss(lq, gt, steps)\n",
        "\n",
        "    #     self.optimD.zero_grad()\n",
        "    #     D_losses['loss'].mean().backward()\n",
        "    #     self.optimD.step()\n",
        "    #     self.loss_dict.update(D_losses)\n",
        "    #     self.d_losses = D_losses\n",
        "\n",
        "    # def run_generator_one_step(self, data, step):\n",
        "    #     \"\"\"Run one step of generator training.\n",
        "    #     \"\"\"\n",
        "\n",
        "    #     requires_grad(self.netG, True)\n",
        "\n",
        "    #     if hasattr(self, 'netD'):\n",
        "    #          requires_grad(self.netD, False)\n",
        "    #     if hasattr(self, 'left_eye_d'):\n",
        "    #         requires_grad(self.left_eye_d, False)\n",
        "    #     if hasattr(self, 'right_eye_d'):\n",
        "    #        requires_grad(self.right_eye_d, False)\n",
        "    #     if hasattr(self, 'mouth_d'):\n",
        "    #         requires_grad(self.mouth_d, False)\n",
        "\n",
        "\n",
        "    #     lq, gt = data\n",
        "\n",
        "    #     G_losses,loss,fake = \\\n",
        "    #     self.compute_g_loss(lq,gt,step)\n",
        "    #     self.optimG.zero_grad()\n",
        "    #     loss.mean().backward()\n",
        "    #     self.optimG.step()\n",
        "    #     if hasattr(self, 'g_ema'):\n",
        "    #         accumulate(self.g_ema,self.netG,self.accum,self.args.use_buffer)\n",
        "    #     self.g_losses = G_losses\n",
        "    #     self.generator = [lq.detach(),fake.detach(),gt.detach()]\n",
        "\n",
        "\n",
        "    # def compute_d_loss(self,lq, gt, steps):\n",
        "    #     pass\n",
        "\n",
        "    # def compute_g_loss(self,lq, gt, steps):\n",
        "    #     pass\n",
        "\n",
        "    # def get_latest_losses(self):\n",
        "    #     return self.loss_dict\n",
        "\n",
        "    # def test_network(self):\n",
        "    #     pass\n",
        "    # def process_input(self,data):\n",
        "    #     return data\n",
        "    # def save_network(self,epoch,steps,best=False):\n",
        "    #     pass\n",
        "\n",
        "    def convert_img(img,unit=False):  #imported from utils.py\n",
        "\n",
        "        img = (img + 1) * 0.5\n",
        "        if unit:\n",
        "            return torch.clamp(img*255+0.5,0,255)\n",
        "\n",
        "        return torch.clamp(img,0,1)\n",
        "\n",
        "\n",
        "    def select_img(self,data,name='fake',axis=2):\n",
        "        if data is None:\n",
        "            return None\n",
        "        cat_img = []\n",
        "        for v in data:\n",
        "            cat_img.append(v.detach().cpu())\n",
        "\n",
        "        cat_img = torch.cat(cat_img,-1)\n",
        "        cat_img = torch.cat(torch.split(cat_img,1,dim=0),axis)[0]\n",
        "\n",
        "        return {name:convert_img(cat_img)}\n",
        "\n",
        "\n",
        "\n",
        "    ##################################################################\n",
        "    # Helper functions\n",
        "    ##################################################################\n",
        "\n",
        "    def get_loss_from_val(self,loss):\n",
        "        return loss['loss']\n",
        "\n",
        "    def get_show_inp(self,data):\n",
        "        if not isinstance(data,list):\n",
        "            return [data]\n",
        "        return data\n",
        "\n",
        "    def use_ddp(self,model,dist=True,use_params=False):\n",
        "        if model is None:\n",
        "            return None,None\n",
        "        if not dist:\n",
        "            return model,model\n",
        "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model) #\n",
        "        # model = DDP(model,broadcast_buffers=False,find_unused_parameters=True) # find_unused_parameters\n",
        "        model = DDP(model,\n",
        "                    broadcast_buffers=False,\n",
        "                    find_unused_parameters=use_params\n",
        "                    )\n",
        "        model_on_one_gpu = model.module\n",
        "        return model,model_on_one_gpu\n",
        "\n",
        "    def process_input(self,data):\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            if isinstance(data,list):\n",
        "                data = [x.cuda() for x in data]\n",
        "            else:\n",
        "                data = data.cuda()\n",
        "        return data"
      ],
      "metadata": {
        "id": "9dl_O9hFkcOy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GFPGAN Trainer.py\n",
        "import torch.distributed as dist\n",
        "from itertools import chain\n",
        "from torchvision.ops import roi_align\n",
        "import pdb\n",
        "from torchvision.models import vgg19, vgg16\n",
        "# from trainer.ModelTrainer import ModelTrainer\n",
        "# from model.generator import GFPGANv1Clean\n",
        "# from model.discriminator import StyleGAN2Discriminator,FacialComponentDiscriminator\n",
        "# from utils.utils import *\n",
        "# from model.loss import *\n",
        "# from model.third.arcface_arch import ResNetArcFace\n",
        "\n",
        "class GFPTrainer(ModelTrainer):\n",
        "\n",
        "    def __init__(self, args):\n",
        "        super().__init__(args)\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = 'cuda'\n",
        "\n",
        "        self.netG = GFPGANv1Clean(out_size=args.out_size,\n",
        "                                channel_multiplier=args.channel_multiplier,\n",
        "                                fix_decoder=args.fix_decoder,\n",
        "                                input_is_latent=args.input_is_latent,\n",
        "                                different_w=args.different_w,\n",
        "                                sft_half=args.sft_half\n",
        "                                ).to(self.device)\n",
        "\n",
        "        self.netD = StyleGAN2Discriminator(1024).to(self.device)\n",
        "        self.left_eye_d = self.right_eye_d = self.mouth_d = None\n",
        "\n",
        "        if self.args.crop_components:\n",
        "            self.left_eye_d = FacialComponentDiscriminator().to(self.device)\n",
        "            self.right_eye_d = FacialComponentDiscriminator().to(self.device)\n",
        "            self.mouth_d = FacialComponentDiscriminator().to(self.device)\n",
        "            init_weights(self.left_eye_d,'xavier_uniform')\n",
        "            init_weights(self.right_eye_d,'xavier_uniform')\n",
        "            init_weights(self.mouth_d,'xavier_uniform')\n",
        "            self.PartGANLoss = GANLoss(gan_type=args.part_gan_type,\n",
        "                               loss_weight=args.lambda_gan_part)\n",
        "\n",
        "\n",
        "\n",
        "        self.g_ema = GFPGANv1Clean(out_size=args.out_size,\n",
        "                                channel_multiplier=args.channel_multiplier,\n",
        "                                fix_decoder=args.fix_decoder,\n",
        "                                input_is_latent=args.input_is_latent,\n",
        "                                different_w=args.different_w,\n",
        "                                sft_half=args.sft_half\n",
        "                                ).to(self.device)\n",
        "        self.g_ema.eval()\n",
        "\n",
        "        init_weights(self.netD,'xavier_uniform')\n",
        "\n",
        "        init_weights(self.netG,'xavier_uniform')\n",
        "\n",
        "        self.optimG,self.optimD = self.create_optimizer()  #This line creates two optimizers: optimG for the generator (netG) and optimD for the discriminator (netD).\n",
        "\n",
        "        if self.args.scratch:\n",
        "            self.load_scratch()\n",
        "\n",
        "        if args.pretrain_path is not None:\n",
        "            self.loadParameters(args.pretrain_path)\n",
        "\n",
        "        accumulate(self.g_ema,self.netG,0,args.use_buffer)\n",
        "\n",
        "        self.netG,self.netG_module = self.use_ddp(self.netG,args.dist,True)   #This line uses self.use_ddp() to wrap the generator model (netG) with Distributed Data Parallel (DDP), which is useful for parallel training across multiple GPUs.\n",
        "        self.netD,self.netD_module = self.use_ddp(self.netD,args.dist)\n",
        "        self.left_eye_d,self.left_eye_d_module = self.use_ddp(self.left_eye_d,args.dist)\n",
        "        self.right_eye_d,self.right_eye_d_module = self.use_ddp(self.right_eye_d,args.dist)\n",
        "        self.mouth_d,self.mouth_d_module = self.use_ddp(self.mouth_d,args.dist)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.L1Loss = nn.L1Loss()\n",
        "\n",
        "        if self.args.perloss:\n",
        "            self.PerLoss = PerceptualLoss(args.layer_weights,args.vgg_type,\n",
        "                                        args.use_input_norm,\n",
        "                                        args.range_norm,\n",
        "                                        perceptual_weight=args.lambda_perceptual,\n",
        "                                        style_weight=args.lambda_style,\n",
        "                                        criterion=args.criterion).to(self.device)\n",
        "            requires_grad(self.PerLoss,False)\n",
        "            self.PerLoss.eval()\n",
        "\n",
        "        if self.args.idloss:\n",
        "            self.idModel = ResNetArcFace(self.args.out_size, self.args.id_block,\n",
        "                                         self.args.id_layers,\n",
        "                                         self.args.id_use_se).to(self.device)\n",
        "            # self.idModel.load_state_dict(torch.load(self.args.id_model))\n",
        "            load_state_dict_with_prefix_removal(self.idModel, self.args.id_model, self.device)\n",
        "            requires_grad(self.idModel,False)\n",
        "            self.idModel.eval()\n",
        "\n",
        "        self.GANLoss = GANLoss(gan_type=args.gan_type,\n",
        "                               loss_weight=args.lambda_gan)\n",
        "\n",
        "        self.accum = 0.5 ** (32 / (10 * 1000))\n",
        "\n",
        "    def create_optimizer(self):\n",
        "\n",
        "        if self.args.mode == \"decoder\":\n",
        "            g_params = self.netG.stylegan_decoder.parameters()\n",
        "        elif self.args.mode == \"encoder\":\n",
        "            requires_grad(self.netG.stylegan_decoder,False)\n",
        "            g_params = [f for f in self.netG.parameters() if f.requires_grad]\n",
        "        else:\n",
        "            g_params = self.netG.parameters()\n",
        "\n",
        "        g_optim = torch.optim.Adam(\n",
        "                    g_params,\n",
        "                    lr=self.args.g_lr,\n",
        "                    betas=(self.args.beta1, self.args.beta2),\n",
        "                    )\n",
        "\n",
        "        if self.args.crop_components:\n",
        "            d_params = chain(self.left_eye_d.parameters(),\n",
        "                             self.right_eye_d.parameters(),\n",
        "                             self.mouth_d.parameters(),\n",
        "                             self.netD.parameters())\n",
        "        else:\n",
        "            d_params = self.netD.parameters()\n",
        "        d_optim = torch.optim.Adam(\n",
        "                    d_params,\n",
        "                    lr=self.args.d_lr,\n",
        "                    betas=(self.args.beta1, self.args.beta2),\n",
        "                    )\n",
        "\n",
        "        return  g_optim,d_optim\n",
        "\n",
        "\n",
        "    def run_single_step(self, data, steps):\n",
        "\n",
        "        self.netG.train()\n",
        "        lq,gt = data  #this is the only line added here, keep or delete it ?? let's see later\n",
        "        # self.run_discriminator_one_step(data,steps)\n",
        "        super().run_single_step(data,steps)   #calls run_single_step function from the parent class of GFPGANv1 class i.e. nn.Module\n",
        "\n",
        "#feb 25 11pm **ModelTrainer.py class has this function, so we call from there and comment it in this class here.\n",
        "    # def run_discriminator_one_step(self, data,step):\n",
        "\n",
        "    #     requires_grad(self.netG, False)\n",
        "    #     requires_grad(self.netD, True)\n",
        "    #     requires_grad(self.left_eye_d, True)\n",
        "    #     requires_grad(self.right_eye_d, True)\n",
        "    #     requires_grad(self.mouth_d, True)\n",
        "\n",
        "    #     # lq,gt,loc_left_eye, loc_right_eye, loc_mouth = data\n",
        "    #     lq, gt = data #the __getitem__() only return two output elements i.e. img_lq and img_hq\n",
        "    #     fake, out_rgbs = self.netG(lq, return_rgb=False)\n",
        "    #     self.optimD.zero_grad()\n",
        "    #     # d_loss,D_losses = self.compute_d_loss(fake,gt,loc_left_eye, loc_right_eye, loc_mouth)\n",
        "    #     d_loss,D_losses = self.compute_d_loss(fake,gt)\n",
        "    #     d_loss.backward()\n",
        "    #     self.optimD.step()\n",
        "\n",
        "    #     if step % self.args.net_d_reg_every == 0:\n",
        "    #         gt.requires_grad = True\n",
        "    #         real_pred,_ = self.netD(gt)\n",
        "    #         l_d_r1 = r1_penalty(real_pred, gt)\n",
        "    #         l_d_r1 = (self.args.r1_reg_weight / 2 * l_d_r1 * self.args.net_d_reg_every + 0 * real_pred[0])\n",
        "    #         D_losses['l_d_r1'] = l_d_r1.detach().mean()\n",
        "    #         l_d_r1.backward()\n",
        "\n",
        "    #     self.d_losses = D_losses\n",
        "\n",
        "\n",
        "    def run_generator_one_step(self, data,step):\n",
        "\n",
        "\n",
        "        requires_grad(self.netG, True)\n",
        "\n",
        "        requires_grad(self.netD, False)\n",
        "        requires_grad(self.left_eye_d, False)\n",
        "        requires_grad(self.right_eye_d, False)\n",
        "        requires_grad(self.mouth_d, False)\n",
        "\n",
        "\n",
        "        # lq,gt,loc_left_eye, loc_right_eye, loc_mouth = data\n",
        "        lq, gt = data\n",
        "\n",
        "        G_losses,loss,fake = \\\n",
        "        self.compute_g_loss(lq,gt,step)\n",
        "        # self.compute_g_loss(lq,gt,loc_left_eye, loc_right_eye, loc_mouth,step)\n",
        "        # pdb.set_trace()\n",
        "        self.optimG.zero_grad()\n",
        "        loss.mean().backward()\n",
        "        self.optimG.step()\n",
        "\n",
        "        accumulate(self.g_ema,self.netG_module,self.accum,self.args.use_buffer)\n",
        "\n",
        "        self.g_losses = G_losses\n",
        "\n",
        "        self.generator = [lq.detach(),fake.detach(),gt.detach()]  #The .detach() method is used to prevent these tensors (lq, fake, gt) from being part of the computational graph. The detach() function is crucial because it ensures that no gradients are computed for these tensors in the current forward pass.\n",
        "\n",
        "\n",
        "\n",
        "    def evalution(self,test_loader,steps,epoch):\n",
        "\n",
        "        loss_dict = {}\n",
        "        index = random.randint(0,len(test_loader)-1)\n",
        "        counter = 0\n",
        "        with torch.no_grad():\n",
        "            for i,data in enumerate(test_loader):\n",
        "\n",
        "                data = self.process_input(data)\n",
        "                # lq,gt,loc_left_eye, loc_right_eye, loc_mouth = data\n",
        "                lq,gt = data\n",
        "                G_losses,loss,fake = \\\n",
        "                self.compute_g_loss(lq,gt,steps)\n",
        "                # self.compute_g_loss(lq,gt,loc_left_eye, loc_right_eye, loc_mouth,steps)\n",
        "\n",
        "                # loss,D_losses = self.compute_d_loss(fake,gt,loc_left_eye, loc_right_eye, loc_mouth)\n",
        "                loss,D_losses = self.compute_d_loss(fake,gt,steps)\n",
        "                G_losses = {**G_losses,**D_losses}\n",
        "                for k,v in G_losses.items():\n",
        "                    loss_dict[k] = loss_dict.get(k,0) + v.detach()\n",
        "                if i == index and self.args.rank == 0 :\n",
        "                    ema_oup,_ = self.g_ema(lq,return_rgb=False)\n",
        "\n",
        "                    show_data = [lq.detach(),\n",
        "                                        fake.detach(),\n",
        "                                        ema_oup.detach(),\n",
        "                                        gt.detach()]\n",
        "\n",
        "                    self.val_vis.display_current_results(self.select_img(show_data,size=show_data[-1].shape[-1]),steps)\n",
        "                counter += 1\n",
        "\n",
        "\n",
        "        for key,val in loss_dict.items():\n",
        "            loss_dict[key] /= counter\n",
        "\n",
        "        if self.args.dist:\n",
        "            # if self.args.rank == 0 :\n",
        "            dist_losses = loss_dict.copy()\n",
        "            for key,val in loss_dict.items():\n",
        "\n",
        "                dist.reduce(dist_losses[key],0)\n",
        "                value = dist_losses[key].item()\n",
        "                loss_dict[key] = value / self.args.world_size\n",
        "\n",
        "        if self.args.rank == 0 :\n",
        "            self.val_vis.plot_current_errors(loss_dict,steps)\n",
        "            self.val_vis.print_current_errors(epoch,steps,loss_dict,0)\n",
        "\n",
        "        return loss_dict\n",
        "\n",
        "\n",
        "#we won't compute discriminator loss because we won't train the discriminator for our usecase! feb 26\n",
        "    # def compute_d_loss(self,fake,gt,loc_left_eye, loc_right_eye, loc_mouth):\n",
        "\n",
        "    #       fake_d_pred,_ = self.netD(fake.detach())\n",
        "    #       real_d_pred,_ = self.netD(gt)\n",
        "    #       real_score = self.GANLoss(real_d_pred, True, is_disc=True)\n",
        "    #       fake_score = self.GANLoss(fake_d_pred, False, is_disc=True)\n",
        "    #       l_d = real_score + fake_score\n",
        "    #       D_losses = {\n",
        "    #           'd':l_d,\n",
        "    #           'd_real':real_score,\n",
        "    #           'd_fake':fake_score\n",
        "    #       }\n",
        "\n",
        "\n",
        "    #       if self.args.crop_components:\n",
        "    #           real_left_eyes,\\\n",
        "    #           real_right_eyes,\\\n",
        "    #           real_mouths,\\\n",
        "    #           fake_left_eyes,\\\n",
        "    #           fake_right_eyes,\\\n",
        "    #           fake_mouths = self.get_roi_regions(fake,gt,[loc_left_eye, loc_right_eye, loc_mouth])\n",
        "    #           # left eye\n",
        "    #           fake_d_pred,_ = self.left_eye_d(fake_left_eyes.detach())\n",
        "    #           real_d_pred,_ = self.left_eye_d(real_left_eyes)\n",
        "    #           real_left_eye_score = self.PartGANLoss(real_d_pred,True, is_disc=True)\n",
        "    #           fake_left_eye_score = self.GANLoss(fake_d_pred, False, is_disc=True)\n",
        "    #           l_d_left_eye = real_left_eye_score + fake_left_eye_score\n",
        "    #           l_d += l_d_left_eye\n",
        "    #           D_losses['d_left_eye_real'] = real_left_eye_score\n",
        "    #           D_losses['d_left_eye_fake'] = fake_left_eye_score\n",
        "    #           D_losses['d_left_eye'] = l_d_left_eye\n",
        "\n",
        "    #           # right eye\n",
        "    #           fake_d_pred,_ = self.right_eye_d(fake_right_eyes.detach())\n",
        "    #           real_d_pred,_ = self.right_eye_d(real_right_eyes)\n",
        "    #           real_right_eye_score = self.PartGANLoss(real_d_pred,True, is_disc=True)\n",
        "    #           fake_right_eye_score = self.GANLoss(fake_d_pred, False, is_disc=True)\n",
        "    #           l_d_right_eye = real_right_eye_score + fake_right_eye_score\n",
        "    #           l_d += l_d_right_eye\n",
        "    #           D_losses['d_right_eye_real'] = real_right_eye_score\n",
        "    #           D_losses['d_right_eye_fake'] = fake_right_eye_score\n",
        "    #           D_losses['d_right_eye'] = l_d_right_eye\n",
        "\n",
        "    #           # mouth\n",
        "    #           fake_d_pred,_ = self.mouth_d(fake_mouths.detach())\n",
        "    #           real_d_pred,_ = self.mouth_d(real_mouths)\n",
        "    #           real_mouth_score = self.PartGANLoss(real_d_pred,True, is_disc=True)\n",
        "    #           fake_mouth_score = self.GANLoss(fake_d_pred, False, is_disc=True)\n",
        "    #           l_d_mouth = real_mouth_score + fake_mouth_score\n",
        "    #           l_d += l_d_mouth\n",
        "    #           D_losses['d_mouth_real'] = real_mouth_score\n",
        "    #           D_losses['d_mouth_fake'] = fake_mouth_score\n",
        "    #           D_losses['d_mouth'] = l_d_mouth\n",
        "\n",
        "    #       return l_d,D_losses\n",
        "\n",
        "\n",
        "    # def compute_g_loss(self,lq,gt,loc_left_eye, loc_right_eye, loc_mouth,step): #because we don't use facial landmarks for eyes and mouth\n",
        "    def compute_g_loss(self,lq,gt,step):\n",
        "\n",
        "            G_losses = {}\n",
        "            loss = 0\n",
        "            fake, out_rgbs = self.netG(lq, return_rgb=False)\n",
        "\n",
        "            if self.args.pixloss:\n",
        "                pixloss = self.L1Loss(fake,gt) * self.args.lambda_l1\n",
        "                loss += pixloss\n",
        "                G_losses['g_pixloss'] = pixloss\n",
        "\n",
        "            if self.args.perloss:\n",
        "\n",
        "                l_g_percep, l_g_style = self.PerLoss(fake, gt)\n",
        "                if l_g_percep is not None:\n",
        "                    loss += l_g_percep\n",
        "                    G_losses['g_percep_loss'] = l_g_percep\n",
        "                if l_g_style is not None:\n",
        "                    loss += l_g_style\n",
        "                    G_losses['g_style_loss'] = l_g_style\n",
        "\n",
        "            if self.args.idloss:\n",
        "                out_gray = self.gray_resize_for_identity(fake)\n",
        "                gt_gray = self.gray_resize_for_identity(gt)\n",
        "\n",
        "                identity_gt = self.idModel(gt_gray).detach()\n",
        "                identity_out = self.idModel(out_gray)\n",
        "                l_identity = self.L1Loss(identity_out, identity_gt) * self.args.lambda_id\n",
        "                loss += l_identity\n",
        "                G_losses['g_id_loss'] = l_identity\n",
        "\n",
        "            if self.args.crop_components:\n",
        "                real_left_eyes,\\\n",
        "                real_right_eyes,\\\n",
        "                real_mouths,\\\n",
        "                fake_left_eyes,\\\n",
        "                fake_right_eyes,\\\n",
        "                fake_mouths = self.get_roi_regions(fake,gt,[loc_left_eye, loc_right_eye, loc_mouth])\n",
        "\n",
        "                # left eye\n",
        "                fake_left_eye, fake_left_eye_feats = self.left_eye_d(fake_left_eyes, return_feats=True)\n",
        "                l_g_gan = self.PartGANLoss(fake_left_eye, True, is_disc=False)\n",
        "                loss += l_g_gan\n",
        "                G_losses['g_gan_left_eye'] = l_g_gan\n",
        "\n",
        "                # right eye\n",
        "                fake_right_eye, fake_right_eye_feats = self.right_eye_d(fake_right_eyes, return_feats=True)\n",
        "                l_g_gan = self.PartGANLoss(fake_right_eye, True, is_disc=False)\n",
        "                loss += l_g_gan\n",
        "                G_losses['g_gan_right_eye'] = l_g_gan\n",
        "                # mouth\n",
        "                fake_mouth, fake_mouth_feats = self.mouth_d(fake_mouths, return_feats=True)\n",
        "                l_g_gan = self.PartGANLoss(fake_mouth, True, is_disc=False)\n",
        "                loss += l_g_gan\n",
        "                G_losses['g_gan_mouth'] = l_g_gan\n",
        "\n",
        "                if self.args.comp_style_weight  > 0:\n",
        "                    # get gt feat\n",
        "                    _, real_left_eye_feats = self.left_eye_d(real_left_eyes, return_feats=True)\n",
        "                    _, real_right_eye_feats = self.right_eye_d(real_right_eyes, return_feats=True)\n",
        "                    _, real_mouth_feats = self.mouth_d(real_mouths, return_feats=True)\n",
        "\n",
        "                    def _comp_style(feat, feat_gt, criterion):\n",
        "                        return criterion(self._gram_mat(feat[0]), self._gram_mat(\n",
        "                            feat_gt[0].detach())) * 0.5 + criterion(\n",
        "                                self._gram_mat(feat[1]), self._gram_mat(feat_gt[1].detach()))\n",
        "\n",
        "                    # facial component style loss\n",
        "                    comp_style_loss = 0\n",
        "                    comp_style_loss += _comp_style(fake_left_eye_feats, real_left_eye_feats, self.L1Loss)\n",
        "                    comp_style_loss += _comp_style(fake_right_eye_feats, real_right_eye_feats, self.L1Loss)\n",
        "                    comp_style_loss += _comp_style(fake_mouth_feats, real_mouth_feats, self.L1Loss)\n",
        "                    comp_style_loss = comp_style_loss * self.args.comp_style_weight\n",
        "                    loss += comp_style_loss\n",
        "                    G_losses['g_comp_style_loss'] = comp_style_loss\n",
        "\n",
        "\n",
        "            # gan loss\n",
        "            fake_g_pred,fake_feat = self.netD(fake)\n",
        "            l_g_gan = self.GANLoss(fake_g_pred, True, is_disc=False)\n",
        "            loss += l_g_gan\n",
        "            G_losses['g_gan'] = l_g_gan\n",
        "\n",
        "            if self.args.featloss:\n",
        "                _,real_feat = self.netD(gt)\n",
        "                fm_loss = 0\n",
        "                for r_f,f_f in zip(real_feat,fake_feat):\n",
        "                    fm_loss += self.L1Loss(r_f,f_f)\n",
        "\n",
        "                fm_loss = fm_loss / len(real_feat) * self.args.lambda_fm\n",
        "                loss += fm_loss\n",
        "                G_losses['g_fm_loss'] = fm_loss\n",
        "\n",
        "            G_losses['loss'] = loss\n",
        "\n",
        "            return G_losses,loss,fake\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_latest_losses(self):\n",
        "      losses = {}\n",
        "      if hasattr(self,'G_losses'):\n",
        "          for loss_name in self.G_losses:\n",
        "              losses[loss_name] = self.G_losses[loss_name].item()\n",
        "      if hasattr(self,'D_losses'):\n",
        "          for loss_name in self.D_losses:\n",
        "             losses[loss_name] = self.D_losses[loss_name].item()\n",
        "      return losses\n",
        "\n",
        "\n",
        "    def get_latest_generated(self):\n",
        "\n",
        "        return self.generator\n",
        "\n",
        "    def get_loss_from_val(self,loss):\n",
        "        return loss['g_comp_style_loss'] + \\\n",
        "              loss['g_id_loss'] +\\\n",
        "              loss['g_percep_loss'] +\\\n",
        "              loss['g_pixloss'] + loss['g_fm_loss']\n",
        "\n",
        "\n",
        "    def loadParameters(self,path):\n",
        "        ckpt = torch.load(path, map_location=lambda storage, loc: storage)\n",
        "\n",
        "        self.netG.load_state_dict(ckpt['G'],strict=True)  #he strict=True argument ensures that the model's current architecture exactly matches the structure of the saved model in the checkpoint.\n",
        "#It means, if there are missing or extra keys in ckpt['G'] or ckpt['D'] compared to self.netG and self.netD, an error will be raised.\n",
        "        self.netD.load_state_dict(ckpt['D'],strict=True)\n",
        "\n",
        "        if self.args.crop_components:\n",
        "            self.left_eye_d.load_state_dict(ckpt['left_eye_d'])\n",
        "            self.right_eye_d.load_state_dict(ckpt['right_eye_d'])\n",
        "            self.mouth_d.load_state_dict(ckpt['mouth_d'])\n",
        "        try:\n",
        "            self.optimG.load_state_dict(ckpt['g_optim'])\n",
        "            self.optimD.load_state_dict(ckpt['d_optim'])\n",
        "        except:\n",
        "            print(\"'you change train mode!!! It means the state_dict of pre-trained optimizers 'g_optim' and 'd_optim' has not been loaded and the Optimizers will start to train from scratch. It might cause spike in losses in initial epochs.\")  #this has been displaying when i am starting to train\n",
        "#it means that the optimizers state_dict 'g_optim' and 'd_optim' are not loaded properly thus, the optimizer states are not restored, meaning training will start from scratch for the optimizers.\n",
        "#Issues if optimizers previous state_dict is not loaded:-> Since, the model 'final.pth' was well trained i.e. the training was near convergence, and the optim_state_dict are not loaded so, we loose the Adaptive Momentum learned by the optimizers in its previous training. Hence, it will take longer to recover, and loss will spike initially.\n",
        "\n",
        "\n",
        "    def load_scratch(self):\n",
        "\n",
        "        state_dict = torch.load(self.args.scratch_gan_path)['params_ema']\n",
        "        model_dict = self.netG.state_dict()\n",
        "        state_dict = {k:v for k,v in state_dict.items() if k in model_dict.keys()}\n",
        "        model_dict.update(state_dict)\n",
        "        self.netG.load_state_dict(model_dict)\n",
        "\n",
        "        self.netD.load_state_dict(torch.load(self.args.scratch_d_path))\n",
        "\n",
        "        if self.args.crop_components:\n",
        "            self.left_eye_d.load_state_dict(torch.load(self.args.scratch_left_eye_path)['params'])\n",
        "            self.right_eye_d.load_state_dict(torch.load(self.args.scratch_right_eye_path)['params'])\n",
        "            self.mouth_d.load_state_dict(torch.load(self.args.scratch_mouth_path)['params'])\n",
        "\n",
        "\n",
        "\n",
        "    def saveParameters(self,path):\n",
        "\n",
        "        save_dict = {\n",
        "            \"G\": self.netG_module.state_dict(),\n",
        "            'g_ema':self.g_ema.state_dict(),\n",
        "            \"g_optim\": self.optimG.state_dict(),\n",
        "            'D':self.netD_module.state_dict(),\n",
        "            'd_optim': self.optimD.state_dict(),\n",
        "            \"args\": self.args\n",
        "        }\n",
        "\n",
        "        if self.args.crop_components:\n",
        "            save_dict['left_eye_d'] = self.left_eye_d_module.state_dict()\n",
        "            save_dict['right_eye_d'] = self.right_eye_d_module.state_dict()\n",
        "            save_dict['mouth_d'] = self.mouth_d_module.state_dict()\n",
        "\n",
        "        torch.save(\n",
        "                   save_dict,\n",
        "                   path\n",
        "                )\n",
        "\n",
        "    def get_lr(self):\n",
        "        return self.optimG.state_dict()['param_groups'][0]['lr']\n",
        "\n",
        "\n",
        "    def select_img(self, data,size=None, name='fake', axis=2):\n",
        "        if size is None:\n",
        "            size = self.args.size\n",
        "        data = [F.adaptive_avg_pool2d(x,size) for x in data] #applies Adaptive Average Pooling to all images in data. This ensures that all images are resized to the required size while preserving the aspect ratio.\n",
        "        return super().select_img(data, name, axis)\n",
        "\n",
        "    def freeze_models(self,frozen_params):\n",
        "\n",
        "        for n in frozen_params:  #frozen_params is expected to be a list of layer names (e.g., [\"conv1\", \"conv2\"]).\n",
        "            for p in self.netG.__getattr__(n).parameters():  #self.netG.__getattr__(n) gets the layer n from the generator (netG).\n",
        "                p.requires_grad = False\n",
        "\n",
        "\n",
        "#Not used for our use case feb 26\n",
        "    # def get_roi_regions(self,fake,gt,location,eye_out_size=80, mouth_out_size=120):\n",
        "\n",
        "    #     loc_left_eyes,loc_right_eyes,loc_mouths = location\n",
        "    #     rois_eyes = []\n",
        "    #     rois_mouths = []\n",
        "    #     for b in range(loc_left_eyes.size(0)):  # loop for batch size\n",
        "    #         # left eye and right eye\n",
        "    #         img_inds = loc_left_eyes.new_full((2, 1), b)\n",
        "    #         bbox = torch.stack([loc_left_eyes[b, :], loc_right_eyes[b, :]], dim=0)  # shape: (2, 4)\n",
        "    #         rois = torch.cat([img_inds, bbox], dim=-1)  # shape: (2, 5)\n",
        "    #         rois_eyes.append(rois)\n",
        "    #         # mouse\n",
        "    #         img_inds = loc_left_eyes.new_full((1, 1), b)\n",
        "    #         rois = torch.cat([img_inds, loc_mouths[b:b + 1, :]], dim=-1)  # shape: (1, 5)\n",
        "    #         rois_mouths.append(rois)\n",
        "\n",
        "    #     rois_eyes = torch.cat(rois_eyes, 0).to(self.device)\n",
        "    #     rois_mouths = torch.cat(rois_mouths, 0).to(self.device)\n",
        "\n",
        "    #     # real images\n",
        "    #     all_eyes = roi_align(gt, boxes=rois_eyes, output_size=eye_out_size)\n",
        "    #     real_left_eyes = all_eyes[0::2, :, :, :]\n",
        "    #     real_right_eyes = all_eyes[1::2, :, :, :]\n",
        "    #     real_mouths = roi_align(gt, boxes=rois_mouths, output_size=mouth_out_size)\n",
        "    #     # output\n",
        "    #     all_eyes = roi_align(fake, boxes=rois_eyes, output_size=eye_out_size)\n",
        "    #     fake_left_eyes = all_eyes[0::2, :, :, :]\n",
        "    #     fake_right_eyes = all_eyes[1::2, :, :, :]\n",
        "    #     fake_mouths = roi_align(fake, boxes=rois_mouths, output_size=mouth_out_size)\n",
        "    #     return real_left_eyes,real_right_eyes,real_mouths,fake_left_eyes,fake_right_eyes,fake_mouths\n",
        "\n",
        "#The Gram matrix captures the correlation between different feature maps, making it useful for: Feature correlation analysis, Feature Analysis\n",
        "    def _gram_mat(self, x):\n",
        "        \"\"\"Calculate Gram matrix.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Tensor with shape of (n, c, h, w).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Gram matrix.\n",
        "        \"\"\"\n",
        "        n, c, h, w = x.size()\n",
        "        features = x.view(n, c, w * h)  #This converts each feature map (channel) into a long vector (flattened spatial dimensions).\n",
        "        features_t = features.transpose(1, 2)  #Transposes features to shape (n, h*w, c).\n",
        "        gram = features.bmm(features_t) / (c * h * w)  #computes Gram Matrix by performing batch matrix multiplication(BMM) between features and features_t.\n",
        "        return gram\n",
        "\n",
        "\n",
        "    def gray_resize_for_identity(self, out, size=128):\n",
        "        out_gray = (0.2989 * out[:, 0, :, :] + 0.5870 * out[:, 1, :, :] + 0.1140 * out[:, 2, :, :])\n",
        "        out_gray = out_gray.unsqueeze(1)\n",
        "        out_gray = F.interpolate(out_gray, (size, size), mode='bilinear', align_corners=False)\n",
        "        return out_gray\n"
      ],
      "metadata": {
        "id": "-I4OwoPNgmCo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#config.py\n",
        "\n",
        "class Params:\n",
        "    def __init__(self):\n",
        "\n",
        "        self.name = 'GFPGAN'\n",
        "        self.mode = 'decoder'\n",
        "        self.pretrain_path = '/content/drive/MyDrive/GFPGAN/pretrained/final.pth'\n",
        "        self.scratch_gan_path = ''  #'pretrained_models/GFPGANv1.4.pth'\n",
        "        self.scratch_d_path = ''  #'pretrained_models/d.pth'\n",
        "        self.scratch_left_eye_path = ''  #'pretrained_models/GFPGANv1_net_d_left_eye.pth'F\n",
        "        self.scratch_right_eye_path = ''  #'pretrained_models/GFPGANv1_net_d_right_eye.pth'\n",
        "        self.scratch_mouth_path = ''  # 'pretrained_models/GFPGANv1_net_d_mouth.pth'\n",
        "        self.id_model = '/content/drive/MyDrive/GFPGAN/pretrained/arcface_resnet18.pth'  #'pretrained_models/arcface_resnet18.pth'\n",
        "        self.img_root = '/content/drive/MyDrive/GFPGAN/sample_and_results/100_samples_hq' # ffhq data\n",
        "        self.train_hq_root = '/content/drive/MyDrive/GFPGAN/sample_and_results/100_samples_hq' # 1024 data by some api\n",
        "        self.train_lq_root = '/content/drive/MyDrive/GFPGAN/sample_and_results/damaged_images' # custom data\n",
        "        # self.train_lmk_base = '' # lmk info\n",
        "        # self.val_lmk_base = ''\n",
        "        # self.val_lq_root = '/content/drive/MyDrive/GFPGAN/dataset/train/high_quality/hq_images'\n",
        "        # self.val_hq_root = '/content/drive/MyDrive/GFPGAN/dataset/train/low_quality/lq_images'\n",
        "        # data\n",
        "        self.crop_components = False\n",
        "        self.eye_enlarge_ratio = 1.4\n",
        "\n",
        "\n",
        "        self.mean = torch.tensor([0.5, 0.5, 0.5])\n",
        "        self.std = torch.tensor([0.5, 0.5, 0.5])\n",
        "        self.out_size = 512\n",
        "        # self.size = 1024\n",
        "        self.size = 512 #size changed to 512 for compatibility to calculate loss functions on feb18\n",
        "\n",
        "        # self.blur_kernel_size = 41\n",
        "        # self.kernel_list = ['iso', 'aniso']\n",
        "        # self.kernel_prob = [0.5, 0.5]\n",
        "        # self.blur_sigma = [0.1, 10]\n",
        "        # self.downsample_range = [0.8, 8]\n",
        "        # self.noise_range = [0, 20]\n",
        "        # self.jpeg_range = [60, 100]\n",
        "        # self.use_flip = True\n",
        "\n",
        "        self.use_buffer = False\n",
        "        # model\n",
        "        self.channel_multiplier = 2\n",
        "        self.fix_decoder = False\n",
        "        self.input_is_latent = True\n",
        "        self.different_w = True\n",
        "        self.sft_half = True\n",
        "\n",
        "        self.id_block = 'IRBlock'\n",
        "        self.id_layers = [2,2,2,2]\n",
        "        self.id_use_se = False\n",
        "\n",
        "        self.g_lr = 1e-4\n",
        "        self.d_lr = 4e-5\n",
        "        self.beta1 = 0.\n",
        "        self.beta2 = 0.99\n",
        "\n",
        "        # loss\n",
        "        self.perloss = True\n",
        "        self.pixloss = True\n",
        "        self.idloss = True\n",
        "        self.featloss = True\n",
        "\n",
        "\n",
        "\n",
        "        self.layer_weights = {'conv1_2': 0.1,\n",
        "            'conv2_2': 0.1,\n",
        "            'conv3_4': 1,\n",
        "            'conv4_4': 1,\n",
        "            'conv5_4': 1}\n",
        "        # before relu\n",
        "\n",
        "        self.vgg_type = 'vgg19'\n",
        "        self.use_input_norm = True\n",
        "\n",
        "        # self.lambda_perceptual = 1\n",
        "        # self.lambda_style = 50\n",
        "        # self.lambda_id = 10\n",
        "        # self.lambda_fm = 1\n",
        "        # self.lambda_gan_part = 1e-1\n",
        "        # self.lambda_gan = 1e-1\n",
        "        # self.lambda_l1 = 10\n",
        "        # self.comp_style_weight = 10\n",
        "\n",
        "        self.lambda_perceptual = 1\n",
        "        self.lambda_style = 50\n",
        "        self.lambda_id = 10\n",
        "        self.lambda_fm = 1\n",
        "        self.lambda_gan_part = 1e-1\n",
        "        self.lambda_gan = 1e-1\n",
        "        self.lambda_l1 = 10\n",
        "        self.comp_style_weight = 20\n",
        "\n",
        "        self.range_norm = True\n",
        "        self.criterion = 'l1'\n",
        "\n",
        "        self.gan_type = 'wgan_softplus'\n",
        "        self.part_gan_type = 'vanilla'\n",
        "\n",
        "\n",
        "        # optim\n",
        "        self.net_d_reg_every = 16\n",
        "        self.r1_reg_weight = 10"
      ],
      "metadata": {
        "id": "QDbctLXM0n_e"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training.py\n",
        "# Updated on Feb 22 to handle -f args passed by Colab Jupyter Notebook ***********$$$$$\n",
        "\n",
        "import argparse\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "import sys\n",
        "# from utils import load_state_dict_with_prefix_removal\n",
        "\n",
        "# Function to parse arguments\n",
        "def parse_args():\n",
        "    # Remove Jupyter's automatically passed arguments\n",
        "    if 'ipykernel' in sys.modules:\n",
        "        sys.argv = sys.argv[:1]  # Remove extra arguments except the script name\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"GFPGAN Training\")\n",
        "\n",
        "    # Training settings\n",
        "    parser.add_argument('--isTrain', action=\"store_false\", default=True, help='Enable Training mode')\n",
        "    parser.add_argument('--dist', action=\"store_false\", default=False, help='Use distributed training')\n",
        "    parser.add_argument('--apply_begin_it', action=\"store_false\", default=False, help='Start from a specific iteration')\n",
        "    parser.add_argument('--batch_size', default=1, type=int, help='Batch size for training')\n",
        "    parser.add_argument('--seed', default=42, type=int, help='Random seed for reproducibility')\n",
        "    parser.add_argument('--eval', default=0, type=int, help='Enable evaluation mode (1: Yes, 0: No)')\n",
        "    parser.add_argument('--nDataLoaderThread', default=5, type=int, help='Number of threads for data loading')\n",
        "    parser.add_argument('--print_interval', default=100, type=int, help='How often to print progress')\n",
        "    parser.add_argument('--test_interval', default=500, type=int, help='Test and save every [test_interval] epochs')\n",
        "    parser.add_argument('--save_interval', default=100, type=int, help='Save model interval')\n",
        "    parser.add_argument('--stop_interval', default=20, type=int, help='Interval for early stopping')\n",
        "    parser.add_argument('--begin_it', default=0, type=int, help='Begin epoch')\n",
        "    parser.add_argument('--mx_data_length', default=100, type=int, help='Max data length')\n",
        "    parser.add_argument('--max_epoch', default=10, type=int, help='Total training epochs')\n",
        "    parser.add_argument('--early_stop', action=\"store_true\", default=True, help='Enable early stopping')\n",
        "    parser.add_argument('--scratch', action=\"store_true\", default=False, help='Start from scratch')\n",
        "\n",
        "    # Path settings\n",
        "    parser.add_argument('--checkpoint_path', default='/content/drive/MyDrive/GFPGAN/checkpoints', type=str, help='Path to save model checkpoints')\n",
        "    parser.add_argument('--pretrain_path', default='/content/drive/MyDrive/GFPGAN/pretrained/final.pth', type=str, help='Path to pretrained model')\n",
        "    parser.add_argument('--train_hq_root', default='/content/drive/MyDrive/GFPGAN/sample_and_results/100_samples_hq', type=str, help='Path to high-quality training images')\n",
        "    parser.add_argument('--train_lq_root', default='/content/drive/MyDrive/GFPGAN/sample_and_results/damaged_images', type=str, help='Path to low-quality training images')\n",
        "\n",
        "    # Optimizer settings\n",
        "    parser.add_argument('--lr', default=0.002, type=float, help='Learning rate')\n",
        "    parser.add_argument('--local_rank', type=int, default=0, help='Local rank passed from distributed launcher')\n",
        "\n",
        "    # Parse known arguments to avoid Colab/Jupyter issues\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    print(vars(args))  # Convert args to dictionary and print\n",
        "  # Show all available keys\n",
        "\n",
        "    # Warn about unrecognized arguments\n",
        "    if unknown:\n",
        "        print(f\"Warning: Ignoring unrecognized arguments: {unknown}\")\n",
        "\n",
        "    return args\n",
        "\n",
        "# Function to validate paths\n",
        "def validate_paths(args):\n",
        "    \"\"\"Ensure required paths exist before proceeding.\"\"\"\n",
        "    required_paths = [args.checkpoint_path, args.train_hq_root, args.train_lq_root]\n",
        "\n",
        "    for path in required_paths:\n",
        "        if not path or path.strip() == \"\":\n",
        "            raise ValueError(f\"Invalid root path: {path}. Please provide a valid path.\")\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)  # Create missing directories\n",
        "            print(f\"Created missing directory: {path}\")\n",
        "\n",
        "# Train function\n",
        "def train_net(args):\n",
        "    \"\"\"Placeholder function for the training process.\"\"\"\n",
        "    print(f\"Training started with HQ images from: {args.train_hq_root}\")\n",
        "    print(f\"Training started with LQ images from: {args.train_lq_root}\")\n",
        "    print(f\"Batch Size: {args.batch_size}, Epochs: {args.max_epoch}, Learning Rate: {args.lr}\")\n",
        "\n",
        "    # Assuming these functions are already defined elsewhere\n",
        "    train_loader, test_loader, mx_length = get_data_loader(args)\n",
        "    args.mx_data_length = mx_length\n",
        "    trainer = GFPTrainer(args)\n",
        "    trainer.train_network(train_loader, test_loader)\n",
        "\n",
        "# Main function to set up the training environment\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()  # Parse arguments\n",
        "    validate_paths(args)  # Validate paths before running training\n",
        "\n",
        "    # Example of how `merge_args` function can be used\n",
        "    params = Params()  # Assuming Params() is defined elsewhere\n",
        "    args = merge_args(args, params)\n",
        "\n",
        "    # Setup distributed training if enabled\n",
        "    if args.dist:\n",
        "        dist.init_process_group(backend=\"nccl\")  # Use NCCL for distributed training\n",
        "        dist.barrier()\n",
        "        args.world_size = dist.get_world_size()\n",
        "        args.rank = dist.get_rank()\n",
        "    else:\n",
        "        args.world_size = 1\n",
        "        args.rank = 0\n",
        "\n",
        "    setup_seed(args.seed + args.rank)  # Assuming `setup_seed` is defined elsewhere\n",
        "    print(args)\n",
        "\n",
        "    # Modify checkpoint path to include model name and mode\n",
        "    args.checkpoint_path = os.path.join(args.checkpoint_path, args.name, args.mode)\n",
        "\n",
        "    print(f\"local_rank {int(os.environ.get('LOCAL_RANK', '0'))} | rank {args.rank} | world_size: {args.world_size}\")\n",
        "\n",
        "    # Ensure checkpoint directory exists (only on rank 0)\n",
        "    if args.rank == 0:\n",
        "        if not os.path.exists(args.checkpoint_path):\n",
        "            os.makedirs(args.checkpoint_path)\n",
        "            print(f\"Created checkpoint directory: {args.checkpoint_path}\")\n",
        "\n",
        "    # Call the training function\n",
        "    train_net(args)\n"
      ],
      "metadata": {
        "id": "HREwGFjA3PSt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eae774e-82ab-41b5-84e9-d7c424d85019"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'isTrain': True, 'dist': False, 'apply_begin_it': False, 'batch_size': 1, 'seed': 42, 'eval': 0, 'nDataLoaderThread': 5, 'print_interval': 100, 'test_interval': 500, 'save_interval': 100, 'stop_interval': 20, 'begin_it': 0, 'mx_data_length': 100, 'max_epoch': 10, 'early_stop': True, 'scratch': False, 'checkpoint_path': '/content/drive/MyDrive/GFPGAN/checkpoints', 'pretrain_path': '/content/drive/MyDrive/GFPGAN/pretrained/final.pth', 'train_hq_root': '/content/drive/MyDrive/GFPGAN/sample_and_results/100_samples_hq', 'train_lq_root': '/content/drive/MyDrive/GFPGAN/sample_and_results/damaged_images', 'lr': 0.002, 'local_rank': 0}\n",
            "Namespace(isTrain=True, dist=False, apply_begin_it=False, batch_size=1, seed=42, eval=0, nDataLoaderThread=5, print_interval=100, test_interval=500, save_interval=100, stop_interval=20, begin_it=0, mx_data_length=100, max_epoch=10, early_stop=True, scratch=False, checkpoint_path='/content/drive/MyDrive/GFPGAN/checkpoints', pretrain_path='/content/drive/MyDrive/GFPGAN/pretrained/final.pth', train_hq_root='/content/drive/MyDrive/GFPGAN/sample_and_results/100_samples_hq', train_lq_root='/content/drive/MyDrive/GFPGAN/sample_and_results/damaged_images', lr=0.002, local_rank=0, name='GFPGAN', mode='decoder', scratch_gan_path='', scratch_d_path='', scratch_left_eye_path='', scratch_right_eye_path='', scratch_mouth_path='', id_model='/content/drive/MyDrive/GFPGAN/pretrained/arcface_resnet18.pth', img_root='/content/drive/MyDrive/GFPGAN/sample_and_results/100_samples_hq', crop_components=False, eye_enlarge_ratio=1.4, mean=tensor([0.5000, 0.5000, 0.5000]), std=tensor([0.5000, 0.5000, 0.5000]), out_size=512, size=512, use_buffer=False, channel_multiplier=2, fix_decoder=False, input_is_latent=True, different_w=True, sft_half=True, id_block='IRBlock', id_layers=[2, 2, 2, 2], id_use_se=False, g_lr=0.0001, d_lr=4e-05, beta1=0.0, beta2=0.99, perloss=True, pixloss=True, idloss=True, featloss=True, layer_weights={'conv1_2': 0.1, 'conv2_2': 0.1, 'conv3_4': 1, 'conv4_4': 1, 'conv5_4': 1}, vgg_type='vgg19', use_input_norm=True, lambda_perceptual=1, lambda_style=50, lambda_id=10, lambda_fm=1, lambda_gan_part=0.1, lambda_gan=0.1, lambda_l1=10, comp_style_weight=20, range_norm=True, criterion='l1', gan_type='wgan_softplus', part_gan_type='vanilla', net_d_reg_every=16, r1_reg_weight=10, world_size=1, rank=0)\n",
            "local_rank 0 | rank 0 | world_size: 1\n",
            "Training started with HQ images from: /content/drive/MyDrive/GFPGAN/sample_and_results/100_samples_hq\n",
            "Training started with LQ images from: /content/drive/MyDrive/GFPGAN/sample_and_results/damaged_images\n",
            "Batch Size: 1, Epochs: 10, Learning Rate: 0.002\n",
            "Keys available in kwargs: dict_keys(['mean', 'std', 'hq_root', 'lq_root', 'img_root', 'size', 'eval'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-18-d22a3319af6c>:434: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(path, map_location=lambda storage, loc: storage)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'you change train mode!!! It means the state_dict of pre-trained optimizers 'g_optim' and 'd_optim' has not been loaded and the Optimizers will start to train from scratch. It might cause spike in losses in initial epochs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 548M/548M [00:07<00:00, 75.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "standard torchvision vgg model is used here\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-62b398b7983a>:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(state_dict_path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 'module.' prefix from state_dict keys.\n",
            "current steps: 0 | one epoch steps: 100 \n",
            "(epoch: 0\t iters: 0\t time: 4.50281)\ttime: 4.50281\tlr: 0.00010\t\n",
            "(epoch: 1\t iters: 100\t time: 1.94562)\ttime: 1.94562\tlr: 0.00010\t\n",
            "(epoch: 2\t iters: 200\t time: 1.92057)\ttime: 1.92057\tlr: 0.00010\t\n",
            "(epoch: 3\t iters: 300\t time: 1.93499)\ttime: 1.93499\tlr: 0.00010\t\n",
            "(epoch: 4\t iters: 400\t time: 1.89089)\ttime: 1.89089\tlr: 0.00010\t\n",
            "(epoch: 5\t iters: 500\t time: 1.89402)\ttime: 1.89402\tlr: 0.00010\t\n",
            "(epoch: 6\t iters: 600\t time: 2.00607)\ttime: 2.00607\tlr: 0.00010\t\n",
            "(epoch: 7\t iters: 700\t time: 2.12746)\ttime: 2.12746\tlr: 0.00010\t\n",
            "(epoch: 8\t iters: 800\t time: 2.08232)\ttime: 2.08232\tlr: 0.00010\t\n",
            "(epoch: 9\t iters: 900\t time: 2.14990)\ttime: 2.14990\tlr: 0.00010\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.path.exists('/content/drive/MyDrive/GFPGAN/dataset/train/high_quality/hq_images'))\n",
        "print(os.path.exists('/content/drive/MyDrive/GFPGAN/sample_and_results/hq_images_test'))\n",
        "print(os.listdir('/content/drive/MyDrive/GFPGAN/dataset/train/high_quality/hq_images'))\n"
      ],
      "metadata": {
        "id": "ZxSu0IFpjpXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58963d8c-a6d6-49ca-e26d-9d4804158658"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n",
            "['1006.jpg', '1020.jpg', '1003.jpg', '1014.jpg', '1011.jpg', '1001.jpg', '1005.jpg', '1016.jpg', '1023.jpg', '1022.jpg', '1007.jpg', '1004.jpg', '1015.jpg', '1013.jpg', '1008.jpg', '1002.jpg', '1010.jpg', '0999.jpg', '1000.jpg', '1017.jpg', '1009.jpg', '1019.jpg', '1018.jpg', '1021.jpg', '1032.jpg', '1035.jpg', '1034.jpg', '1026.jpg', '1025.jpg', '1036.jpg', '1038.jpg', '1037.jpg', '1033.jpg', '1031.jpg', '1029.jpg', '1024.jpg', '1030.jpg', '1028.jpg', '1027.jpg', '1056.jpg', '1051.jpg', '1043.jpg', '1050.jpg', '1053.jpg', '1044.jpg', '1046.jpg', '1047.jpg', '1057.jpg', '1039.jpg', '1049.jpg', '1052.jpg', '1055.jpg', '1054.jpg', '1045.jpg', '1042.jpg', '1040.jpg', '1048.jpg', '1041.jpg', '1058.jpg', '1065.jpg', '1080.jpg', '1083.jpg', '1066.jpg', '1060.jpg', '1082.jpg', '1086.jpg', '1070.jpg', '1078.jpg', '1075.jpg', '1072.jpg', '1063.jpg', '1084.jpg', '1068.jpg', '1069.jpg', '1074.jpg', '1059.jpg', '1079.jpg', '1064.jpg', '1062.jpg', '1076.jpg', '1071.jpg', '1061.jpg', '1081.jpg', '1073.jpg', '1067.jpg', '1087.jpg', '1077.jpg', '1085.jpg', '1108.jpg', '1091.jpg', '1099.jpg', '1093.jpg', '1089.jpg', '1109.jpg', '1090.jpg', '1113.jpg', '1106.jpg', '1114.jpg', '1110.jpg', '1094.jpg', '1115.jpg', '1105.jpg', '1102.jpg', '1104.jpg', '1117.jpg', '1111.jpg', '1116.jpg', '1088.jpg', '1112.jpg', '1092.jpg', '1095.jpg', '1097.jpg', '1096.jpg', '1098.jpg', '1100.jpg', '1107.jpg', '1101.jpg', '1103.jpg', '1143.jpg', '1130.jpg', '1135.jpg', '1142.jpg', '1118.jpg', '1134.jpg', '1139.jpg', '1125.jpg', '1140.jpg', '1144.jpg', '1137.jpg', '1121.jpg', '1120.jpg', '1136.jpg', '1132.jpg', '1122.jpg', '1138.jpg', '1145.jpg', '1128.jpg', '1141.jpg', '1133.jpg', '1124.jpg', '1123.jpg', '1119.jpg', '1131.jpg', '1129.jpg', '1127.jpg', '1126.jpg', '1157.jpg', '1162.jpg', '1152.jpg', '1167.jpg', '1160.jpg', '1159.jpg', '1147.jpg', '1153.jpg', '1155.jpg', '1156.jpg', '1158.jpg', '1165.jpg', '1172.jpg', '1148.jpg', '1146.jpg', '1154.jpg', '1169.jpg', '1168.jpg', '1171.jpg', '1150.jpg', '1151.jpg', '1149.jpg', '1164.jpg', '1161.jpg', '1163.jpg', '1170.jpg', '1166.jpg', '1192.jpg', '1185.jpg', '1193.jpg', '1181.jpg', '1198.jpg', '1194.jpg', '1177.jpg', '1201.jpg', '1182.jpg', '1184.jpg', '1174.jpg', '1190.jpg', '1186.jpg', '1179.jpg', '1200.jpg', '1196.jpg', '1178.jpg', '1189.jpg', '1183.jpg', '1199.jpg', '1195.jpg', '1180.jpg', '1191.jpg', '1187.jpg', '1173.jpg', '1205.jpg', '1204.jpg', '1202.jpg', '1188.jpg', '1175.jpg', '1197.jpg', '1176.jpg', '1203.jpg', '1211.jpg', '1209.jpg', '1212.jpg', '1206.jpg', '1207.jpg', '1213.jpg', '1208.jpg', '1210.jpg', '1232.jpg', '1227.jpg', '1229.jpg', '1224.jpg', '1216.jpg', '1221.jpg', '1228.jpg', '1225.jpg', '1219.jpg', '1217.jpg', '1218.jpg', '1214.jpg', '1231.jpg', '1222.jpg', '1215.jpg', '1220.jpg', '1223.jpg', '1230.jpg', '1226.jpg', '1252.jpg', '1246.jpg', '1247.jpg', '1249.jpg', '1248.jpg', '1236.jpg', '1242.jpg', '1238.jpg', '1245.jpg', '1250.jpg', '1243.jpg', '1237.jpg', '1244.jpg', '1234.jpg', '1235.jpg', '1240.jpg', '1251.jpg', '1239.jpg', '1233.jpg', '1241.jpg', '1262.jpg', '1276.jpg', '1277.jpg', '1271.jpg', '1269.jpg', '1273.jpg', '1256.jpg', '1275.jpg', '1263.jpg', '1254.jpg', '1261.jpg', '1258.jpg', '1272.jpg', '1278.jpg', '1257.jpg', '1274.jpg', '1265.jpg', '1259.jpg', '1260.jpg', '1264.jpg', '1268.jpg', '1267.jpg', '1266.jpg', '1255.jpg', '1270.jpg', '1253.jpg', '1287.jpg', '1281.jpg', '1291.jpg', '1290.jpg', '1293.jpg', '1294.jpg', '1288.jpg', '1283.jpg', '1282.jpg', '1297.jpg', '1279.jpg', '1285.jpg', '1284.jpg', '1295.jpg', '1289.jpg', '1296.jpg', '1280.jpg', '1292.jpg', '1286.jpg', '1300.jpg', '1313.jpg', '1309.jpg', '1317.jpg', '1299.jpg', '1302.jpg', '1306.jpg', '1304.jpg', '1305.jpg', '1301.jpg', '1303.jpg', '1316.jpg', '1315.jpg', '1312.jpg', '1314.jpg', '1310.jpg', '1298.jpg', '1308.jpg', '1311.jpg', '1307.jpg', '1322.jpg', '1327.jpg', '1325.jpg', '1324.jpg', '1320.jpg', '1323.jpg', '1332.jpg', '1319.jpg', '1328.jpg', '1329.jpg', '1321.jpg', '1326.jpg', '1330.jpg', '1331.jpg', '1318.jpg', '1355.jpg', '1351.jpg', '1338.jpg', '1349.jpg', '1333.jpg', '1341.jpg', '1340.jpg', '1345.jpg', '1356.jpg', '1337.jpg', '1335.jpg', '1352.jpg', '1344.jpg', '1339.jpg', '1346.jpg', '1342.jpg', '1347.jpg', '1334.jpg', '1336.jpg', '1354.jpg', '1348.jpg', '1353.jpg', '1343.jpg', '1357.jpg', '1350.jpg', '1372.jpg', '1370.jpg', '1375.jpg', '1359.jpg', '1363.jpg', '1361.jpg', '1362.jpg', '1365.jpg', '1366.jpg', '1368.jpg', '1373.jpg', '1377.jpg', '1360.jpg', '1364.jpg', '1367.jpg', '1374.jpg', '1371.jpg', '1369.jpg', '1358.jpg', '1376.jpg', '1393.jpg', '1382.jpg', '1389.jpg', '1395.jpg', '1390.jpg', '1398.jpg', '1406.jpg', '1397.jpg', '1388.jpg', '1379.jpg', '1386.jpg', '1396.jpg', '1400.jpg', '1385.jpg', '1404.jpg', '1387.jpg', '1401.jpg', '1384.jpg', '1383.jpg', '1403.jpg', '1391.jpg', '1392.jpg', '1381.jpg', '1405.jpg', '1399.jpg', '1378.jpg', '1380.jpg', '1394.jpg', '1402.jpg', '1426.jpg', '1412.jpg', '1415.jpg', '1411.jpg', '1407.jpg', '1414.jpg', '1432.jpg', '1427.jpg', '1417.jpg', '1438.jpg', '1409.jpg', '1423.jpg', '1433.jpg', '1428.jpg', '1421.jpg', '1422.jpg', '1431.jpg', '1418.jpg', '1420.jpg', '1436.jpg', '1429.jpg', '1419.jpg', '1408.jpg', '1425.jpg', '1416.jpg', '1434.jpg', '1413.jpg', '1435.jpg', '1424.jpg', '1430.jpg', '1410.jpg', '1437.jpg', '1469.jpg', '1456.jpg', '1470.jpg', '1443.jpg', '1467.jpg', '1458.jpg', '1440.jpg', '1452.jpg', '1449.jpg', '1455.jpg', '1441.jpg', '1465.jpg', '1446.jpg', '1447.jpg', '1459.jpg', '1448.jpg', '1464.jpg', '1454.jpg', '1471.jpg', '1462.jpg', '1468.jpg', '1450.jpg', '1439.jpg', '1460.jpg', '1466.jpg', '1457.jpg', '1442.jpg', '1453.jpg', '1461.jpg', '1444.jpg', '1445.jpg', '1472.jpg', '1451.jpg', '1463.jpg', '1473.jpg', '1484.jpg', '1485.jpg', '1498.jpg', '1475.jpg', '1490.jpg', '1486.jpg', '1504.jpg', '1505.jpg', '1487.jpg', '1506.jpg', '1491.jpg', '1478.jpg', '1494.jpg', '1493.jpg', '1474.jpg', '1477.jpg', '1500.jpg', '1483.jpg', '1476.jpg', '1489.jpg', '1503.jpg', '1495.jpg', '1492.jpg', '1482.jpg', '1479.jpg', '1488.jpg', '1497.jpg', '1499.jpg', '1496.jpg', '1501.jpg', '1480.jpg', '1502.jpg', '1481.jpg', '1535.jpg', '1520.jpg', '1519.jpg', '1524.jpg', '1527.jpg', '1517.jpg', '1538.jpg', '1530.jpg', '1507.jpg', '1525.jpg', '1521.jpg', '1531.jpg', '1532.jpg', '1513.jpg', '1509.jpg', '1518.jpg', '1528.jpg', '1536.jpg', '1508.jpg', '1511.jpg', '1533.jpg', '1514.jpg', '1512.jpg', '1526.jpg', '1523.jpg', '1510.jpg', '1515.jpg', '1516.jpg', '1522.jpg', '1529.jpg', '1534.jpg', '1537.jpg', '1570.jpg', '1552.jpg', '1567.jpg', '1540.jpg', '1571.jpg', '1557.jpg', '1561.jpg', '1565.jpg', '1546.jpg', '1564.jpg', '1559.jpg', '1547.jpg', '1562.jpg', '1544.jpg', '1545.jpg', '1548.jpg', '1554.jpg', '1563.jpg', '1542.jpg', '1568.jpg', '1549.jpg', '1553.jpg', '1550.jpg', '1566.jpg', '1560.jpg', '1558.jpg', '1555.jpg', '1551.jpg', '1539.jpg', '1543.jpg', '1556.jpg', '1569.jpg', '1541.jpg', '1585.jpg', '1598.jpg', '1576.jpg', '1600.jpg', '1573.jpg', '1588.jpg', '1581.jpg', '1589.jpg', '1597.jpg', '1572.jpg', '1579.jpg', '1578.jpg', '1584.jpg', '1574.jpg', '1590.jpg', '1587.jpg', '1595.jpg', '1580.jpg', '1586.jpg', '1575.jpg', '1577.jpg', '1592.jpg', '1583.jpg', '1594.jpg', '1582.jpg', '1599.jpg', '1601.jpg', '1596.jpg', '1593.jpg', '1591.jpg', '1631.jpg', '1630.jpg', '1623.jpg', '1632.jpg', '1617.jpg', '1607.jpg', '1614.jpg', '1612.jpg', '1616.jpg', '1624.jpg', '1626.jpg', '1606.jpg', '1609.jpg', '1625.jpg', '1605.jpg', '1620.jpg', '1622.jpg', '1610.jpg', '1621.jpg', '1619.jpg', '1627.jpg', '1628.jpg', '1603.jpg', '1618.jpg', '1629.jpg', '1611.jpg', '1613.jpg', '1608.jpg', '1604.jpg', '1615.jpg', '1602.jpg', '1635.jpg', '1638.jpg', '1647.jpg', '1634.jpg', '1645.jpg', '1643.jpg', '1639.jpg', '1661.jpg', '1636.jpg', '1640.jpg', '1642.jpg', '1633.jpg', '1656.jpg', '1662.jpg', '1654.jpg', '1649.jpg', '1658.jpg', '1650.jpg', '1637.jpg', '1655.jpg', '1660.jpg', '1648.jpg', '1646.jpg', '1659.jpg', '1651.jpg', '1657.jpg', '1644.jpg', '1663.jpg', '1652.jpg', '1641.jpg', '1653.jpg', '1695.jpg', '1681.jpg', '1674.jpg', '1664.jpg', '1670.jpg', '1685.jpg', '1682.jpg', '1668.jpg', '1666.jpg', '1671.jpg', '1689.jpg', '1680.jpg', '1665.jpg', '1692.jpg', '1677.jpg', '1675.jpg', '1672.jpg', '1673.jpg', '1688.jpg', '1684.jpg', '1694.jpg', '1679.jpg', '1678.jpg', '1669.jpg', '1667.jpg', '1676.jpg', '1683.jpg', '1687.jpg', '1690.jpg', '1693.jpg', '1697.jpg', '1691.jpg', '1686.jpg', '1696.jpg', '1706.jpg', '1717.jpg', '1711.jpg', '1723.jpg', '1715.jpg', '1720.jpg', '1709.jpg', '1704.jpg', '1726.jpg', '1703.jpg', '1707.jpg', '1728.jpg', '1698.jpg', '1725.jpg', '1701.jpg', '1699.jpg', '1716.jpg', '1708.jpg', '1714.jpg', '1700.jpg', '1719.jpg', '1721.jpg', '1729.jpg', '1727.jpg', '1718.jpg', '1710.jpg', '1713.jpg', '1712.jpg', '1705.jpg', '1722.jpg', '1724.jpg', '1702.jpg', '1735.jpg', '1751.jpg', '1749.jpg', '1731.jpg', '1750.jpg', '1736.jpg', '1740.jpg', '1747.jpg', '1733.jpg', '1748.jpg', '1752.jpg', '1741.jpg', '1730.jpg', '1754.jpg', '1756.jpg', '1738.jpg', '1746.jpg', '1755.jpg', '1744.jpg', '1732.jpg', '1742.jpg', '1753.jpg', '1739.jpg', '1737.jpg', '1743.jpg', '1734.jpg', '1745.jpg', '1768.jpg', '1774.jpg', '1773.jpg', '1766.jpg', '1764.jpg', '1770.jpg', '1777.jpg', '1780.jpg', '1763.jpg', '1761.jpg', '1765.jpg', '1775.jpg', '1779.jpg', '1758.jpg', '1767.jpg', '1760.jpg', '1772.jpg', '1769.jpg', '1759.jpg', '1771.jpg', '1776.jpg', '1757.jpg', '1778.jpg', '1762.jpg', '1785.jpg', '1804.jpg', '1792.jpg', '1796.jpg', '1787.jpg', '1788.jpg', '1783.jpg', '1790.jpg', '1789.jpg', '1803.jpg', '1801.jpg', '1791.jpg', '1800.jpg', '1786.jpg', '1793.jpg', '1806.jpg', '1805.jpg', '1808.jpg', '1798.jpg', '1799.jpg', '1784.jpg', '1795.jpg', '1802.jpg', '1807.jpg', '1794.jpg', '1781.jpg', '1797.jpg', '1809.jpg', '1782.jpg', '1812.jpg', '1811.jpg', '1813.jpg', '1826.jpg', '1822.jpg', '1829.jpg', '1820.jpg', '1827.jpg', '1824.jpg', '1818.jpg', '1815.jpg', '1810.jpg', '1817.jpg', '1814.jpg', '1819.jpg', '1816.jpg', '1821.jpg', '1825.jpg', '1830.jpg', '1823.jpg', '1828.jpg', '1833.jpg', '1846.jpg', '1842.jpg', '1852.jpg', '1843.jpg', '1845.jpg', '1835.jpg', '1834.jpg', '1841.jpg', '1839.jpg', '1851.jpg', '1855.jpg', '1840.jpg', '1853.jpg', '1848.jpg', '1854.jpg', '1847.jpg', '1850.jpg', '1849.jpg', '1832.jpg', '1837.jpg', '1856.jpg', '1838.jpg', '1831.jpg', '1836.jpg', '1844.jpg', '1870.jpg', '1875.jpg', '1858.jpg', '1862.jpg', '1877.jpg', '1864.jpg', '1861.jpg', '1866.jpg', '1876.jpg', '1872.jpg', '1869.jpg', '1863.jpg', '1878.jpg', '1873.jpg', '1865.jpg', '1874.jpg', '1857.jpg', '1879.jpg', '1868.jpg', '1860.jpg', '1867.jpg', '1871.jpg', '1859.jpg', '1888.jpg', '1894.jpg', '1884.jpg', '1882.jpg', '1907.jpg', '1893.jpg', '1899.jpg', '1906.jpg', '1905.jpg', '1895.jpg', '1904.jpg', '1889.jpg', '1901.jpg', '1886.jpg', '1890.jpg', '1892.jpg', '1883.jpg', '1898.jpg', '1900.jpg', '1903.jpg', '1891.jpg', '1902.jpg', '1881.jpg', '1896.jpg', '1897.jpg', '1880.jpg', '1887.jpg', '1885.jpg', '1928.jpg', '1908.jpg', '1910.jpg', '1921.jpg', '1909.jpg', '1914.jpg', '1916.jpg', '1927.jpg', '1920.jpg', '1913.jpg', '1924.jpg', '1926.jpg', '1922.jpg', '1911.jpg', '1915.jpg', '1918.jpg', '1925.jpg', '1919.jpg', '1923.jpg', '1917.jpg', '1912.jpg', '1931.jpg', '1935.jpg', '1934.jpg', '1942.jpg', '1929.jpg', '1939.jpg', '1932.jpg', '1938.jpg', '1940.jpg', '1937.jpg', '1936.jpg', '1930.jpg', '1941.jpg', '1933.jpg', '1943.jpg', '1945.jpg', '1952.jpg', '1946.jpg', '1954.jpg', '1953.jpg', '1950.jpg', '1949.jpg', '1951.jpg', '1948.jpg', '1947.jpg', '1944.jpg', '1956.jpg', '1963.jpg', '1955.jpg', '1960.jpg', '1968.jpg', '1958.jpg', '1959.jpg', '1966.jpg', '1967.jpg', '1964.jpg', '1969.jpg', '1957.jpg', '1965.jpg', '1961.jpg', '1962.jpg', '1984.jpg', '1977.jpg', '1982.jpg', '1983.jpg', '1973.jpg', '1980.jpg', '1976.jpg', '1979.jpg', '1975.jpg', '1972.jpg', '1974.jpg', '1970.jpg', '1971.jpg', '1986.jpg', '1978.jpg', '1981.jpg', '1985.jpg', '1987.jpg', '1994.jpg', '1993.jpg', '1992.jpg', '1990.jpg', '1991.jpg', '1995.jpg', '1998.jpg', '1997.jpg', '1988.jpg', '1999.jpg', '1989.jpg', '1996.jpg', '0009.jpg', '0010.jpg', '0007.jpg', '0011.jpg', '0000.jpg', '0003.jpg', '0005.jpg', '0002.jpg', '0001.jpg', '0008.jpg', '0006.jpg', '0004.jpg', '0029.jpg', '0018.jpg', '0015.jpg', '0023.jpg', '0020.jpg', '0024.jpg', '0027.jpg', '0014.jpg', '0019.jpg', '0026.jpg', '0025.jpg', '0021.jpg', '0016.jpg', '0022.jpg', '0017.jpg', '0013.jpg', '0028.jpg', '0012.jpg', '0030.jpg', '0046.jpg', '0049.jpg', '0047.jpg', '0038.jpg', '0031.jpg', '0041.jpg', '0036.jpg', '0048.jpg', '0032.jpg', '0044.jpg', '0033.jpg', '0050.jpg', '0045.jpg', '0034.jpg', '0039.jpg', '0037.jpg', '0040.jpg', '0042.jpg', '0043.jpg', '0035.jpg', '0063.jpg', '0069.jpg', '0052.jpg', '0060.jpg', '0071.jpg', '0061.jpg', '0065.jpg', '0057.jpg', '0058.jpg', '0068.jpg', '0053.jpg', '0055.jpg', '0067.jpg', '0066.jpg', '0064.jpg', '0073.jpg', '0070.jpg', '0051.jpg', '0072.jpg', '0056.jpg', '0054.jpg', '0059.jpg', '0062.jpg', '0078.jpg', '0094.jpg', '0076.jpg', '0099.jpg', '0090.jpg', '0091.jpg', '0087.jpg', '0075.jpg', '0096.jpg', '0095.jpg', '0084.jpg', '0080.jpg', '0097.jpg', '0079.jpg', '0092.jpg', '0085.jpg', '0098.jpg', '0074.jpg', '0088.jpg', '0086.jpg', '0081.jpg', '0083.jpg', '0077.jpg', '0082.jpg', '0093.jpg', '0089.jpg', '0108.jpg', '0106.jpg', '0105.jpg', '0118.jpg', '0101.jpg', '0114.jpg', '0112.jpg', '0104.jpg', '0116.jpg', '0115.jpg', '0103.jpg', '0111.jpg', '0113.jpg', '0109.jpg', '0102.jpg', '0110.jpg', '0119.jpg', '0100.jpg', '0107.jpg', '0117.jpg', '0122.jpg', '0132.jpg', '0125.jpg', '0135.jpg', '0126.jpg', '0127.jpg', '0128.jpg', '0136.jpg', '0123.jpg', '0133.jpg', '0134.jpg', '0121.jpg', '0120.jpg', '0124.jpg', '0129.jpg', '0131.jpg', '0137.jpg', '0130.jpg', '0143.jpg', '0141.jpg', '0139.jpg', '0140.jpg', '0142.jpg', '0138.jpg', '0144.jpg', '0158.jpg', '0149.jpg', '0159.jpg', '0152.jpg', '0156.jpg', '0155.jpg', '0151.jpg', '0153.jpg', '0157.jpg', '0154.jpg', '0148.jpg', '0145.jpg', '0150.jpg', '0147.jpg', '0146.jpg', '0160.jpg', '0167.jpg', '0161.jpg', '0163.jpg', '0166.jpg', '0164.jpg', '0162.jpg', '0165.jpg', '0178.jpg', '0175.jpg', '0174.jpg', '0171.jpg', '0168.jpg', '0170.jpg', '0173.jpg', '0177.jpg', '0169.jpg', '0176.jpg', '0172.jpg', '0179.jpg', '0180.jpg', '0181.jpg', '0183.jpg', '0182.jpg', '0185.jpg', '0184.jpg', '0190.jpg', '0186.jpg', '0189.jpg', '0191.jpg', '0192.jpg', '0188.jpg', '0193.jpg', '0194.jpg', '0187.jpg', '0200.jpg', '0199.jpg', '0195.jpg', '0198.jpg', '0203.jpg', '0196.jpg', '0197.jpg', '0201.jpg', '0202.jpg', '0204.jpg', '0214.jpg', '0207.jpg', '0210.jpg', '0209.jpg', '0213.jpg', '0206.jpg', '0211.jpg', '0205.jpg', '0208.jpg', '0212.jpg', '0234.jpg', '0227.jpg', '0237.jpg', '0229.jpg', '0232.jpg', '0233.jpg', '0240.jpg', '0224.jpg', '0219.jpg', '0223.jpg', '0239.jpg', '0228.jpg', '0217.jpg', '0215.jpg', '0230.jpg', '0236.jpg', '0218.jpg', '0222.jpg', '0231.jpg', '0225.jpg', '0235.jpg', '0216.jpg', '0221.jpg', '0226.jpg', '0238.jpg', '0220.jpg', '0256.jpg', '0245.jpg', '0263.jpg', '0262.jpg', '0260.jpg', '0265.jpg', '0243.jpg', '0252.jpg', '0249.jpg', '0257.jpg', '0251.jpg', '0247.jpg', '0258.jpg', '0271.jpg', '0270.jpg', '0248.jpg', '0242.jpg', '0244.jpg', '0241.jpg', '0266.jpg', '0273.jpg', '0264.jpg', '0261.jpg', '0268.jpg', '0253.jpg', '0246.jpg', '0274.jpg', '0269.jpg', '0272.jpg', '0267.jpg', '0250.jpg', '0254.jpg', '0259.jpg', '0255.jpg', '0308.jpg', '0293.jpg', '0294.jpg', '0278.jpg', '0307.jpg', '0300.jpg', '0279.jpg', '0289.jpg', '0296.jpg', '0304.jpg', '0298.jpg', '0291.jpg', '0302.jpg', '0299.jpg', '0285.jpg', '0290.jpg', '0305.jpg', '0280.jpg', '0281.jpg', '0287.jpg', '0276.jpg', '0288.jpg', '0282.jpg', '0286.jpg', '0292.jpg', '0277.jpg', '0283.jpg', '0297.jpg', '0309.jpg', '0303.jpg', '0301.jpg', '0284.jpg', '0275.jpg', '0306.jpg', '0295.jpg', '0313.jpg', '0333.jpg', '0316.jpg', '0325.jpg', '0342.jpg', '0338.jpg', '0318.jpg', '0317.jpg', '0331.jpg', '0337.jpg', '0329.jpg', '0324.jpg', '0336.jpg', '0319.jpg', '0327.jpg', '0310.jpg', '0312.jpg', '0332.jpg', '0315.jpg', '0326.jpg', '0321.jpg', '0314.jpg', '0330.jpg', '0339.jpg', '0340.jpg', '0311.jpg', '0328.jpg', '0334.jpg', '0323.jpg', '0322.jpg', '0320.jpg', '0335.jpg', '0341.jpg', '0347.jpg', '0354.jpg', '0372.jpg', '0357.jpg', '0350.jpg', '0375.jpg', '0359.jpg', '0371.jpg', '0364.jpg', '0373.jpg', '0348.jpg', '0370.jpg', '0356.jpg', '0363.jpg', '0351.jpg', '0369.jpg', '0361.jpg', '0344.jpg', '0355.jpg', '0343.jpg', '0346.jpg', '0376.jpg', '0360.jpg', '0358.jpg', '0368.jpg', '0352.jpg', '0366.jpg', '0367.jpg', '0365.jpg', '0362.jpg', '0353.jpg', '0349.jpg', '0345.jpg', '0374.jpg', '0389.jpg', '0386.jpg', '0385.jpg', '0383.jpg', '0394.jpg', '0396.jpg', '0398.jpg', '0380.jpg', '0387.jpg', '0408.jpg', '0399.jpg', '0390.jpg', '0407.jpg', '0381.jpg', '0404.jpg', '0377.jpg', '0392.jpg', '0405.jpg', '0391.jpg', '0379.jpg', '0406.jpg', '0388.jpg', '0395.jpg', '0393.jpg', '0409.jpg', '0378.jpg', '0401.jpg', '0403.jpg', '0384.jpg', '0400.jpg', '0402.jpg', '0397.jpg', '0382.jpg', '0441.jpg', '0430.jpg', '0438.jpg', '0424.jpg', '0417.jpg', '0423.jpg', '0429.jpg', '0416.jpg', '0419.jpg', '0442.jpg', '0443.jpg', '0426.jpg', '0433.jpg', '0440.jpg', '0425.jpg', '0432.jpg', '0434.jpg', '0413.jpg', '0437.jpg', '0420.jpg', '0411.jpg', '0431.jpg', '0412.jpg', '0415.jpg', '0439.jpg', '0422.jpg', '0435.jpg', '0410.jpg', '0428.jpg', '0414.jpg', '0418.jpg', '0436.jpg', '0421.jpg', '0427.jpg', '0450.jpg', '0455.jpg', '0474.jpg', '0477.jpg', '0467.jpg', '0469.jpg', '0465.jpg', '0457.jpg', '0473.jpg', '0464.jpg', '0454.jpg', '0461.jpg', '0475.jpg', '0447.jpg', '0466.jpg', '0462.jpg', '0448.jpg', '0460.jpg', '0472.jpg', '0471.jpg', '0452.jpg', '0468.jpg', '0444.jpg', '0445.jpg', '0453.jpg', '0459.jpg', '0458.jpg', '0446.jpg', '0463.jpg', '0451.jpg', '0476.jpg', '0449.jpg', '0456.jpg', '0470.jpg', '0490.jpg', '0510.jpg', '0505.jpg', '0484.jpg', '0479.jpg', '0502.jpg', '0504.jpg', '0487.jpg', '0493.jpg', '0489.jpg', '0508.jpg', '0488.jpg', '0486.jpg', '0507.jpg', '0495.jpg', '0496.jpg', '0511.jpg', '0500.jpg', '0501.jpg', '0503.jpg', '0482.jpg', '0485.jpg', '0481.jpg', '0478.jpg', '0494.jpg', '0492.jpg', '0497.jpg', '0509.jpg', '0491.jpg', '0498.jpg', '0483.jpg', '0506.jpg', '0499.jpg', '0480.jpg', '0543.jpg', '0515.jpg', '0519.jpg', '0546.jpg', '0538.jpg', '0535.jpg', '0532.jpg', '0533.jpg', '0520.jpg', '0521.jpg', '0534.jpg', '0544.jpg', '0512.jpg', '0540.jpg', '0545.jpg', '0525.jpg', '0517.jpg', '0542.jpg', '0526.jpg', '0516.jpg', '0541.jpg', '0522.jpg', '0537.jpg', '0529.jpg', '0513.jpg', '0524.jpg', '0539.jpg', '0518.jpg', '0527.jpg', '0514.jpg', '0531.jpg', '0536.jpg', '0528.jpg', '0523.jpg', '0530.jpg', '0564.jpg', '0561.jpg', '0550.jpg', '0574.jpg', '0555.jpg', '0553.jpg', '0563.jpg', '0568.jpg', '0573.jpg', '0570.jpg', '0548.jpg', '0567.jpg', '0547.jpg', '0554.jpg', '0565.jpg', '0562.jpg', '0551.jpg', '0549.jpg', '0575.jpg', '0558.jpg', '0569.jpg', '0566.jpg', '0578.jpg', '0552.jpg', '0572.jpg', '0556.jpg', '0559.jpg', '0560.jpg', '0577.jpg', '0576.jpg', '0571.jpg', '0557.jpg', '0586.jpg', '0598.jpg', '0585.jpg', '0592.jpg', '0588.jpg', '0593.jpg', '0601.jpg', '0591.jpg', '0597.jpg', '0605.jpg', '0584.jpg', '0581.jpg', '0602.jpg', '0579.jpg', '0594.jpg', '0595.jpg', '0596.jpg', '0599.jpg', '0582.jpg', '0589.jpg', '0604.jpg', '0603.jpg', '0590.jpg', '0580.jpg', '0587.jpg', '0583.jpg', '0600.jpg', '0635.jpg', '0632.jpg', '0618.jpg', '0634.jpg', '0628.jpg', '0615.jpg', '0608.jpg', '0607.jpg', '0616.jpg', '0630.jpg', '0627.jpg', '0624.jpg', '0620.jpg', '0621.jpg', '0611.jpg', '0614.jpg', '0606.jpg', '0622.jpg', '0613.jpg', '0610.jpg', '0626.jpg', '0625.jpg', '0631.jpg', '0629.jpg', '0609.jpg', '0623.jpg', '0619.jpg', '0617.jpg', '0633.jpg', '0612.jpg', '0645.jpg', '0642.jpg', '0644.jpg', '0662.jpg', '0637.jpg', '0656.jpg', '0643.jpg', '0668.jpg', '0648.jpg', '0670.jpg', '0657.jpg', '0655.jpg', '0649.jpg', '0665.jpg', '0666.jpg', '0669.jpg', '0654.jpg', '0659.jpg', '0667.jpg', '0661.jpg', '0646.jpg', '0636.jpg', '0664.jpg', '0651.jpg', '0658.jpg', '0647.jpg', '0652.jpg', '0663.jpg', '0640.jpg', '0660.jpg', '0639.jpg', '0638.jpg', '0653.jpg', '0641.jpg', '0650.jpg', '0698.jpg', '0678.jpg', '0692.jpg', '0685.jpg', '0687.jpg', '0676.jpg', '0684.jpg', '0689.jpg', '0674.jpg', '0677.jpg', '0699.jpg', '0694.jpg', '0675.jpg', '0680.jpg', '0701.jpg', '0672.jpg', '0690.jpg', '0683.jpg', '0700.jpg', '0681.jpg', '0679.jpg', '0686.jpg', '0671.jpg', '0693.jpg', '0688.jpg', '0691.jpg', '0703.jpg', '0697.jpg', '0695.jpg', '0682.jpg', '0702.jpg', '0696.jpg', '0673.jpg', '0737.jpg', '0706.jpg', '0707.jpg', '0736.jpg', '0714.jpg', '0715.jpg', '0729.jpg', '0721.jpg', '0722.jpg', '0710.jpg', '0720.jpg', '0718.jpg', '0738.jpg', '0716.jpg', '0727.jpg', '0734.jpg', '0723.jpg', '0735.jpg', '0708.jpg', '0704.jpg', '0731.jpg', '0732.jpg', '0713.jpg', '0712.jpg', '0719.jpg', '0724.jpg', '0709.jpg', '0726.jpg', '0733.jpg', '0717.jpg', '0725.jpg', '0728.jpg', '0711.jpg', '0705.jpg', '0730.jpg', '0769.jpg', '0770.jpg', '0743.jpg', '0755.jpg', '0744.jpg', '0746.jpg', '0762.jpg', '0758.jpg', '0745.jpg', '0750.jpg', '0771.jpg', '0764.jpg', '0773.jpg', '0748.jpg', '0759.jpg', '0761.jpg', '0752.jpg', '0754.jpg', '0740.jpg', '0765.jpg', '0741.jpg', '0757.jpg', '0751.jpg', '0739.jpg', '0742.jpg', '0760.jpg', '0772.jpg', '0768.jpg', '0766.jpg', '0747.jpg', '0767.jpg', '0753.jpg', '0749.jpg', '0763.jpg', '0756.jpg', '0780.jpg', '0784.jpg', '0774.jpg', '0787.jpg', '0793.jpg', '0779.jpg', '0792.jpg', '0776.jpg', '0794.jpg', '0785.jpg', '0790.jpg', '0791.jpg', '0783.jpg', '0797.jpg', '0788.jpg', '0799.jpg', '0775.jpg', '0796.jpg', '0801.jpg', '0781.jpg', '0795.jpg', '0777.jpg', '0782.jpg', '0789.jpg', '0798.jpg', '0786.jpg', '0778.jpg', '0802.jpg', '0800.jpg', '0829.jpg', '0807.jpg', '0827.jpg', '0804.jpg', '0834.jpg', '0833.jpg', '0814.jpg', '0825.jpg', '0822.jpg', '0824.jpg', '0828.jpg', '0803.jpg', '0818.jpg', '0813.jpg', '0826.jpg', '0821.jpg', '0830.jpg', '0835.jpg', '0831.jpg', '0819.jpg', '0832.jpg', '0817.jpg', '0805.jpg', '0823.jpg', '0808.jpg', '0812.jpg', '0806.jpg', '0815.jpg', '0816.jpg', '0810.jpg', '0811.jpg', '0820.jpg', '0809.jpg', '0864.jpg', '0866.jpg', '0845.jpg', '0854.jpg', '0859.jpg', '0868.jpg', '0852.jpg', '0841.jpg', '0840.jpg', '0855.jpg', '0839.jpg', '0867.jpg', '0865.jpg', '0847.jpg', '0857.jpg', '0858.jpg', '0869.jpg', '0851.jpg', '0863.jpg', '0836.jpg', '0870.jpg', '0843.jpg', '0862.jpg', '0837.jpg', '0848.jpg', '0849.jpg', '0856.jpg', '0853.jpg', '0838.jpg', '0850.jpg', '0842.jpg', '0861.jpg', '0844.jpg', '0846.jpg', '0860.jpg', '0877.jpg', '0890.jpg', '0874.jpg', '0882.jpg', '0893.jpg', '0886.jpg', '0878.jpg', '0875.jpg', '0888.jpg', '0900.jpg', '0896.jpg', '0880.jpg', '0901.jpg', '0902.jpg', '0872.jpg', '0895.jpg', '0889.jpg', '0903.jpg', '0898.jpg', '0884.jpg', '0883.jpg', '0894.jpg', '0871.jpg', '0885.jpg', '0897.jpg', '0879.jpg', '0876.jpg', '0899.jpg', '0881.jpg', '0873.jpg', '0891.jpg', '0887.jpg', '0892.jpg', '0908.jpg', '0915.jpg', '0906.jpg', '0912.jpg', '0917.jpg', '0913.jpg', '0907.jpg', '0918.jpg', '0931.jpg', '0925.jpg', '0927.jpg', '0922.jpg', '0909.jpg', '0921.jpg', '0911.jpg', '0910.jpg', '0929.jpg', '0905.jpg', '0930.jpg', '0914.jpg', '0924.jpg', '0919.jpg', '0926.jpg', '0904.jpg', '0916.jpg', '0932.jpg', '0920.jpg', '0923.jpg', '0928.jpg', '0951.jpg', '0958.jpg', '0940.jpg', '0946.jpg', '0933.jpg', '0936.jpg', '0950.jpg', '0953.jpg', '0948.jpg', '0935.jpg', '0956.jpg', '0937.jpg', '0955.jpg', '0942.jpg', '0941.jpg', '0947.jpg', '0943.jpg', '0945.jpg', '0949.jpg', '0952.jpg', '0954.jpg', '0957.jpg', '0939.jpg', '0944.jpg', '0938.jpg', '0934.jpg', '0960.jpg', '0959.jpg', '0962.jpg', '0966.jpg', '0965.jpg', '0963.jpg', '0964.jpg', '0961.jpg', '0983.jpg', '0972.jpg', '0970.jpg', '0976.jpg', '0973.jpg', '0968.jpg', '0980.jpg', '0971.jpg', '0967.jpg', '0969.jpg', '0982.jpg', '0979.jpg', '0981.jpg', '0978.jpg', '0975.jpg', '0977.jpg', '0974.jpg', '0987.jpg', '0986.jpg', '0997.jpg', '0993.jpg', '0984.jpg', '0988.jpg', '0989.jpg', '0990.jpg', '0995.jpg', '0996.jpg', '0992.jpg', '0991.jpg', '0994.jpg', '0985.jpg', '0998.jpg', '1012.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing with plotting perceptual loss only\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg19\n",
        "import matplotlib.pyplot as plt  # For plotting\n",
        "\n",
        "\n",
        "class Infer:\n",
        "    def __init__(self, model_path, batch_size=1):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Load the GFPGAN model\n",
        "        self.netG = GFPGANv1Clean(\n",
        "            out_size=512,\n",
        "            num_style_feat=512,\n",
        "            channel_multiplier=2,\n",
        "            decoder_load_path=None,\n",
        "            fix_decoder=True,\n",
        "            num_mlp=8,\n",
        "            input_is_latent=True,\n",
        "            different_w=True,\n",
        "            narrow=1,\n",
        "            sft_half=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Load the model checkpoint\n",
        "        state_dict = torch.load(model_path, map_location=self.device, weights_only=False)\n",
        "        self.netG.load_state_dict(state_dict['g_ema'])\n",
        "        self.netG.eval()\n",
        "\n",
        "        # Initialize Perceptual Loss\n",
        "        self.perceptual_loss = PerceptualLoss(\n",
        "            layer_weights={'conv5_4': 1.0},  # Example layer weights\n",
        "            vgg_type='vgg19',\n",
        "            use_input_norm=True,\n",
        "            range_norm=False,\n",
        "            perceptual_weight=1.0,\n",
        "            style_weight=0.0,\n",
        "            criterion='l1'\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Initialize a list to store test losses\n",
        "        self.test_losses = []\n",
        "\n",
        "    def run(self, img_paths, hq_paths=None, save=None):\n",
        "        \"\"\"Run inference on a list of images.\n",
        "\n",
        "        Args:\n",
        "            img_paths (list): List of paths to input images.\n",
        "            hq_paths (list): List of paths to ground-truth images (optional).\n",
        "            save (str): Directory to save output images (optional).\n",
        "        \"\"\"\n",
        "        if save:\n",
        "            os.makedirs(save, exist_ok=True)\n",
        "\n",
        "        num_images = len(img_paths)\n",
        "        for i in range(0, num_images, self.batch_size):\n",
        "            batch_lq_paths = img_paths[i:i + self.batch_size]\n",
        "            batch_hq_paths = hq_paths[i:i + self.batch_size] if hq_paths else None\n",
        "\n",
        "            # Preprocess batch\n",
        "            batch_lq = torch.cat([self.preprocess(p) for p in batch_lq_paths], dim=0)\n",
        "            batch_hq = torch.cat([self.preprocess(p) for p in batch_hq_paths], dim=0) if batch_hq_paths else None\n",
        "\n",
        "\n",
        "            # Run inference on batch\n",
        "            with torch.no_grad():\n",
        "                batch_oup, _ = self.netG(batch_lq)\n",
        "            # # Debug: Print input tensor size\n",
        "            # print(f\"I/P tensor size: {batch_lq.shape}\")\n",
        "\n",
        "            # # Debug: Print output tensor size\n",
        "            # print(f\"O/P tensor size: {batch_oup.shape}\")\n",
        "\n",
        "            # Calculate perceptual loss if ground-truth images are provided\n",
        "            if batch_hq is not None:\n",
        "                # Ensure ground-truth and output tensors have the same size\n",
        "                if batch_hq.shape != batch_oup.shape:\n",
        "                    batch_hq = F.interpolate(batch_hq, size=batch_oup.shape[2:], mode='bilinear', align_corners=False)\n",
        "                percep_loss, _ = self.perceptual_loss(batch_oup, batch_hq)\n",
        "                self.test_losses.append(percep_loss.item())  # Store the loss\n",
        "                print(f\"Perceptual Loss for batch {i // self.batch_size + 1}: {percep_loss.item()}\")\n",
        "\n",
        "            # Save output images\n",
        "            if save:\n",
        "                for j, img in enumerate(batch_oup):  # Iterate over each image in the batch\n",
        "                    img_processed = self.postprocess(img)  # Postprocess each image\n",
        "                    cv2.imwrite(os.path.join(save, os.path.basename(batch_lq_paths[j])), img_processed)\n",
        "\n",
        "            print(f'\\rProcessed {i + len(batch_lq_paths)}/{num_images} images', end='', flush=True)\n",
        "\n",
        "        print(\"\\nProcessing Complete!\")\n",
        "\n",
        "        # Plot the test loss curve\n",
        "        self.plot_loss_curve()\n",
        "\n",
        "    def preprocess(self, img):\n",
        "        \"\"\"Preprocess an image for model input.\n",
        "\n",
        "        Args:\n",
        "            img (str or np.ndarray): Path to the image or the image itself.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Preprocessed image tensor.\n",
        "        \"\"\"\n",
        "        if isinstance(img, str):\n",
        "            img = cv2.imread(img)\n",
        "        img = cv2.resize(img, (512, 512))  # Resize to expected input size\n",
        "        img = img.astype(np.float32)[..., ::-1] / 127.5 - 1  # Normalize to [-1, 1]\n",
        "        return torch.from_numpy(img.transpose(2, 0, 1)[np.newaxis, ...]).to(self.device)\n",
        "\n",
        "    def postprocess(self, img):\n",
        "        \"\"\"Postprocess an output image tensor.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Output image tensor with shape [channels, height, width].\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Postprocessed image in BGR format.\n",
        "        \"\"\"\n",
        "        # Ensure the input tensor has 3 dimensions\n",
        "        if img.dim() != 3:\n",
        "            raise ValueError(f\"Expected 3 dimensions, but got {img.dim()} dimensions\")\n",
        "\n",
        "        # Clip values to [-1, 1], permute dimensions, and convert to BGR\n",
        "        img = (torch.clip(img, -1, 1).permute(1, 2, 0).cpu().numpy()[..., ::-1])\n",
        "        return ((img + 1) * 127.5).astype(np.uint8)  # Denormalize to [0, 255]\n",
        "\n",
        "    def plot_loss_curve(self):\n",
        "        \"\"\"Plot the test loss curve.\"\"\"\n",
        "        if not self.test_losses:\n",
        "            print(\"No test losses to plot.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.test_losses, label=\"Perceptual Loss\", color='blue')\n",
        "        plt.xlabel(\"Batch Index\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Test Loss Curve\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths\n",
        "    model_path = '/content/drive/MyDrive/GFPGAN/pretrained/final.pth'\n",
        "    base_lq = '/content/drive/MyDrive/GFPGAN/sample_and_results/damaged_images'\n",
        "    base_hq = '/content/drive/MyDrive/GFPGAN/sample_and_results/100_samples_hq'\n",
        "    save = '/content/drive/MyDrive/GFPGAN/sample_and_results/reconstructed_images'\n",
        "\n",
        "    # Get image paths\n",
        "    def get_image_paths(folder):\n",
        "        return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    lq_paths = get_image_paths(base_lq)\n",
        "    hq_paths = get_image_paths(base_hq) if base_hq else None\n",
        "\n",
        "    # Ensure matching filenames\n",
        "    lq_paths.sort()\n",
        "    if hq_paths:\n",
        "        hq_paths.sort()\n",
        "\n",
        "    # Initialize and run inference\n",
        "    model = Infer(model_path, batch_size=1)\n",
        "    model.run(lq_paths, hq_paths, save)"
      ],
      "metadata": {
        "id": "qbtMXcKprWoO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2094f39-b701-47a0-b982-c68cafe7c4f1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "standard torchvision vgg model is used here\n",
            "Perceptual Loss for batch 1: 1.2125568389892578\n",
            "Processed 1/100 imagesPerceptual Loss for batch 2: 1.199852466583252\n",
            "Processed 2/100 imagesPerceptual Loss for batch 3: 1.2295644283294678\n",
            "Processed 3/100 imagesPerceptual Loss for batch 4: 1.3141244649887085\n",
            "Processed 4/100 imagesPerceptual Loss for batch 5: 1.3159440755844116\n",
            "Processed 5/100 imagesPerceptual Loss for batch 6: 1.3495863676071167\n",
            "Processed 6/100 imagesPerceptual Loss for batch 7: 1.1429463624954224\n",
            "Processed 7/100 imagesPerceptual Loss for batch 8: 1.2372866868972778\n",
            "Processed 8/100 imagesPerceptual Loss for batch 9: 1.2344682216644287\n",
            "Processed 9/100 imagesPerceptual Loss for batch 10: 1.071902871131897\n",
            "Processed 10/100 imagesPerceptual Loss for batch 11: 1.018613338470459\n",
            "Processed 11/100 imagesPerceptual Loss for batch 12: 1.3913848400115967\n",
            "Processed 12/100 imagesPerceptual Loss for batch 13: 1.1993293762207031\n",
            "Processed 13/100 imagesPerceptual Loss for batch 14: 1.0383316278457642\n",
            "Processed 14/100 imagesPerceptual Loss for batch 15: 1.188429594039917\n",
            "Processed 15/100 imagesPerceptual Loss for batch 16: 1.256797194480896\n",
            "Processed 16/100 imagesPerceptual Loss for batch 17: 1.3293031454086304\n",
            "Processed 17/100 imagesPerceptual Loss for batch 18: 1.1862801313400269\n",
            "Processed 18/100 imagesPerceptual Loss for batch 19: 1.3464678525924683\n",
            "Processed 19/100 imagesPerceptual Loss for batch 20: 1.3779323101043701\n",
            "Processed 20/100 imagesPerceptual Loss for batch 21: 1.0482563972473145\n",
            "Processed 21/100 imagesPerceptual Loss for batch 22: 1.2129145860671997\n",
            "Processed 22/100 imagesPerceptual Loss for batch 23: 0.9529684782028198\n",
            "Processed 23/100 imagesPerceptual Loss for batch 24: 1.248147964477539\n",
            "Processed 24/100 imagesPerceptual Loss for batch 25: 1.3052022457122803\n",
            "Processed 25/100 imagesPerceptual Loss for batch 26: 1.0588325262069702\n",
            "Processed 26/100 imagesPerceptual Loss for batch 27: 1.4387333393096924\n",
            "Processed 27/100 imagesPerceptual Loss for batch 28: 1.2803153991699219\n",
            "Processed 28/100 imagesPerceptual Loss for batch 29: 1.3015373945236206\n",
            "Processed 29/100 imagesPerceptual Loss for batch 30: 1.3208649158477783\n",
            "Processed 30/100 imagesPerceptual Loss for batch 31: 1.414506196975708\n",
            "Processed 31/100 imagesPerceptual Loss for batch 32: 1.1231248378753662\n",
            "Processed 32/100 imagesPerceptual Loss for batch 33: 1.3807003498077393\n",
            "Processed 33/100 imagesPerceptual Loss for batch 34: 1.0486774444580078\n",
            "Processed 34/100 imagesPerceptual Loss for batch 35: 1.4622122049331665\n",
            "Processed 35/100 imagesPerceptual Loss for batch 36: 1.1330687999725342\n",
            "Processed 36/100 imagesPerceptual Loss for batch 37: 1.0991100072860718\n",
            "Processed 37/100 imagesPerceptual Loss for batch 38: 1.133141279220581\n",
            "Processed 38/100 imagesPerceptual Loss for batch 39: 0.945957601070404\n",
            "Processed 39/100 imagesPerceptual Loss for batch 40: 1.3211784362792969\n",
            "Processed 40/100 imagesPerceptual Loss for batch 41: 1.3320286273956299\n",
            "Processed 41/100 imagesPerceptual Loss for batch 42: 1.2821961641311646\n",
            "Processed 42/100 imagesPerceptual Loss for batch 43: 1.619558572769165\n",
            "Processed 43/100 imagesPerceptual Loss for batch 44: 1.1167521476745605\n",
            "Processed 44/100 imagesPerceptual Loss for batch 45: 1.437819004058838\n",
            "Processed 45/100 imagesPerceptual Loss for batch 46: 1.2752645015716553\n",
            "Processed 46/100 imagesPerceptual Loss for batch 47: 1.3359642028808594\n",
            "Processed 47/100 imagesPerceptual Loss for batch 48: 1.1925489902496338\n",
            "Processed 48/100 imagesPerceptual Loss for batch 49: 1.2904770374298096\n",
            "Processed 49/100 imagesPerceptual Loss for batch 50: 1.2362195253372192\n",
            "Processed 50/100 imagesPerceptual Loss for batch 51: 1.3326818943023682\n",
            "Processed 51/100 imagesPerceptual Loss for batch 52: 0.9532254934310913\n",
            "Processed 52/100 imagesPerceptual Loss for batch 53: 1.1501834392547607\n",
            "Processed 53/100 imagesPerceptual Loss for batch 54: 1.4258018732070923\n",
            "Processed 54/100 imagesPerceptual Loss for batch 55: 1.2548739910125732\n",
            "Processed 55/100 imagesPerceptual Loss for batch 56: 1.3307147026062012\n",
            "Processed 56/100 imagesPerceptual Loss for batch 57: 1.4982949495315552\n",
            "Processed 57/100 imagesPerceptual Loss for batch 58: 1.2987545728683472\n",
            "Processed 58/100 imagesPerceptual Loss for batch 59: 1.357174277305603\n",
            "Processed 59/100 imagesPerceptual Loss for batch 60: 0.9854848384857178\n",
            "Processed 60/100 imagesPerceptual Loss for batch 61: 1.3914117813110352\n",
            "Processed 61/100 imagesPerceptual Loss for batch 62: 1.171234130859375\n",
            "Processed 62/100 imagesPerceptual Loss for batch 63: 1.3440427780151367\n",
            "Processed 63/100 imagesPerceptual Loss for batch 64: 1.2730357646942139\n",
            "Processed 64/100 imagesPerceptual Loss for batch 65: 1.2227884531021118\n",
            "Processed 65/100 imagesPerceptual Loss for batch 66: 1.2745646238327026\n",
            "Processed 66/100 imagesPerceptual Loss for batch 67: 1.2664597034454346\n",
            "Processed 67/100 imagesPerceptual Loss for batch 68: 1.106848955154419\n",
            "Processed 68/100 imagesPerceptual Loss for batch 69: 1.6247236728668213\n",
            "Processed 69/100 imagesPerceptual Loss for batch 70: 1.2733221054077148\n",
            "Processed 70/100 imagesPerceptual Loss for batch 71: 1.6371560096740723\n",
            "Processed 71/100 imagesPerceptual Loss for batch 72: 1.217606782913208\n",
            "Processed 72/100 imagesPerceptual Loss for batch 73: 1.3602956533432007\n",
            "Processed 73/100 imagesPerceptual Loss for batch 74: 1.44105064868927\n",
            "Processed 74/100 imagesPerceptual Loss for batch 75: 1.5663437843322754\n",
            "Processed 75/100 imagesPerceptual Loss for batch 76: 1.180683970451355\n",
            "Processed 76/100 imagesPerceptual Loss for batch 77: 1.4385461807250977\n",
            "Processed 77/100 imagesPerceptual Loss for batch 78: 1.4297288656234741\n",
            "Processed 78/100 imagesPerceptual Loss for batch 79: 1.2259503602981567\n",
            "Processed 79/100 imagesPerceptual Loss for batch 80: 1.3108378648757935\n",
            "Processed 80/100 imagesPerceptual Loss for batch 81: 1.2255358695983887\n",
            "Processed 81/100 imagesPerceptual Loss for batch 82: 1.1488603353500366\n",
            "Processed 82/100 imagesPerceptual Loss for batch 83: 1.1216143369674683\n",
            "Processed 83/100 imagesPerceptual Loss for batch 84: 0.9308023452758789\n",
            "Processed 84/100 imagesPerceptual Loss for batch 85: 1.3156238794326782\n",
            "Processed 85/100 imagesPerceptual Loss for batch 86: 1.2375836372375488\n",
            "Processed 86/100 imagesPerceptual Loss for batch 87: 1.187207579612732\n",
            "Processed 87/100 imagesPerceptual Loss for batch 88: 1.3325886726379395\n",
            "Processed 88/100 imagesPerceptual Loss for batch 89: 1.4592020511627197\n",
            "Processed 89/100 imagesPerceptual Loss for batch 90: 1.365073561668396\n",
            "Processed 90/100 imagesPerceptual Loss for batch 91: 1.180555820465088\n",
            "Processed 91/100 imagesPerceptual Loss for batch 92: 0.9625668525695801\n",
            "Processed 92/100 imagesPerceptual Loss for batch 93: 1.256706714630127\n",
            "Processed 93/100 imagesPerceptual Loss for batch 94: 1.1853201389312744\n",
            "Processed 94/100 imagesPerceptual Loss for batch 95: 1.1560447216033936\n",
            "Processed 95/100 imagesPerceptual Loss for batch 96: 1.1050305366516113\n",
            "Processed 96/100 imagesPerceptual Loss for batch 97: 1.771545171737671\n",
            "Processed 97/100 imagesPerceptual Loss for batch 98: 1.022676706314087\n",
            "Processed 98/100 imagesPerceptual Loss for batch 99: 1.266263723373413\n",
            "Processed 99/100 imagesPerceptual Loss for batch 100: 1.1167840957641602\n",
            "Processed 100/100 images\n",
            "Processing Complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing with plotting Perceputal loss, L1 loss and Style loss\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg19\n",
        "import matplotlib.pyplot as plt  # For plotting\n",
        "\n",
        "\n",
        "class Infer:\n",
        "    def __init__(self, model_path, batch_size=1):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Load the GFPGAN model\n",
        "        self.netG = GFPGANv1Clean(\n",
        "            out_size=512,\n",
        "            num_style_feat=512,\n",
        "            channel_multiplier=2,\n",
        "            decoder_load_path=None,\n",
        "            fix_decoder=True,\n",
        "            num_mlp=8,\n",
        "            input_is_latent=True,\n",
        "            different_w=True,\n",
        "            narrow=1,\n",
        "            sft_half=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Load the model checkpoint\n",
        "        state_dict = torch.load(model_path, map_location=self.device, weights_only=False)\n",
        "        self.netG.load_state_dict(state_dict['g_ema'])\n",
        "        self.netG.eval()\n",
        "\n",
        "        # Initialize Perceptual Loss\n",
        "        self.perceptual_loss = PerceptualLoss(\n",
        "            layer_weights={'conv5_4': 1.0},  # Example layer weights\n",
        "            vgg_type='vgg19',\n",
        "            use_input_norm=True,\n",
        "            range_norm=False,\n",
        "            perceptual_weight=1.0,\n",
        "            style_weight=0.0,\n",
        "            criterion='l1'\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Initialize L1 Loss\n",
        "        self.l1_loss = nn.L1Loss().to(self.device)\n",
        "\n",
        "        # Initialize lists to store test losses\n",
        "        self.perceptual_losses = []\n",
        "        self.l1_losses = []\n",
        "        self.style_losses = []\n",
        "\n",
        "    def run(self, img_paths, hq_paths=None, save=None):\n",
        "        \"\"\"Run inference on a list of images.\n",
        "\n",
        "        Args:\n",
        "            img_paths (list): List of paths to input images.\n",
        "            hq_paths (list): List of paths to ground-truth images (optional).\n",
        "            save (str): Directory to save output images (optional).\n",
        "        \"\"\"\n",
        "        if save:\n",
        "            os.makedirs(save, exist_ok=True)\n",
        "\n",
        "        num_images = len(img_paths)\n",
        "        for i in range(0, num_images, self.batch_size):\n",
        "            batch_lq_paths = img_paths[i:i + self.batch_size]\n",
        "            batch_hq_paths = hq_paths[i:i + self.batch_size] if hq_paths else None\n",
        "\n",
        "            # Preprocess batch\n",
        "            batch_lq = torch.cat([self.preprocess(p) for p in batch_lq_paths], dim=0)\n",
        "            batch_hq = torch.cat([self.preprocess(p) for p in batch_hq_paths], dim=0) if batch_hq_paths else None\n",
        "\n",
        "            # Run inference on batch\n",
        "            with torch.no_grad():\n",
        "                batch_oup, _ = self.netG(batch_lq)\n",
        "\n",
        "            # Calculate losses if ground-truth images are provided\n",
        "            if batch_hq is not None:\n",
        "                # Ensure ground-truth and output tensors have the same size\n",
        "                if batch_hq.shape != batch_oup.shape:\n",
        "                    batch_hq = F.interpolate(batch_hq, size=batch_oup.shape[2:], mode='bilinear', align_corners=False)\n",
        "\n",
        "                # Calculate Perceptual Loss\n",
        "                percep_loss, style_loss = self.perceptual_loss(batch_oup, batch_hq)\n",
        "                self.perceptual_losses.append(percep_loss.item())\n",
        "                self.style_losses.append(style_loss.item() if style_loss is not None else 0.0)\n",
        "\n",
        "                # Calculate L1 Loss\n",
        "                l1_loss = self.l1_loss(batch_oup, batch_hq)\n",
        "                self.l1_losses.append(l1_loss.item())\n",
        "\n",
        "                print(f\"Batch {i // self.batch_size + 1}: \"\n",
        "                      f\"Perceptual Loss = {percep_loss.item():.4f}, \"\n",
        "                      f\"L1 Loss = {l1_loss.item():.4f}, \"\n",
        "                      f\"Style Loss = {style_loss.item() if style_loss is not None else 0.0:.4f}\")\n",
        "\n",
        "            # Save output images\n",
        "            if save:\n",
        "                for j, img in enumerate(batch_oup):  # Iterate over each image in the batch\n",
        "                    img_processed = self.postprocess(img)  # Postprocess each image\n",
        "                    cv2.imwrite(os.path.join(save, os.path.basename(batch_lq_paths[j])), img_processed)\n",
        "\n",
        "            print(f'\\rProcessed {i + len(batch_lq_paths)}/{num_images} images', end='', flush=True)\n",
        "\n",
        "        print(\"\\nProcessing Complete!\")\n",
        "\n",
        "        # Plot the test loss curve\n",
        "        self.plot_loss_curve()\n",
        "\n",
        "    def preprocess(self, img):\n",
        "        \"\"\"Preprocess an image for model input.\n",
        "\n",
        "        Args:\n",
        "            img (str or np.ndarray): Path to the image or the image itself.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Preprocessed image tensor.\n",
        "        \"\"\"\n",
        "        if isinstance(img, str):\n",
        "            img = cv2.imread(img)\n",
        "        img = cv2.resize(img, (512, 512))  # Resize to expected input size\n",
        "        img = img.astype(np.float32)[..., ::-1] / 127.5 - 1  # Normalize to [-1, 1]\n",
        "        return torch.from_numpy(img.transpose(2, 0, 1)[np.newaxis, ...]).to(self.device)\n",
        "\n",
        "    def postprocess(self, img):\n",
        "        \"\"\"Postprocess an output image tensor.\n",
        "\n",
        "        Args:\n",
        "            img (torch.Tensor): Output image tensor with shape [channels, height, width].\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Postprocessed image in BGR format.\n",
        "        \"\"\"\n",
        "        # Ensure the input tensor has 3 dimensions\n",
        "        if img.dim() != 3:\n",
        "            raise ValueError(f\"Expected 3 dimensions, but got {img.dim()} dimensions\")\n",
        "\n",
        "        # Clip values to [-1, 1], permute dimensions, and convert to BGR\n",
        "        img = (torch.clip(img, -1, 1).permute(1, 2, 0).cpu().numpy()[..., ::-1])\n",
        "        return ((img + 1) * 127.5).astype(np.uint8)  # Denormalize to [0, 255]\n",
        "\n",
        "    def plot_loss_curve(self):\n",
        "        \"\"\"Plot the test loss curve.\"\"\"\n",
        "        if not self.perceptual_losses:\n",
        "            print(\"No test losses to plot.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(self.perceptual_losses, label=\"Perceptual Loss\", color='blue')\n",
        "        plt.plot(self.l1_losses, label=\"L1 Loss\", color='red')\n",
        "        plt.plot(self.style_losses, label=\"Style Loss\", color='green')\n",
        "        plt.xlabel(\"Batch Index\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.title(\"Test Loss Curve\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths\n",
        "    model_path = '/content/drive/MyDrive/GFPGAN/pretrained/final.pth'\n",
        "    base_lq = '/content/drive/MyDrive/GFPGAN/sample_and_results/damaged_images'\n",
        "    base_hq = '/content/drive/MyDrive/GFPGAN/sample_and_results/100_samples_hq'\n",
        "    save = '/content/drive/MyDrive/GFPGAN/sample_and_results/reconstructed_images_feb23'\n",
        "\n",
        "    # Get image paths\n",
        "    def get_image_paths(folder):\n",
        "        return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    lq_paths = get_image_paths(base_lq)\n",
        "    hq_paths = get_image_paths(base_hq) if base_hq else None\n",
        "\n",
        "    # Ensure matching filenames\n",
        "    lq_paths.sort()\n",
        "    if hq_paths:\n",
        "        hq_paths.sort()\n",
        "\n",
        "    # Initialize and run inference\n",
        "    model = Infer(model_path, batch_size=1)\n",
        "    model.run(lq_paths, hq_paths, save)"
      ],
      "metadata": {
        "id": "1rawf9xj1M7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee1746d7-79ce-4eec-f5e7-70284698810d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "standard torchvision vgg model is used here\n",
            "Batch 1: Perceptual Loss = 1.2195, L1 Loss = 0.0611, Style Loss = 0.0000\n",
            "Processed 1/100 imagesBatch 2: Perceptual Loss = 1.1938, L1 Loss = 0.0800, Style Loss = 0.0000\n",
            "Processed 2/100 imagesBatch 3: Perceptual Loss = 1.2285, L1 Loss = 0.2347, Style Loss = 0.0000\n",
            "Processed 3/100 imagesBatch 4: Perceptual Loss = 1.3105, L1 Loss = 0.1080, Style Loss = 0.0000\n",
            "Processed 4/100 imagesBatch 5: Perceptual Loss = 1.3211, L1 Loss = 0.0586, Style Loss = 0.0000\n",
            "Processed 5/100 imagesBatch 6: Perceptual Loss = 1.3529, L1 Loss = 0.2550, Style Loss = 0.0000\n",
            "Processed 6/100 imagesBatch 7: Perceptual Loss = 1.1494, L1 Loss = 0.0621, Style Loss = 0.0000\n",
            "Processed 7/100 imagesBatch 8: Perceptual Loss = 1.2446, L1 Loss = 0.1017, Style Loss = 0.0000\n",
            "Processed 8/100 imagesBatch 9: Perceptual Loss = 1.2260, L1 Loss = 0.1389, Style Loss = 0.0000\n",
            "Processed 9/100 imagesBatch 10: Perceptual Loss = 1.0791, L1 Loss = 0.1294, Style Loss = 0.0000\n",
            "Processed 10/100 imagesBatch 11: Perceptual Loss = 1.0149, L1 Loss = 0.1173, Style Loss = 0.0000\n",
            "Processed 11/100 imagesBatch 12: Perceptual Loss = 1.3939, L1 Loss = 0.1379, Style Loss = 0.0000\n",
            "Processed 12/100 imagesBatch 13: Perceptual Loss = 1.1995, L1 Loss = 0.1106, Style Loss = 0.0000\n",
            "Processed 13/100 imagesBatch 14: Perceptual Loss = 1.0373, L1 Loss = 0.1711, Style Loss = 0.0000\n",
            "Processed 14/100 imagesBatch 15: Perceptual Loss = 1.1932, L1 Loss = 0.1924, Style Loss = 0.0000\n",
            "Processed 15/100 imagesBatch 16: Perceptual Loss = 1.2515, L1 Loss = 0.0862, Style Loss = 0.0000\n",
            "Processed 16/100 imagesBatch 17: Perceptual Loss = 1.3371, L1 Loss = 0.1063, Style Loss = 0.0000\n",
            "Processed 17/100 imagesBatch 18: Perceptual Loss = 1.1819, L1 Loss = 0.0884, Style Loss = 0.0000\n",
            "Processed 18/100 imagesBatch 19: Perceptual Loss = 1.3528, L1 Loss = 0.2038, Style Loss = 0.0000\n",
            "Processed 19/100 imagesBatch 20: Perceptual Loss = 1.3786, L1 Loss = 0.0813, Style Loss = 0.0000\n",
            "Processed 20/100 imagesBatch 21: Perceptual Loss = 1.0510, L1 Loss = 0.0829, Style Loss = 0.0000\n",
            "Processed 21/100 imagesBatch 22: Perceptual Loss = 1.2112, L1 Loss = 0.0561, Style Loss = 0.0000\n",
            "Processed 22/100 imagesBatch 23: Perceptual Loss = 0.9556, L1 Loss = 0.0672, Style Loss = 0.0000\n",
            "Processed 23/100 imagesBatch 24: Perceptual Loss = 1.2426, L1 Loss = 0.0846, Style Loss = 0.0000\n",
            "Processed 24/100 imagesBatch 25: Perceptual Loss = 1.2928, L1 Loss = 0.0518, Style Loss = 0.0000\n",
            "Processed 25/100 imagesBatch 26: Perceptual Loss = 1.0559, L1 Loss = 0.1576, Style Loss = 0.0000\n",
            "Processed 26/100 imagesBatch 27: Perceptual Loss = 1.4399, L1 Loss = 0.0638, Style Loss = 0.0000\n",
            "Processed 27/100 imagesBatch 28: Perceptual Loss = 1.2909, L1 Loss = 0.1175, Style Loss = 0.0000\n",
            "Processed 28/100 imagesBatch 29: Perceptual Loss = 1.2966, L1 Loss = 0.2058, Style Loss = 0.0000\n",
            "Processed 29/100 imagesBatch 30: Perceptual Loss = 1.3150, L1 Loss = 0.1592, Style Loss = 0.0000\n",
            "Processed 30/100 imagesBatch 31: Perceptual Loss = 1.4018, L1 Loss = 0.0783, Style Loss = 0.0000\n",
            "Processed 31/100 imagesBatch 32: Perceptual Loss = 1.1281, L1 Loss = 0.1017, Style Loss = 0.0000\n",
            "Processed 32/100 imagesBatch 33: Perceptual Loss = 1.3787, L1 Loss = 0.0716, Style Loss = 0.0000\n",
            "Processed 33/100 imagesBatch 34: Perceptual Loss = 1.0524, L1 Loss = 0.1292, Style Loss = 0.0000\n",
            "Processed 34/100 imagesBatch 35: Perceptual Loss = 1.4545, L1 Loss = 0.0860, Style Loss = 0.0000\n",
            "Processed 35/100 imagesBatch 36: Perceptual Loss = 1.1274, L1 Loss = 0.1856, Style Loss = 0.0000\n",
            "Processed 36/100 imagesBatch 37: Perceptual Loss = 1.1015, L1 Loss = 0.1977, Style Loss = 0.0000\n",
            "Processed 37/100 imagesBatch 38: Perceptual Loss = 1.1248, L1 Loss = 0.0580, Style Loss = 0.0000\n",
            "Processed 38/100 imagesBatch 39: Perceptual Loss = 0.9518, L1 Loss = 0.0390, Style Loss = 0.0000\n",
            "Processed 39/100 imagesBatch 40: Perceptual Loss = 1.3252, L1 Loss = 0.1094, Style Loss = 0.0000\n",
            "Processed 40/100 imagesBatch 41: Perceptual Loss = 1.3366, L1 Loss = 0.0710, Style Loss = 0.0000\n",
            "Processed 41/100 imagesBatch 42: Perceptual Loss = 1.2804, L1 Loss = 0.0828, Style Loss = 0.0000\n",
            "Processed 42/100 imagesBatch 43: Perceptual Loss = 1.6285, L1 Loss = 0.1426, Style Loss = 0.0000\n",
            "Processed 43/100 imagesBatch 44: Perceptual Loss = 1.1084, L1 Loss = 0.1120, Style Loss = 0.0000\n",
            "Processed 44/100 imagesBatch 45: Perceptual Loss = 1.4309, L1 Loss = 0.1439, Style Loss = 0.0000\n",
            "Processed 45/100 imagesBatch 46: Perceptual Loss = 1.2718, L1 Loss = 0.0973, Style Loss = 0.0000\n",
            "Processed 46/100 imagesBatch 47: Perceptual Loss = 1.3442, L1 Loss = 0.1493, Style Loss = 0.0000\n",
            "Processed 47/100 imagesBatch 48: Perceptual Loss = 1.1952, L1 Loss = 0.0717, Style Loss = 0.0000\n",
            "Processed 48/100 imagesBatch 49: Perceptual Loss = 1.2887, L1 Loss = 0.1145, Style Loss = 0.0000\n",
            "Processed 49/100 imagesBatch 50: Perceptual Loss = 1.2364, L1 Loss = 0.0746, Style Loss = 0.0000\n",
            "Processed 50/100 imagesBatch 51: Perceptual Loss = 1.3316, L1 Loss = 0.1276, Style Loss = 0.0000\n",
            "Processed 51/100 imagesBatch 52: Perceptual Loss = 0.9529, L1 Loss = 0.0569, Style Loss = 0.0000\n",
            "Processed 52/100 imagesBatch 53: Perceptual Loss = 1.1355, L1 Loss = 0.0594, Style Loss = 0.0000\n",
            "Processed 53/100 imagesBatch 54: Perceptual Loss = 1.4175, L1 Loss = 0.0771, Style Loss = 0.0000\n",
            "Processed 54/100 imagesBatch 55: Perceptual Loss = 1.2693, L1 Loss = 0.0426, Style Loss = 0.0000\n",
            "Processed 55/100 imagesBatch 56: Perceptual Loss = 1.3287, L1 Loss = 0.0805, Style Loss = 0.0000\n",
            "Processed 56/100 imagesBatch 57: Perceptual Loss = 1.4953, L1 Loss = 0.1249, Style Loss = 0.0000\n",
            "Processed 57/100 imagesBatch 58: Perceptual Loss = 1.2932, L1 Loss = 0.0682, Style Loss = 0.0000\n",
            "Processed 58/100 imagesBatch 59: Perceptual Loss = 1.3534, L1 Loss = 0.0853, Style Loss = 0.0000\n",
            "Processed 59/100 imagesBatch 60: Perceptual Loss = 0.9914, L1 Loss = 0.0957, Style Loss = 0.0000\n",
            "Processed 60/100 imagesBatch 61: Perceptual Loss = 1.3867, L1 Loss = 0.0813, Style Loss = 0.0000\n",
            "Processed 61/100 imagesBatch 62: Perceptual Loss = 1.1664, L1 Loss = 0.0849, Style Loss = 0.0000\n",
            "Processed 62/100 imagesBatch 63: Perceptual Loss = 1.3566, L1 Loss = 0.0695, Style Loss = 0.0000\n",
            "Processed 63/100 imagesBatch 64: Perceptual Loss = 1.2658, L1 Loss = 0.0813, Style Loss = 0.0000\n",
            "Processed 64/100 imagesBatch 65: Perceptual Loss = 1.2192, L1 Loss = 0.0678, Style Loss = 0.0000\n",
            "Processed 65/100 imagesBatch 66: Perceptual Loss = 1.2639, L1 Loss = 0.1102, Style Loss = 0.0000\n",
            "Processed 66/100 imagesBatch 67: Perceptual Loss = 1.2606, L1 Loss = 0.0998, Style Loss = 0.0000\n",
            "Processed 67/100 imagesBatch 68: Perceptual Loss = 1.1014, L1 Loss = 0.1523, Style Loss = 0.0000\n",
            "Processed 68/100 imagesBatch 69: Perceptual Loss = 1.6145, L1 Loss = 0.1737, Style Loss = 0.0000\n",
            "Processed 69/100 imagesBatch 70: Perceptual Loss = 1.2759, L1 Loss = 0.0828, Style Loss = 0.0000\n",
            "Processed 70/100 imagesBatch 71: Perceptual Loss = 1.6397, L1 Loss = 0.1374, Style Loss = 0.0000\n",
            "Processed 71/100 imagesBatch 72: Perceptual Loss = 1.2175, L1 Loss = 0.2340, Style Loss = 0.0000\n",
            "Processed 72/100 imagesBatch 73: Perceptual Loss = 1.3573, L1 Loss = 0.0810, Style Loss = 0.0000\n",
            "Processed 73/100 imagesBatch 74: Perceptual Loss = 1.4278, L1 Loss = 0.1023, Style Loss = 0.0000\n",
            "Processed 74/100 imagesBatch 75: Perceptual Loss = 1.5728, L1 Loss = 0.0770, Style Loss = 0.0000\n",
            "Processed 75/100 imagesBatch 76: Perceptual Loss = 1.1838, L1 Loss = 0.1203, Style Loss = 0.0000\n",
            "Processed 76/100 imagesBatch 77: Perceptual Loss = 1.4372, L1 Loss = 0.1708, Style Loss = 0.0000\n",
            "Processed 77/100 imagesBatch 78: Perceptual Loss = 1.4255, L1 Loss = 0.0941, Style Loss = 0.0000\n",
            "Processed 78/100 imagesBatch 79: Perceptual Loss = 1.2310, L1 Loss = 0.0449, Style Loss = 0.0000\n",
            "Processed 79/100 imagesBatch 80: Perceptual Loss = 1.3193, L1 Loss = 0.0906, Style Loss = 0.0000\n",
            "Processed 80/100 imagesBatch 81: Perceptual Loss = 1.2298, L1 Loss = 0.1184, Style Loss = 0.0000\n",
            "Processed 81/100 imagesBatch 82: Perceptual Loss = 1.1425, L1 Loss = 0.0760, Style Loss = 0.0000\n",
            "Processed 82/100 imagesBatch 83: Perceptual Loss = 1.1264, L1 Loss = 0.1027, Style Loss = 0.0000\n",
            "Processed 83/100 imagesBatch 84: Perceptual Loss = 0.9324, L1 Loss = 0.0897, Style Loss = 0.0000\n",
            "Processed 84/100 imagesBatch 85: Perceptual Loss = 1.3101, L1 Loss = 0.0845, Style Loss = 0.0000\n",
            "Processed 85/100 imagesBatch 86: Perceptual Loss = 1.2321, L1 Loss = 0.0682, Style Loss = 0.0000\n",
            "Processed 86/100 imagesBatch 87: Perceptual Loss = 1.1859, L1 Loss = 0.2142, Style Loss = 0.0000\n",
            "Processed 87/100 imagesBatch 88: Perceptual Loss = 1.3185, L1 Loss = 0.1853, Style Loss = 0.0000\n",
            "Processed 88/100 imagesBatch 89: Perceptual Loss = 1.4618, L1 Loss = 0.0930, Style Loss = 0.0000\n",
            "Processed 89/100 imagesBatch 90: Perceptual Loss = 1.3619, L1 Loss = 0.0822, Style Loss = 0.0000\n",
            "Processed 90/100 imagesBatch 91: Perceptual Loss = 1.1852, L1 Loss = 0.0909, Style Loss = 0.0000\n",
            "Processed 91/100 imagesBatch 92: Perceptual Loss = 0.9657, L1 Loss = 0.0927, Style Loss = 0.0000\n",
            "Processed 92/100 imagesBatch 93: Perceptual Loss = 1.2519, L1 Loss = 0.0826, Style Loss = 0.0000\n",
            "Processed 93/100 imagesBatch 94: Perceptual Loss = 1.1854, L1 Loss = 0.1031, Style Loss = 0.0000\n",
            "Processed 94/100 imagesBatch 95: Perceptual Loss = 1.1584, L1 Loss = 0.1508, Style Loss = 0.0000\n",
            "Processed 95/100 imagesBatch 96: Perceptual Loss = 1.1095, L1 Loss = 0.1868, Style Loss = 0.0000\n",
            "Processed 96/100 imagesBatch 97: Perceptual Loss = 1.7751, L1 Loss = 0.1201, Style Loss = 0.0000\n",
            "Processed 97/100 imagesBatch 98: Perceptual Loss = 1.0247, L1 Loss = 0.0760, Style Loss = 0.0000\n",
            "Processed 98/100 imagesBatch 99: Perceptual Loss = 1.2720, L1 Loss = 0.1257, Style Loss = 0.0000\n",
            "Processed 99/100 imagesBatch 100: Perceptual Loss = 1.1161, L1 Loss = 0.2196, Style Loss = 0.0000\n",
            "Processed 100/100 images\n",
            "Processing Complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "blj6H2u92l_7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Inference.py\n",
        "\n",
        "#TRY RUNNING THIS SEGMENT\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "\n",
        "class Infer:\n",
        "    def __init__(self,model_path):\n",
        "        self.device = 'cpu'\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = 'cuda'\n",
        "        self.netG = GFPGANv1Clean(out_size=512,\n",
        "                                channel_multiplier=2,\n",
        "                                fix_decoder=False,\n",
        "                                input_is_latent=True,\n",
        "                                different_w=True,\n",
        "                                sft_half=True\n",
        "                                ).to(self.device)\n",
        "        self.netG.load_state_dict(torch.load(model_path)['g_ema'])\n",
        "        self.netG.eval()\n",
        "\n",
        "    def run(self,img_paths,save):\n",
        "        os.makedirs(save,exist_ok=True)\n",
        "        for i,img_path in enumerate(img_paths):\n",
        "            self.run_single(img_path,save)\n",
        "            print('\\rhave done %06d'%i,end='',flush=True)\n",
        "        print()\n",
        "\n",
        "    def run_single(self,img_path,save):\n",
        "\n",
        "        inp = self.preprocess(img_path)\n",
        "        with torch.no_grad():\n",
        "            oup,_ = self.netG(inp,)\n",
        "        oup = self.postprocess(oup)\n",
        "        cv2.imwrite(os.path.join(save,os.path.basename(img_path)),oup)\n",
        "\n",
        "    def preprocess(self,img):\n",
        "        if isinstance(img,str):\n",
        "            img = cv2.imread(img)\n",
        "        img = cv2.resize(img,(512,512))\n",
        "        img = img.astype(np.float32)[...,::-1] / 127.5 - 1\n",
        "        return torch.from_numpy(img.transpose(2,0,1)[np.newaxis,...]).to(self.device)\n",
        "\n",
        "    def postprocess(self,img):\n",
        "\n",
        "        return (torch.clip(img,-1,1)[0].permute(1,2,0).cpu().numpy()[...,::-1]+1)*127.5\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = '/content/drive/MyDrive/GFPGAN/pretrained/final.pth'\n",
        "    base_lq = '/content/drive/MyDrive/GFPGAN/sample_and_results/personal_imgs'\n",
        "    model = Infer(model_path)\n",
        "\n",
        "    def get_image_paths(folder):\n",
        "      return [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "    img_paths = get_image_paths(base_lq)\n",
        "\n",
        "    save =  '/content/drive/MyDrive/GFPGAN/sample_and_results/personal_imgs_output'\n",
        "    model.run(img_paths,save)"
      ],
      "metadata": {
        "id": "rYhUN5XuLpLL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a1ef25a-d3bd-41d9-9671-874219a4a96c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-e6a80346e78b>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.netG.load_state_dict(torch.load(model_path)['g_ema'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have done 000003"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#down_upto here\n"
      ],
      "metadata": {
        "id": "i2KQXkprrK7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training Loop WITH Visualization\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "\n",
        "# Track losses and accuracy\n",
        "batch_losses = []\n",
        "epoch_losses = []\n",
        "epoch_accuracies = []\n",
        "\n",
        "num_epochs = 10\n",
        "batch_size = 10\n",
        "checkpoint_dir = \"/content/drive/MyDrive/GFPGAN/checkpoints\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# Training Loop\n",
        "#epoch loop starts\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n Starting Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    # Manually Load the Latest Checkpoint\n",
        "    checkpoint_files = sorted([f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')])\n",
        "\n",
        "    if checkpoint_files:\n",
        "        latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_files[-1])\n",
        "        print(f\"Loading latest checkpoint: {latest_checkpoint}\")\n",
        "        checkpoint = torch.load(latest_checkpoint, map_location=device)\n",
        "\n",
        "        # Load model state_dict\n",
        "        if 'params_ema' in checkpoint:\n",
        "            model.load_state_dict(checkpoint['params_ema'], strict=False)\n",
        "        else:\n",
        "            model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "        print(f\"‚úÖ Loaded Checkpoint: {latest_checkpoint}\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No previous checkpoint found, training from scratch.\")\n",
        "\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    batch_loss_list = []\n",
        "\n",
        "    tqdm_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    last_batch_images = None  # Store the last batch for visualization\n",
        "\n",
        "    # Batch loop starts within the Epoch\n",
        "    for idx, (lq, hq) in tqdm_bar:\n",
        "        lq, hq = lq.to(device), hq.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        reconstructed, _ = model(lq)\n",
        "\n",
        "        # Compute losses\n",
        "        loss_recon = recon_loss(reconstructed, hq)\n",
        "        loss_total = loss_recon  # Add perceptual loss if needed\n",
        "\n",
        "        # Backward pass\n",
        "        loss_total.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store batch loss\n",
        "        batch_losses.append(loss_total.item())\n",
        "        batch_loss_list.append(loss_total.item())\n",
        "        epoch_loss += loss_total.item()\n",
        "\n",
        "        # Update progress bar\n",
        "        tqdm_bar.set_postfix({\"Batch Loss\": loss_total.item()})\n",
        "\n",
        "        # Store last batch for visualization\n",
        "        last_batch_images = (lq, reconstructed, hq)\n",
        "\n",
        "    # Store epoch loss\n",
        "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
        "    epoch_losses.append(avg_epoch_loss)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_epoch_loss:.4f}')\n",
        "\n",
        "\n",
        "    # Save checkpoint at the end of each epoch\n",
        "    checkpoint_path = f\"/content/drive/MyDrive/GFPGAN/checkpoints/Clean_model_epoch_{epoch+1}.pth\"\n",
        "    torch.save(model.state_dict(), checkpoint_path)\n",
        "    print(f\"Model checkpoint saved at: {checkpoint_path}\")\n",
        "\n",
        "\n",
        "\n",
        "    ### SHOW IMAGE IN COLAB FOR LAST BATCH ###\n",
        "    if last_batch_images is not None:\n",
        "        lq, reconstructed, hq = last_batch_images\n",
        "\n",
        "        # Randomly select one image from the last batch\n",
        "        random_idx = random.randint(0, lq.size(0) - 1)\n",
        "        lq_img = lq[random_idx].unsqueeze(0)\n",
        "        rec_img = reconstructed[random_idx].unsqueeze(0)\n",
        "        hq_img = hq[random_idx].unsqueeze(0)\n",
        "\n",
        "        ### SHOW ALL IMAGES IN LAST BATCH & SAVE TO DRIVE ###\n",
        "    if last_batch_images is not None:\n",
        "        lq, reconstructed, hq = last_batch_images\n",
        "\n",
        "        os.makedirs('/content/drive/MyDrive/GFPGAN/results/', exist_ok=True)\n",
        "        for i in range(batch_size):  # Save all images in last batch\n",
        "            lq_img = lq[i].unsqueeze(0)\n",
        "            rec_img = reconstructed[i].unsqueeze(0)\n",
        "            hq_img = hq[i].unsqueeze(0)\n",
        "\n",
        "            output_path = f'/content/drive/MyDrive/GFPGAN/results/epoch_{epoch+1}_image_{i}.jpg'\n",
        "            comparison = torch.cat((lq_img, rec_img, hq_img))\n",
        "            save_image(comparison, output_path)\n",
        "\n",
        "        # Display a random image from last batch\n",
        "        random_idx = random.randint(0, batch_size - 1)\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "        import torchvision.transforms as transforms\n",
        "        to_pil = transforms.ToPILImage()\n",
        "\n",
        "        axes[0].imshow(to_pil(lq[random_idx].cpu()))\n",
        "        axes[0].set_title(\"Low-Quality Input\")\n",
        "        axes[1].imshow(to_pil(reconstructed[random_idx].cpu()))\n",
        "        axes[1].set_title(\"Reconstructed Output\")\n",
        "        axes[2].imshow(to_pil(hq[random_idx].cpu()))\n",
        "        axes[2].set_title(\"Ground Truth\")\n",
        "        plt.show()\n",
        "\n",
        "    ### ACCURACY CALCULATION ###\n",
        "    with torch.no_grad():\n",
        "        accuracy = 100 - (avg_epoch_loss * 100)  # Approximate accuracy\n",
        "        epoch_accuracies.append(accuracy)\n",
        "\n",
        "# PLOT LOSS CURVE SEPARATELY\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(batch_losses, label=\"Batch Loss\", alpha=0.5, color='blue')\n",
        "plt.plot([i * len(train_dataloader) for i in range(num_epochs)], epoch_losses, label=\"Epoch Loss\", marker='o', color='red')\n",
        "plt.xlabel(\"Batches / Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc='upper right')\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# PLOT ACCURACY CURVE SEPARATELY\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot([i for i in range(1, num_epochs+1)], epoch_accuracies, label=\"Epoch Accuracy\", marker='s', linestyle='--', color='green')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.legend(loc='upper left')\n",
        "plt.title(\"Training Accuracy Curve\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "USY8V299gldk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "# from gfpgan.archs.gfpganv1_clean_arch import GFPGANv1Clean\n",
        "\n",
        "# Set Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Paths to fine-tuned model & Test Dataset\n",
        "final_model_path = \"/content/drive/MyDrive/GFPGAN/fine_tuned_model/final_gfpgan.pth\"\n",
        "hq_folder = '/content/drive/MyDrive/GFPGAN/dataset/test/high_quality/hq_images_test'\n",
        "lq_folder = '/content/drive/MyDrive/GFPGAN/dataset/test/low_quality/lq_images_test'  # Low-quality images\n",
        "batch_size = 10\n",
        "\n",
        "\n",
        "# Initialize Model\n",
        "model = GFPGANv1Clean(out_size=512, num_style_feat=512).to(device)\n",
        "\n",
        "# Load the Final Fine-Tuned Model\n",
        "if os.path.exists(final_model_path):\n",
        "    print(f\"Loading final fine-tuned model from: {final_model_path}\")\n",
        "    model.load_state_dict(torch.load(final_model_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Final fine-tuned model not found at {final_model_path}\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Load Test Dataset\n",
        "test_dataloader = DataLoader(HighLowQualityDataset(hq_folder, lq_folder, transform=transform), batch_size=batch_size)\n",
        "\n",
        "# Initialize Metrics\n",
        "test_losses = []\n",
        "accuracies = []\n",
        "\n",
        "# Testing Loop with Loss and Accuracy Tracking\n",
        "with torch.no_grad():\n",
        "    for lq, hq in test_dataloader:\n",
        "        lq, hq = lq.to(device), hq.to(device)\n",
        "\n",
        "        # Forward Pass\n",
        "        reconstructed, _ = model(lq)\n",
        "\n",
        "        # Compute Loss\n",
        "        loss = recon_loss(reconstructed, hq)\n",
        "        test_losses.append(loss.item())\n",
        "\n",
        "        # Compute Accuracy (1 - Normalized Loss)\n",
        "        accuracy = 100 - (loss.item() * 100)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "print(f'Average Test Loss: {sum(test_losses)/len(test_losses):.4f}')\n",
        "print(f'Average Test Accuracy: {sum(accuracies)/len(accuracies):.2f}%')\n",
        "\n",
        "# Plot Test Loss Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(test_losses, label=\"Test Loss\", color='red')\n",
        "plt.xlabel(\"Test Samples\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Test Loss Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Accuracy Curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(accuracies, label=\"Test Accuracy (%)\", color='green')\n",
        "plt.xlabel(\"Test Samples\")\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Test Accuracy Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "keLaWmFUrek7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INFERENCE CODE\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image\n",
        "from gfpgan.archs.gfpganv1_clean_arch import GFPGANv1Clean\n",
        "from collections import OrderedDict\n",
        "\n",
        "# Set Device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Path to Final Fine-Tuned Model\n",
        "# final_model_path = \"/content/drive/MyDrive/GFPGAN/fine_tuned_model/pretrained_GFPGANv1.3.pth\"\n",
        "final_model_path = \"/content/drive/MyDrive/GFPGAN/fine_tuned_model/final_gfpgan.pth\"\n",
        "\n",
        "# Initialize Model\n",
        "model = GFPGANv1Clean(out_size=512, num_style_feat=512).to(device)\n",
        "\n",
        "# # Load the Final Fine-Tuned Model\n",
        "if os.path.exists(final_model_path):\n",
        "    print(f\"Loading final fine-tuned model from: {final_model_path}\")\n",
        "    model.load_state_dict(torch.load(final_model_path, map_location=device))\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Final fine-tuned model not found at {final_model_path}\")\n",
        "\n",
        "\n",
        "# alternative code to load the Original GFPGAN and to handle the architecture mismatch##\n",
        "# Load the Final Fine-Tuned Model\n",
        "# if os.path.exists(final_model_path):\n",
        "#     print(f\"Loading final fine-tuned model from: {final_model_path}\")\n",
        "\n",
        "#     # Load the state_dict\n",
        "#     state_dict = torch.load(final_model_path, map_location=device)\n",
        "\n",
        "#     # Remove unexpected keys\n",
        "#     if 'params_ema' in state_dict:\n",
        "#         del state_dict['params_ema']\n",
        "\n",
        "#     # Load with strict=False, but warn about mismatches\n",
        "#     missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "#     # Print warnings if there are mismatched keys\n",
        "#     if missing_keys:\n",
        "#         print(f\"Missing keys in the checkpoint: {missing_keys}\")\n",
        "#     if unexpected_keys:\n",
        "#         print(f\"Unexpected keys in the checkpoint: {unexpected_keys}\")\n",
        "\n",
        "#     print(\"Model loaded successfully with adjustments.\")\n",
        "\n",
        "# else:\n",
        "#     raise FileNotFoundError(f\"Final fine-tuned model not found at {final_model_path}\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Inference Function\n",
        "def run_inference(image_path, output_folder):\n",
        "    \"\"\"Perform inference on a single image using the fine-tuned model.\"\"\"\n",
        "\n",
        "    # Define Transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((512, 512)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    # Load and preprocess the input image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate output\n",
        "    with torch.no_grad():\n",
        "        output_tensor, _ = model(input_tensor)\n",
        "\n",
        "    # Convert back to PIL image\n",
        "    to_pil = transforms.ToPILImage()\n",
        "    input_image = to_pil(input_tensor.squeeze(0).cpu().clamp(0, 1))\n",
        "    output_image = to_pil(output_tensor.squeeze(0).cpu().clamp(0, 1))\n",
        "\n",
        "    # Save the output in Google Drive\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    output_image_path = os.path.join(output_folder, os.path.basename(image_path))\n",
        "    output_image.save(output_image_path)\n",
        "\n",
        "    print(f\"Inference completed. Output saved at: {output_image_path}\")\n",
        "\n",
        "    # Display images inline\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(input_image)\n",
        "    axes[0].set_title(\"Low-Quality Input\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(output_image)\n",
        "    axes[1].set_title(\"Reconstructed Output\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage\n",
        "input_image_path = \"/content/drive/MyDrive/GFPGAN/dataset/inference_input_images/gfp_input_image.png\"\n",
        "output_folder = \"/content/drive/MyDrive/GFPGAN/results/inference_results\"\n",
        "run_inference(input_image_path, output_folder)\n"
      ],
      "metadata": {
        "id": "-ltb-ZaKred2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFsLVc2AUHpE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}